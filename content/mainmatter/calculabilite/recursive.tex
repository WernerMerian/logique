\chapter{Fonctions récursives, machines de Turing}
\label{chp.recur}

\minitoc

\lettrine{C}{e} chapitre est celui commençant à traiter la calculabilité à
proprement parler. Jusque là, nous avons vu des versions simples d'automates, et
de langages traités par ces automates, pour introduire donner une meilleure
connaissance du traitement des mots par des automates.

Les langages algébriques (et encore plus les langages rationnels) restent
restreints. Par exemple, un langage tel que $\{a^nb^nc^n\mid n \in \mathbb N\}$
n'est pas algébrique, bien qu'il existe clairement un algorithme permettant de
le reconnaître. On souhaite donc trouver une notion plus forte et expressive de
fonction.

Cette notion plus forte est celle de fonction calculable, que nous allons voir
dans ce chapitre à travers les fonctions récursives en premier lieu, puis par
les machines de Turing.

Une fonction calculable est une version mathématique de l'idée intuitive que
l'on se fait de \og fonction programmable sur un ordinateur\fg. Bien sûr, le
caractère mathématique pousse à certaines généralisations (comme le fait que le
modèle mathématique a une mémoire non bornée), mais il reste important, pour
fonder son intuition des fonctions calculables, de garder à l'esprit qu'une
fonction calculable n'est rien d'autre qu'une fonction obtenue en exécutant un
programme d'un langage de programmation suffisamment expressif (tel que
C, Python, Ocaml \latinexpr{etc}).

\section{Fonctions primitives récursives}

Avant d'introduire les fonctions récursives, nous en donnons une version faible
mais très importante~: les fonctions primitives récursives (abrégées en
fonctions RP).

Si l'on imagine une fonction calculable comme un programme
C ou Python, une fonction RP est un programme n'employant comme seule boucle que
la boucle \texttt{for}. Comme nous le verrons, ces
fonctions ne donnent pas une expressivité aussi forte qu'on pourrait le souhaiter, d'où
l'introduction plus tard des fonctions récursives. Cependant, cette première notion de
calcul est centrale pour plusieurs raisons~:
\begin{itemize}
\item elle permet en fait d'exprimer toutes les fonctions qu'un ordinateur peut
  programmer à l'exception d'une unique instruction, que l'on peut rapprocher d'une
  boucle \texttt{while}~;
\item elle contient déjà la plupart des fonctions qui nous sont utiles en mathéamtiques,
  et possède de meilleures propriétés (par exemple un programme n'utilisant que des
  boucles \texttt{for} est toujours sûr de terminer).
\end{itemize}

Nous allons d'abord voir la notion de fonction RP la plus classique (on peut retrouver
les mêmes définitions par exemple dans \cite{cori1993logique}), puis nous introduirons
une version généralisée de cette classe de fonctions. L'intérêt premier de cette version
généralisée est de faciliter le travail d'encodage dans les futurs chapitres, et de
mettre en valeur la classe des fonctions RP non comme un pur objet logique mais bien
comme un modèle de calcul intermédiaire entre les langages algébriques et les langages
récursifs (que nous verrons ensuite).

Historiquement, la notion de fonction primitive récursive est apparue avant celle des
fonctions calculables, notamment dans \cite{skolem1923begrundung} qui a mis en avant
l'importance et la polyvalence des définitions par récurrence. \`A l'époque, de telles
fonctions étaient simplement appelées \og fonctions définies par récurrence\fg car, du
fait de l'absence de la notion de fonction récursive générale, l'adjectif
\og primitive\fg n'avait pas encore de raison d'être introduit. Notons malgré tout que
durant une assez longue période, les fonctions que l'on appelle maintenant récursives
primitives étaient appelées récursives, et les fonctions que l'on appelle maintenant
récursives étaient appelées récursives générales, comme introduites dans
\cite{godel1934undecidable}.

\subsection{Définitions et premières fonctions}

Donnons dès maintenant la définition de la classe RP.

\begin{definition}[Fonctions primitives récursives \cite{skolem1923begrundung}]
  On définit la classe
  $\RecP\subseteq \bigcup_{n \in \mathbb N}\Funct(\mathbb N^n,\mathbb N)$
  comme la plus petite classe telle que~:
  \begin{itemize}
  \item pour toutes les projections
    \[\begin{array}{ccccc}
    \pi_k^n &:& \mathbb N^n &\longrightarrow& \mathbb N\\
    & & (x_1,\ldots,x_n) & \longmapsto & x_k
    \end{array}\]
    avec $n,k\in \mathbb N, \pi_k^n \in \RecP$.
  \item pour tout $n \in \mathbb N$, en notant
    $\mathrm k_0^n : \mathbb N^p \to \mathbb N$ la fonction nulle,
    $\mathrm k_0^n \in \RecP$.
  \item la fonction $n \mapsto n + 1$ est primitive récursive~: $\sucs\in\RecP$.
  \item si $f_1,\ldots,f_n : \mathbb N^k \to \mathbb N$ sont des fonctions RP
    et $g : \mathbb N^n \to \mathbb N$ est une fonction RP, alors la fonction
    \[\begin{array}{ccccc}
    g \circ (f_1,\ldots,f_n) & : & \mathbb N^k &\longrightarrow & \mathbb N\\
    & & (x_1,\ldots,x_k) & \longmapsto &
    g(f_1(x_1,\ldots,x_k),\ldots,f_n(x_1,\ldots,x_k))
    \end{array}\]
    est elle aussi RP.
  \item si $f : \mathbb N^k \to \mathbb N$ et
    $g : \mathbb N^{k+2} \to \mathbb N$ sont des fonctions RP, alors la fonction
    \[\begin{array}{ccccc}
    \rec(f,g) & : & \mathbb N^{k+1} &\longrightarrow & \mathbb N \\
    & & (x_1,\ldots,x_k,0) &\longmapsto & f(x_1,\ldots,x_k)\\
    & & (x_1,\ldots,x_k,y+1) & \longmapsto &
    g(x_1,\ldots,x_k,y,\rec(f,g)(x_1,\ldots,x_k,y))
    \end{array}\]
    est elle aussi RP. On appelle cette opération la récursion primitive.
  \end{itemize}
\end{definition}

\begin{remark}
  Comme toujours, on identifie $\mathbb N^0 \to \mathbb N$ avec $\mathbb N$.
  Ainsi on peut définir une fonction $\mathbb N \to \mathbb N$ à partir d'une
  constante $x_0 \in \mathbb N$ et d'une fonction
  $f_s : \mathbb N^2 \to \mathbb N$.
\end{remark}

\begin{remark}
  Toute fonction RP est une fonction totale, puisque les opérations permettant
  de construire de nouvelles fonctions RP conservent la totalité, et les
  fonctions RP de base sont totales.
\end{remark}

On peut donc, dès lors, donner plusieurs fonctions RP élémentaires.

\begin{proposition}
  Les fonctions suivantes sont RP~:
  \begin{itemize}
  \item pour tout $n \in \mathbb N$ et tout $p \in \mathbb N$, la fonction
    constante
    \[\begin{array}{ccccc}
    \mathrm k_n^p &:& \mathbb N^p&\longrightarrow &\mathbb N\\
    & & (x_1,\ldots,x_p) &\longmapsto & n
    \end{array}\]
  \item $+ : \mathbb N^2 \to \mathbb N$
  \item $\times : \mathbb N^2 \to \mathbb N$
  \item $\exp : \mathbb N^2 \to \mathbb N$
  \end{itemize}
\end{proposition}

\begin{proof}
  Pour les fonctions constantes, il nous suffit de composer $n$ fois la fonction
  $\sucs$ à la fonction nulle $\mathrm k_0^p$.
  
  On sait que les autres fonctions souhaitées sont définies par ces équations~:
  \begin{itemize}
  \item $\forall n \in \mathbb N, n+0 = n$
  \item $\forall n,m \in \mathbb N, n + \sucs(m) = \sucs(n+m)$
  \item $\forall n \in \mathbb N, n \times 0 = 0$
  \item $\forall n,m \in \mathbb N, n \times \sucs(m) = n + (n\times m)$
  \item $\forall n \in \mathbb N, \exp(n,0) = 1$
  \item $\forall n,m\in \mathbb N, \exp(n,\sucs(m)) = n \times \exp(n,m)$
  \end{itemize}
  On définit les fonctions suivantes~:
  \begin{itemize}
  \item $+ \defeq \rec(\pi_1^1,\sucs\circ \pi_3^3)$
  \item $\times \defeq \rec(k_0^1,+ \circ (\pi_2^3,\pi_3^3))$
  \item $\exp \defeq \rec(k_1^1,\times\circ (\pi_2^3,\pi_3^3))$
  \end{itemize}
  Il est direct de vérifier que ces fonctions vérifient les équations attendues.
\end{proof}

A partir de ces fonctions élémentaires, on peut construire les fonctions de
somme et de produit.

\begin{proposition}
  Les fonctions suivantes sont RP~:
  \[
  \begin{array}{ccccc}
    \Sigma^p &: & \mathbb N^p &\longrightarrow & \mathbb N \\
    & & (x_1,\ldots,x_p) &\longmapsto & \sum_{i = 1}^p x_i
  \end{array}\qquad
  \begin{array}{ccccc}
    \Pi^p &: & \mathbb N^p &\longrightarrow & \mathbb N\\
    & & (x_1,\ldots,x_p) &\longmapsto & \prod_{i = 1}^p x_i
  \end{array}
  \]
\end{proposition}

\begin{proof}
  On prouve par récurrence sur $p$ que chaque $\Sigma^p,\Pi^p$ est une fonction
  RP~:
  \begin{itemize}
  \item dans le cas où $p = 0$, on a simplement deux constantes, respectivement
    $0$ et $1$, qui sont RP.
  \item supposons que $\Sigma^p$ est RP, alors on peut définir $\Sigma^{p+1}$
    par
    \[\Sigma^{p+1} \defeq +\circ
    (\Sigma^p \circ (\pi_1^{p+1},\ldots,\pi_p^{p+1}),
    \pi_{p+1}^{p+1})\]
    et $\Pi^{p+1}$ par
    \[\Pi^{p+1}\defeq \times\circ
    (\Pi^p \circ (\pi_1^{p+1},\ldots,\pi_p^{p+1}),
    \pi_{p+1}^{p+1})\]
  \end{itemize}
  Les deux fonctions sont donc RP.
\end{proof}

\begin{exercise}
  Soit une fonction RP $f : \mathbb N^n \to \mathbb N$. Montrer que la
  fonction
  \[\begin{array}{ccccc}
  \Sigma^f &: &\mathbb N^{n+1} &\longrightarrow & \mathbb N\\
  & & (x_1,\ldots,x_n,p) &\longmapsto & \sum_{i = 1}^p f(x_1,\ldots,x_n,i)
  \end{array}\]
  est une fonction RP. Montrer la même chose pour la fonction $\Pi^f$ dans
  laquelle la somme est remplacée par le produit.
\end{exercise}

D'autres fonctions arithmétiques parmi les plus élémentaires sont facilement
prouvables comme étant RP.

\begin{proposition}
  Les fonctions $\min,\max : \mathbb N^2 \to \mathbb N$, la fonction
  $d : \mathbb N^2 \to \mathbb N$ définie par $d(n,m) = | n - m |$,
  ainsi que la fonction
  \[\begin{array}{ccccc}
  - &: & \mathbb N^2 &\longrightarrow & \mathbb N\\
  & & (n,0) & \longmapsto & n\\
  & & (0,\sucs(m)) &\longmapsto & 0\\
  & & (\sucs(n),\sucs(m)) &\longmapsto & n - m
  \end{array}\]
  sont RP.
\end{proposition}

\begin{proof}
  On commence par prouver que $-$ est RP. Pour cela, on définit d'abord la
  fonction prédécesseur, qui envoie $0$ sur $0$ et $\sucs(n)$ sur $n$~:
  \[p \defeq \rec(\mathrm k_0^0,\pi_1^2)\]
  A partir de cette fonction $p$, on définit la fonction $-$ comme son
  itération~:
  \[- \defeq \rec(\pi_1^1,p \circ \pi_3^3)\]
  On voit qu'alors, $n - 0 = n$ pour tout $n \in \mathbb N$. On montre ensuite
  par récurrence sur $m$ que $0 - m = 0$~:
  \begin{itemize}
  \item $0 - 0 = 0$
  \item si $0 - m = 0$, alors $0 - \sucs(m) = p(0-m) = p(0) = 0$
  \end{itemize}
  Il nous reste alors à montrer que $\sucs(n) - \sucs(m) = n - m$. On raisonne
  par induction sur $m$~:
  \begin{itemize}
  \item $\sucs(n) - \sucs(0) = p(\sucs(n) - 0) = p(\sucs(n)) = n = n - 0$
  \item supposons que $\sucs(n) - \sucs(m) = n - m$, alors
    \[
    \sucs(n) - \sucs(\sucs(m)) = p(\sucs(n) - \sucs(m)) = p(n - m)
    = n - \sucs(m)
    \]
  \end{itemize}
  Donc $-$ est RP.

  A partir de cette définition de $-$, on peut prouver que $n-m = \max(0,n-m)$
  (nous considérons la preuve suffisamment directe pour la laisser en
  exercice). On peut alors définir $d(n,m) = (n - m) + (m - n)$.

  On peut vérifier que pour tous $n,m$, on a les deux équations suivantes~:
  \[\min(n,m) = \frac{1}{2}(n + m - |n - m|)\qquad
  \max(n,m) = \frac{1}{2}(n + m + |n - m|)\]
  donc les fonctions $\min$ et $\max$ s'écrivent comme
  \begin{align*}
    \min &\defeq \mathrm{demi}\circ - \circ (+,d)\\
    \max &\defeq \mathrm{demi}\circ + \circ (+,d)
  \end{align*}
  On en déduit que les fonctions souhaitées sont RP.
\end{proof}

\begin{exercise}
  Montrer que les fonctions $\min$ et $\max$ sur un $n$-uplet sont encore des
  fonctions RP.
\end{exercise}

\begin{exercise}
  Vérifier que la fonction
  \[\begin{array}{ccccc}
  \mathrm{demi} & : & \mathbb N &\longrightarrow & \mathbb N\\
  & & n &\longmapsto & \displaystyle\left\lfloor \frac{n}{2}\right\rfloor
  \end{array}\]
  est bien une fonction RP.
\end{exercise}

\begin{exercise}
  Montrer que la fonction $q : \mathbb N^2 \to \mathbb N$ et la fonction
  $r : \mathbb N^2 \to \mathbb N$, renvoyant respectivement le quotient et le
  reste de la division euclidienne d'un entier par un autre, sont des fonctions
  RP.
\end{exercise}

Attardons-nous maintenant sur la notion de prédicat RP. On a vu qu'une fonction
RP était une certaine fonction $\mathbb N^n \to \mathbb N$, mais il est aussi
intéressant de considérer à la place de fonctions des relations
$R\subseteq \mathbb N^n$. On adapte donc notre notion de RP à ces relations.

\begin{definition}[Relation RP]
  On dit qu'une relation $R \subseteq \mathbb N^n$ est RP si sa fonction
  caractéristique $\chi_R : \mathbb N^n \to \btwo$ est une fonction RP.
\end{definition}

\begin{remark}
  On parlera aussi d'ensemble RP ou de partie RP, ce dernier cas plutôt pour
  parler de partie de $\mathbb N$.
\end{remark}

Les relations RP se comportent bien vis à vis de la logique classique, au sens
de la proposition suivante.

\begin{proposition}\label{prop.PRBool}
  Pour tout $n \in \mathbb N$, l'ensemble des relations RP sur $\mathbb N^n$
  est une sous-algèbre de Boole de $\powerset(\mathbb N^n)$.
\end{proposition}

\begin{proof}
  On cherche donc à prouver que les relations RP sont stables par intersection,
  union et complément, et que $\mathbb N^n$ et $\varnothing$ sont des relations
  RP. Les deux derniers points sont directs puisque ces deux relations
  correspondent respectivement à $\mathrm k_1^n$ et $\mathrm k_0^n$.

  Soient $R,S$ deux relations RP, alors~:
  \[\forall n_1,\ldots,n_p \in \mathbb N,
  \begin{cases}
    \chi_{R\cap S}(n_1,\ldots,n_p) =
    \min(\chi_R(n_1,\ldots,n_p),\chi_S(n_1,\ldots,n_p))\\
    \chi_{R\cup S}(n_1,\ldots,n_p) =
    \max(\chi_R(n_1,\ldots,n_p),\chi_S(n_1,\ldots,n_p))
  \end{cases}
  \]
  donc $R\cap S$ et $R\cup S$ sont RP, en composant par $\min$ et $\max$,
  respectivement.

  De plus,
  \[\forall n_1,\ldots,n_p \in \mathbb N,
  \chi_{\mathbb N^p\setminus R}(n_1,\ldots,n_p) = 1 - \chi_R(n_1,\ldots,n_p)\]
  donc $\mathbb N^p \setminus R$ est aussi RP, en composant par
  $x \mapsto 1 - x$.

  Ainsi les relations RP forment une algèbre de Boole pour les opérations
  ensemblistes usuelles.
\end{proof}

\begin{proposition}
  Les relations $\triangle \defeq \{(x,x) \mid x \in \mathbb N^n\}$ et
  $\leq \defeq \{(n,m) \mid n \leq m\}$ sont RP.
\end{proposition}

\begin{proof}
  On prouve d'abord que $\leq$ est RP. Pour cela, on remarque que $n \leq m$
  exactement lorsque $n - m \leq 0$, c'est-à-dire lorsque $n - m = 0$.
  En prenant alors $(n,m) \mapsto 1 - (n - m)$, on a bien une fonction RP à
  valeurs dans $\btwo$ et qui coïncide avec $\chi_\leq$. Pour montrer que
  la relation $\triangle$ est aussi RP, il suffit de remarquer que
  \[(n,m) \in \triangle \iff (n \leq m) \land (m \leq n)\]
  et d'utiliser la \cref{prop.PRBool} pour en déduire le résultat.
\end{proof}

\begin{exercise}
  Montrer que $\triangle^n \defeq
  \{(n,\ldots,n) \in \mathbb N^n\mid n \in \mathbb N\}$ est
  une relation RP.
\end{exercise}

Grâce à ces propriétés de base, on sait déjà que des formules sans
quantification utilisant l'égalité et l'inégalité comme symbole de relation
sont, en s'interprétant dans la structure $\mathbb N$, des relations RP.

On verra qu'il n'est pas possible de rajouter n'importe quelle quantification,
mais le résultat suivant est déjà un résultat particulièrement fort~: une
quantification bornée sur une relation RP reste une relation RP.

\begin{proposition}
  Soit une relation RP $R\subseteq \mathbb N^{n+1}$. On définit les fonctions
  \[\begin{array}{ccccc}
  \exists R&:&\mathbb N^{n+1} & \longrightarrow & \btwo\\
  & & (x_1,\ldots,x_n,y) &\longmapsto &
  \begin{cases}
    1 \text{ si } \exists z < y, R(x_1,\ldots,x_n,z)\\
    0 \text{ sinon }
  \end{cases}
  \end{array}\]
  \[\begin{array}{ccccc}
  \forall R & : & \mathbb N^{n+1} & \longrightarrow & \btwo\\
  & & (x_1,\ldots,x_n,y) &\longmapsto &
  \begin{cases}
    1 \text{ si } \forall z < y, R(x_1,\ldots,x_n,z)\\
    0 \text{ sinon }
  \end{cases}
  \end{array}\]
  Ces deux fonctions sont RP.
\end{proposition}

\begin{proof}
  On construit les deux fonctions par récursion primitive~:
  \[\exists R \defeq \rec(\mathrm k_0^n,\max\circ
  (\pi_{n+2}^{n+2},R\circ(\pi_1^{n+2},\ldots,\pi_{n+1}^{n+2})))\]
  \[\forall R \defeq \rec(\mathrm k_1^n,\min\circ
  (\pi_{n+2}^{n+2},R\circ(\pi_1^{n+2},\ldots,\pi_{n+1}^{n+2})))\]
  On montre seulement le fait que la fonction $\exists R$ ainsi définie
  vérifie la propriété voulue, le cas de $\forall$ étant parfaitement analogue.

  On remarque d'abord qu'il n'existe aucun $z < 0$, donc dans le cas de
  $(x_1,\ldots,x_n,0)$ la valeur retournée est $0$. Dans le cas où l'on appelle
  la fonction sur $(x_1,\ldots,x_n,\sucs(y))$, alors on voit qu'il existe
  $z < \sucs (y)$ tel que $R(x_1,\ldots,x_n,z)$ si et seulement s'il existe
  $z < y$ tel que $R(x_1,\ldots,x_n,z)$ ou si $R(x_1,\ldots,x_n,y)$, ce qui
  correspond à l'équation vérifiée dans le cas récursif.
\end{proof}

\begin{exercise}
  Montrer que l'ensemble des nombres premiers est un ensemble RP.
\end{exercise}

On peut aussi construire des instructions conditionnelles sur des relations RP.

\begin{proposition}
  Soit $R \subseteq \mathbb N^n$ une fonction RP et deux fonctions
  $f,g : \mathbb N^n \to \mathbb N$ deux fonctions RP. Alors la fonction
  \[\begin{array}{ccccc}
  \ifrm(R,f,g)& : & \mathbb N^n &\longrightarrow & \mathbb N\\
  & & (x_1,\ldots,x_n) &\longmapsto &
  \begin{cases}
    f(x_1,\ldots,x_n) \text{ si } (x_1,\ldots,x_n) \in R\\
    g(x_1,\ldots,x_n) \text{ sinon }
  \end{cases}
  \end{array}\]
\end{proposition}

\begin{proof}
  On définit $\ifrm$ de la façon suivante~:
  \[\ifrm(R,f,g) \defeq \rec(g,f\circ(\pi_1^{n+2},\ldots,\pi_n^{n+2}))\circ
  (\pi_1^n,\ldots,\pi_n^n,R)\]
  Lorsque $R(x_1,\ldots,x_n) = 0$, la fonction vaut
  $(g\circ (\pi_1^n,\ldots,\pi_n^n)) (x_1,\ldots,x_n)$, c'est-à-dire
  $g(x_1,\ldots,x_n)$. Lorsque $R(x_1,\ldots,x_n) = 1$, la fonction
  vaut $(f\circ (\pi_1^n,\ldots,\pi_n^n))(x_1,\ldots,x_n)$, c'est-à-dire
  $f(x_1,\ldots,x_n)$. D'où le résultat.
\end{proof}

\begin{remark}
  La fonction $\ifrm(R,f,g)$ peut s'appliquer avec une fonction $h$ à la place
  de $R$. La condition sera alors que la valeur retournée par $h$ est non nulle.
\end{remark}

On voit qu'il est possible de passer facilement d'une fonction à un ensemble,
en considérant la fonction caractéristique. Réciproquement, on souhaite avoir
un moyen, étant donné un prédicat, d'en extraire une fonction~: c'est la notion
de minimisation qui nous sera utile.

\begin{definition}[Minimisation, minimisation bornée]
  Soit $R \subseteq \mathbb N^{n+1}$ une relation. On définit la fonction
  partielle de minimisation comme suit~:
  \[\begin{array}{ccccc}
  \mu R & : & \mathbb N^n & \longrightarrow & \mathbb N\\
  & & (x_1,\ldots,x_n) & \longmapsto & \min
  \{ y \in \mathbb N\mid (x_1,\ldots,x_n,y)\in R\}
  \end{array}\]
  où la fonction n'est pas définie lorsque l'ensemble sur lequel est pris le
  minimum est vide.

  On appelle minimisation bornée la fonction totale suivante~:
  \[\begin{array}{ccccc}
  \mu_B R & : & \mathbb N^{n+1} & \longrightarrow &\mathbb N\\
  & & (x_1,\ldots,x_n,y) & \longmapsto & \min
  \{ z < y \mid (x_1,\ldots,x_n,z) \in R\}
  \end{array}\]
  où la fonction vaut $0$ lorsque l'ensemble sur lequel est pris le minimum est
  vide.
\end{definition}

La minimisation en général n'est pas RP, mais la minimisation bornée l'est. De
plus, le fait que la fonction n'est pas définie dans le cas de $\mu$ est dû
au fait que si l'on songe à nos fonctions comme à des algorithmes, et à $R$
comme à un algorithme de décision, il est impossible de savoir (en temps fini)
s'il existe ou non un certain $y$ tel que $R(x_1,\ldots,x_n,y)$. Au contraire,
dans le cas de la minimisation bornée, il est possible de le vérifier en temps
fini, et on peut donc donner une valeur dans le cas où l'ensemble est vide.

\begin{proposition}
  Soit $R\subseteq \mathbb N^{n+1}$ une relation RP. La fonction $\mu_B R$ est
  une fonction RP.
\end{proposition}

\begin{proof}
  On définit $\mu_B R$ par récursion primitive. Dans le cas où $y = 0$, alors
  l'ensemble $\{ z < 0 \mid (x_1,\ldots,x_n,z) \in R\}$ est vide donc la
  fonction retourne $0$. Dans le cas inductif, lorsque $y = \sucs(y')$, le
  minimum de l'ensemble est soit le minimum de
  $\{ z < y' \mid (x_1,\ldots,x_n,z)\in R\}$ si cet ensemble est non vide, soit
  $y'$ si $(x_1,\ldots,x_n,y') \in R$ si cet ensemble est non vide, soit $0$. On
  pourrait se contenter de cet algorithme, qui nous fait recalculer en
  permanence l'habitation des ensembles. A la place, on va construire une
  fonction qui retourne $\sucs(m)$ où $m$ est le minimum s'il existe, et $0$
  sinon~:
  \[f_0 \defeq \rec(\km_0^n,\ifrm(\pi_{n+2}^{n+2},\pi_{n+2}^{n+2},
  \times\circ(R\circ(\pi_1^{n+2},\ldots,\pi_{n+1}^{n+2}),
  \sucs\circ \pi_{n+1}^{n+2})))\]
  On définit alors $\mu_B R$ simplement par $p\circ f_0$, où $p$ est la
  fonction précédesseur définie plus haut.
\end{proof}

Avec les outils présents, on peut montrer que la bijection de Cantor introduite
dans la \cref{def.bij.Cantor} est une fonction RP.

\begin{proposition}\label{prop.Cantor.bij}
  La bijection de Cantor $\alpha : \mathbb N \times \mathbb N \to \mathbb N$
  est une fonction RP. Les deux fonctions
  $\pi_1',\pi_2' : \mathbb N \to \mathbb N$ telles que
  $(\pi_1'(\alpha(n,m)),\pi_2'(\alpha(n,m))) = (n,m)$ sont elles aussi RP.
\end{proposition}

\begin{proof}
  On sait que la fonction $f_1 : k \mapsto \sum_{i = 1}^k i$ est RP. On peut
  alors définir $\alpha$~:
  \[\alpha \defeq + \circ (f_1\circ +,\pi_2^2)\]
  Ainsi $\alpha$ est bien une fonction RP.

  Comme la situation est symétrique entre $\pi_1'$ et $\pi_2'$, on montre
  seulement que $\pi_1'$ est RP. Soit $n,m\in \mathbb N$ et $a = \alpha(n,m)$.
  Le prédicat $k = \pi_1'(a)$ revient à $\exists p \leq a,\alpha(k,p) = a$ car
  $p \leq \alpha(p,i)$ pour tout $i\in\mathbb N$. De plus, pour tout
  $a\in \mathbb N$, il existe effectivemnet $n,m$ tels que $a = \alpha(n,m)$,
  donc $\{k \leq a \mid \exists p\leq a, \alpha(k,p) = a\}$
  contient exactement un élément, que l'on peut récupérer par minimisation.
  On en déduit donc
  \[\pi_1' \defeq \mu_B (\exists(\triangle\circ
  (\alpha\circ (\pi_2^3,\pi_3^3), \pi_1^3)))\circ (\pi_1^1,\pi_1^1)\]
  Donc $\pi_1'$ est une fonction RP.
\end{proof}

\begin{exercise}
  Montrer que chaque $\alpha_n$ est aussi une fonction RP, et que chaque
  projection $\varpi_i^n$ telle que $\varpi_i^n(\alpha_n(x_1,\ldots,x_n)) = x_i$,
  est une fonction RP.
\end{exercise}

Avec ces données, on se rend compte que le fait d'avoir des fonctions
$\mathbb N^n \to \mathbb N$ n'apporte pas réellement plus d'expressivité que
d'avoir simplement des fonctions $\mathbb N \to \mathbb N$, puisque toutes les
fonctions RP $f : \mathbb N^k \to \mathbb N$ peuvent se réécrire en une
composée de fonctions
$n \mapsto (\varpi_1^p(n),\ldots,\varpi_p^p(n))
\overset{f}{\mapsto} f(n_1,\ldots,n_p)$ elle aussi
RP. Bien sûr, il est toujours nécessaire de définir les fonctions avec plusieurs
coordonnées de par la définition même des fonctions RP (puisqu'en particulier
la récursion primitive utilise un appel sur une seule coordonnée en laissant les
autres statiques).

\subsection{Codages primitifs récursifs}

Dans cette sous-section, on va définir une notion généralisée de fonctions
primitives récursives, où les fonctions ont comme domaine et codomaine d'autres
ensembles que $\mathbb N$. Comme on peut voir $\mathbb N$ comme le monoïde libre
à un élément, $\{*\}^\star$, on peut généraliser les définitions facilement en
remplaçant $\mathbb N$ par $\Sigma^\star$ pour un ensemble fini $\Sigma$.

Les définitions et propositions données dans cette sous-section sont l'\oe uvre
des auteurs de ce livre, d'où l'absence de références bibliographiques.
L'intérêt de ces constructions est avant tout de faciliter des notions de
codage, en remplaçant de futures preuves fastidieuses en 3 résultats
principaux~:
\begin{itemize}
\item généraliser les fonctions récursives primitives sur des alphabets
  différents ne change pas la classe des fonctions RP de la forme
  $\mathbb N^n \to \mathbb N$,
\item les langages algébriques sont primitifs récursifs,
\item les fonctions définies par induction par la \cref{prop.PU.gram} sont
  compatibles avec la classe des fonctions RP.
\end{itemize}
Le deuxième résultat est déjà plus élégant que si l'on se restreignait aux
fonctions $\mathbb N^n \to \mathbb N$, étant donné que l'exprimer uniquement
avec cette classe de fonctions demanderait de définir des codages préalables.
Ne pas passer par des codages supplémentaires pour exprimer ces résultats donne
un aspect plus intrinsèque à notre notion de fonctions PR~: les langages que
cette notion engendrent sont effectivement plus fort que les langages
algébriques.

Cependant, le réel atout de cette généralisation est de permettre de considérer
des opérations \og syntaxiques\fg sur des langages algébriques en restant dans
une classe plus restreinte que ce que nous verrons dans la suite du chapitre.
De plus, grâce au premier des trois résultats, on pourra montrer que toutes ces
opérations peuvent se traduire en des opérations sur
$\mathbb N^n \to \mathbb N$ modulo un codage prélable, sans avoir à prouver à
nouveau que de telles opérations sont bien des fonctions RP. On le verra dans le
CHAPITRE HIERARCHIE ARITHMETIQUE, mais ces résultats donneront en corollaire
beaucoup de résultats techniques.

\begin{definition}[Fonctions récursives primitives généralisées]
  On définit la classe des fonctions récursives primitives généralisées
  (RPG) comme la plus petite classe de fonctions
  $(\Sigma_1)^\star\times\cdots\times(\Sigma_n)^\star\to \Sigma^\star$
  où $\Sigma_1,\ldots,\Sigma_n,\Sigma$ sont des ensembles finis, telle que~:
  \begin{itemize}
  \item chaque fonction
    \[\begin{array}{ccccc}
    \pi_k^n & : &\displaystyle \left(\prod_{i = 1}^n (\Sigma_i)^\star\right)&
    \longrightarrow & (\Sigma_k)^\star\\
    & & (u_1,\ldots,u_n) & \longmapsto & u_k
    \end{array}\]
    est une fonction RPG.
  \item pour tout $\Sigma$ et $a \in \Sigma$, la fonction
    \[\begin{array}{ccccc}
    \cons_a & : & \Sigma^\star & \longrightarrow & \Sigma^\star\\
    & & u & \longmapsto & u\star a
    \end{array}\]
    est une fonction RPG.
  \item pour tous $\Sigma_1,\ldots,\Sigma_n,\Sigma$, la fonction
    \[\begin{array}{ccccc}
    \km_\varepsilon^n &: & \displaystyle
    \left(\prod_{i = 1}^n (\Sigma_i)^\star\right) &\longrightarrow& \Sigma^\star\\
    & & (u_1,\ldots,u_n) & \longmapsto & \varepsilon
    \end{array}\]
    est une fonction RPG.
  \item si $f_k : \prod_{i = 1}^n (\Sigma_i)^\star \to (\Gamma_k)^\star$ est une
    fonction RPG pour chaque $k = 1,\ldots,p$, et
    $g : \prod_{i = 1}^p (\Gamma_i)^\star \to \Sigma$ est une fonction RPG, alors
    \[\begin{array}{ccccc}
    g\circ (f_1,\ldots,f_p) & : & \displaystyle
    \left(\prod_{i = 1}^n (\Sigma_i)^\star\right) &
    \longrightarrow & \Sigma\\
    & & (u_1,\ldots,u_n) &\longmapsto &
    g(f_1(u_1,\ldots,u_n),\ldots,f_p(u_1,\ldots,u_n))
    \end{array}\]
    est une fonction RPG.
  \item si $f : \prod_{i = 1}^p (\Sigma_i)^\star \to \Sigma^\star$ est une
    fonction RPG, $\Gamma$ est un ensemble fini et, pour tout $a\in \Gamma$,
    \[g_a : \left(\prod_{i = 1}^p (\Sigma_i)^\star\right)\times \Gamma^\star\times
    \Sigma^\star\to\Sigma^\star\]
    est une fonction RPG, alors la fonction
    \[\rec(f,(g_a)_{a \in \Gamma}) : \left(\prod_{i = 1}^p (\Sigma_i)^\star\right)
    \times \Gamma^\star \longrightarrow \Sigma^\star \]
    vérifiant les équations
    \begin{align*}
      \rec(f,(g_a)_{a\in\Gamma})(u_1,\ldots,u_p,\varepsilon) &=
      f(u_1,\ldots,u_p)\\
      \rec(f,(g_a)_{a\in\Gamma})(u_1,\ldots,u_p,a\star u) &=
      g_a(u_1,\ldots,u_p,u,\rec(f,(g_a)_{a \in \Gamma})(u_1,\ldots,u_p,u))
    \end{align*}
    est une fonction RPG.
  \end{itemize}
\end{definition}

\begin{remark}
  On parle ici de tous les ensembles finis, ce qui est trop grand pour obtenir
  un ensemble, et donc définir un prédicat inductif \og être une fonction RPG\fg
  en utilisant le théorème de Knaster-Tarski. Pour régler ce problème, on
  utilise en réalité comme seuls alphabets les ensembles $n \in \omega$, qui
  représentent déjà tous les ensembles finis à bijection près.

  On peut ensuite considérer la version adaptée à tout ensemble fini en disant
  qu'une fonction $f : \Sigma^\star \to \Gamma^\star$ est RPG lorsque,
  pour toutes bijections $\sigma : \Sigma \to \{0,\ldots,n\}$ et
  $\tau : \Gamma\to\{0,\ldots,k\}$, $\tau\circ f \circ \sigma^{-1}$ est une
  fonction RPG. Nous ne considérerons cependant ici que des ensemble de la forme
  $n \in \omega$, car la généralisation des fonctions RPG ajoute déjà de
  nombreuses lourdeurs, et posséder comme alphabet uniquement les ordinaux finis
  suffit à ce que l'on souhaite faire pour des codages.
\end{remark}

Les fonctions RP sont naturellement un sous-ensemble des fonctions RPG, en
assimilant $\mathbb N$ à $\{0\}^\star$. On va considérer pour cette sous-section
que $\{0\}^\star$ est exactement l'ensemble $\mathbb N$ qu'on manipule dans le
cas des fonctions RP/RPG.

Une question naturelle à se poser est si cette notion de fonctions RPG est
strictement plus forte que celle de fonctions RP~: ça n'est pas le cas, et c'est
ce que nous allons montrer dans un premier temps. Pour montrer cela, on va
construire une notion de traduction entre chaque $\Sigma^\star$ et $\mathbb N$.
La traduction $f_\Sigma : \Sigma^\star \cong \mathbb N$ sera choisie RPG, et de
telle sorte que pour toute fonction $f$ RPG, la fonction associée en utilisant
les traductions pour avoir une fonction $\hat{f} : \mathbb N^n \to \mathbb N$
est RP.

\begin{definition}[Traduction RPG canonique]\label{def.trad.RPG}
  Soit $\Sigma$ un ensemble fini. Il existe une fonction
  $f_\Sigma : \Sigma^\star \to \mathbb N$ telle que~:
  \[\begin{cases}
  f_\Sigma\text{ est bijective }\\
  f_\Sigma\in \RPG\\
  f^{-1}_\Sigma\in\RPG
  \end{cases}\]
  et telle que $f_{\{0\}} = \id_{\mathbb N}$.
\end{definition}

\begin{proof}
  Sans perte de généralité, on considère que $\Sigma = \{0,\ldots,n-1\}$ pour un
  certain $n \in \mathbb N$. L'encodage de la suite $u = a_p\cdots a_1$ va être
  donné par
  \[f_\Sigma \defeq \sum_{i = 0}^{p-1} (a_i + 1) n^i\]
  Tout d'abord, on voit que dans le cas où $n = 1$, c'est-à-dire le cas du
  singleton $\{0\}$, cela nous donne
  \[f_{\{0\}}(0^n) = \sum_{i = 0}^{n-1} 1^i = n\]
  donc $f_{\{0\}} = \id_\mathbb N$.

  On souhaite donc montrer que $f_\Sigma$ est une fonction RPG. Pour cela, on
  va définir pour $k \in \{0,\ldots,n-1\}$ la fonction
  \begin{align*}
  g_k &: \Sigma^\star \times \mathbb N \to \mathbb N\\
  g_k(u,m) &\defeq m \times n + (k + 1)
  \end{align*}
  Alors on peut définir
  \[f_\Sigma \defeq \rec(0,(g_k)_{k\in\{0,\ldots,n-1\}})\]
  On voit alors que $f_\Sigma(\varepsilon) = 0$ et que si
  $f_\Sigma(a_p\cdots a_1) = \sum_{i = 1}^{p-1} (a_i + 1)n^{i-1}$ alors
  \begin{align*}
    f_\Sigma(a_p\cdots a_0) &= n \times f_\Sigma(a_p\cdots a_1) + (a_0 + 1)\\
    &= n \times \left(\sum_{i = 1}^{p-1} (a_i + 1) n^{i-1}\right) + (a_0+1)\\
    &= \left(\sum_{i = 1}^{p-1} (a_i+1) n^i\right) + (a_0+1)\\
    &= \sum_{i = 0}^{p-1} (a_i + 1) n^i\\
  \end{align*}
  D'où le résultat souhaité par induction sur $\Sigma^\star$.

  On va donner un candidat à la fonction $f_\Sigma^{-1}$ et montrer que ce
  candidat est RPG. Pour cela, on montre d'abord qu'il est possible
  de construire la décomposition en base $n$ sur $p$ chiffres d'un nombre $m$
  donné en tant que fonction RPG $\mathbb N \to \{0,\ldots,n-1\}^\star$.

  Pour commencer, il nous faut construre une première fonction~: étant donné un
  nombre $k \in \mathbb N$, on souhaite montrer que la fonction
  \[\begin{array}{ccccc}
  \cons_{k\% n} &: & \Sigma^\star & \longrightarrow & \Sigma^\star\\
  & & u &\longmapsto & u\star (k\% n)
  \end{array}\]
  et RPG, où $k \% n$ désigne le reste de la division euclidienne de $k$ par
  $n$. En réalité, comme $n$ est un nombre fini, il nous suffit d'imbriquer des
  conditions d'égalité de $k \% n$ à chaque nombre dans $i\in\{0,\ldots,n-1\}$
  et, dans chaque cas, d'utiliser la fonction $\cons_i$.

  On définit alors la fonction
  \[\begin{array}{ccccc}
  f_n &: & \mathbb N^2 &\longrightarrow & \Sigma^\star\\
  & & (k,0) & \longmapsto & \varepsilon\\
  & & (k,d + 1) & \longmapsto & \cons_{k\% n}(f_n(k//n,d))
  \end{array}\]
  où $k//n$ est le quotient de la division euclidienne de $k$ par $n$. Il est
  clair que cette fonction est RPG, en utilisant une récursion primitive. On
  souhaite maintenant montrer par récurrence sur $d$ que
  $f_n(k,d)$ est la suite des $d$ chiffres de poids le plus faible de la
  décomposition en base $n$ de $k$~:
  \begin{itemize}
  \item le cas pour $d = 0$, donc $f_n(k,d) = \varepsilon$, est automatique
  \item pour l'hérédité, on remarque d'abord que la décomposition en base
    $n$ de $k$ est la décomposition en base $n$ de $k//n$ suivie de
    $k \% n$. Ainsi, si $f_n(k//n,d)$ est la suite des $d$ chiffres de poids le
    plus faible de la décomposition en base $n$ de $k//n$, alors
    $f_n(k//n,d)\star k\%n$ est la suite des $d + 1$ chiffres de poids le plus
    faible de la décomposition en base $n$ de $k$.
  \end{itemize}
  Donc la décomposition en base $n$ sur $d$ chiffres est bien une fonction RPG.

  On peut maintenant définir la fonction $d : \mathbb N \to \mathbb N$ telle que
  $d(m)$ est le plus petit entier $d$ tel que
  \[m < \sum_{i = 0}^d n^i\]
  Cette fonction est RPG puisqu'il s'agit d'une minimisation, dont une borne
  est donnée par $m$ lui-même étant donné que $m < \sum_{i = 0}^m n^i$.
  On construit ensuite l'entier $m' = m - \sum_{i = 0}^{d(m)-1} n^i$ (comme $d(m)$
  est choisi minimal, la valeur de droite est bien positive) puis le
  mot sur $\{0,\ldots,n-1\}^\star$ donné par la décomposition de $m'$ sur
  $d$ chiffres. On sait donc que cette fonction est RPG.

  Montrons maintenant que les deux fonctions sont réciproques l'une de l'autre.
  Tout d'abord, si on a un entier $m$, $d$ le plus petit entier $d$ tel que
  $m < \sum_{i = 0}^d n^i$ et $a_{d-1}\cdots a_0$ la décomposition en base
  $n$ sur $d$ chiffres de $m - \sum_{i = 0}^{d-1}n^i$, alors
  \[m = \sum_{i=0}^{d-1} (a_i + 1)n^i\]
  donc $f_\Sigma\circ f_\Sigma^{-1} = \id_{\mathbb N}$.

  Réciproquement, si $a_d\cdots a_0\in \Sigma^\star$, alors en notant
  $m = f_\Sigma(a_d\cdots a_0)$, $d(m) = d + 1$ puisque
  \[\sum_{i = 0}^d n^i \leq m \leq \sum_{i = 0}^d (n-1)n^i = n^{d+1} - 1 <
  \sum_{i = 1}^{d+1} n^i\]
  On voit alors que $m - \sum_{i = 0}^d n^i = \sum_{i = 0}^d a_i n^i$, donc la
  décomposition en base $n$ sur $d+1$ chiffres nous donne $a_d\cdots a_0$.

  Ainsi $f_\Sigma$ est une fonction RPG bijective de réciproque RPG.
\end{proof}

On peut alors associer à une fonction RPG $f$ une fonction RPG ne prenant en
argument et ne retournant que des entiers.

\begin{definition}[Fonction RP associée à une fonction RPG]
  Soit
  \[f : \left(\prod_{i = 1}^n (\Sigma_i)^\star\right) \longrightarrow
  \Sigma^\star\]
  une fonction RPG. On appelle sa fonction RP associée la fonction RPG
  \begin{align*}
    \hat f &: \mathbb N^n \to \mathbb N\\
    \hat f &\defeq f_\Sigma \circ f \circ
    (f_{\Sigma_1}^{-1},\ldots,f_{\Sigma_n}^{-1})
  \end{align*}
\end{definition}

L'objectif est donc de montrer que pour toute fonction RPG $f$, la fonction
$\hat f$ est une focntion RP. La preuve se faisant par induction sur la
définition des fonctions RPG, donnons donc une étude sommaire des cas les
plus évidents~:
\begin{itemize}
\item dans le cas des projections, la fonction associée est
  $f_{\Sigma_k}\circ f^{-1}_{\Sigma_k} = \id$.
\item dans le cas de $\cons_a : \Sigma \to \Sigma$, pour
  $\Sigma = \{0,\ldots,n-1\}$ et tout $a \in \Sigma$,
  la fonction $f_\Sigma\circ \cons_a\circ f_\Sigma^{-1}$ est simplement la
  fonction $k \mapsto n\times k + a + 1$. Cette fonction est RP en utilisant les
  résultats précédents.
\item pour la fonction $\km_\varepsilon$, la fonction associée est directement
  $\km_0$.
\item si $g$ et $(h_i)_{i \in \{1,\ldots,p\}}$ sont des fonctions RPG,
  alors on peut simplement remarque que l'égalité
  $\widehat{g\circ (h_i)_i} = \hat g \circ (\hat{h_i})_i$
  est vérifiée~:
  \begin{align*}
    \widehat{g\circ h} &= f_\Sigma\circ (g \circ (h_i)) \circ
    (f_{\Sigma_1}^{-1},\ldots,f_{\Sigma_n}^{-1})\\
    &= f_\Sigma\circ g \circ (f_{\Gamma_i}^{-1}\circ f_{\Gamma_i}\circ h_i\circ
    (f_{\Sigma_1}^{-1},\ldots,f_{\Sigma_n}^{-1}))_i\\
    &= f_\Sigma\circ g \circ (f_{\Gamma_1}^{-1},\ldots,f_{\Gamma_p}^{-1})
    \circ (f_{\Gamma_i}\circ h_i\circ
    (f_{\Sigma_1}^{-1},\ldots,f_{\Sigma_n}^{-1}))_i\\
    &= \hat f \circ (\hat{g_i})_i
  \end{align*}
  la stabilité des fonctions RP par composition et l'hypothèse d'induction
  permettent donc de conclure.
\end{itemize}

On voit donc que le point réellement important de cette preuve est le cas de
la récursion primitive.

\begin{lemma}
  Soit $f$ une fonction RPG, alors $\hat f$ est une fonction RP.
\end{lemma}

\begin{proof}
  On procède par induction sur les fonctions RPG. On a prouvé juste avant les
  cas directs, on s'intéresse donc au cas de la récursion primitive.

  Soient des ensembles finis $\Sigma_1,\ldots,\Sigma_p,\Gamma,\Sigma$ et
  \[f : \left(\prod_{i = 1}^p (\Sigma_i)^\star\right)\to \Sigma^\star\qquad
  \forall a \in \Gamma, g_a : \left(\prod_{i = 1}^p
  (\Sigma_i)^\star\right)\times\Gamma^\star\times\Sigma^\star
  \to \Sigma^\star\]
  des fonctions RPG. On souhaite montrer que $\widehat{\rec(f,(g_a))}$ est
  une fonction RP sachant que $\hat f, (\hat{g_a})_{a\in \Gamma}$ sont
  des fonctions RP. On suppose que $\Sigma = \{0,\ldots,n-1\}$.

  On a vu comment extraire un élément de $\Sigma^\star$ à partir d'un
  entier. L'objectif dans cette construction va être d'adapter cette
  extraction pour l'appliquer directement à la récursion. Dans un premier
  temps, avec le même argument que dans celui pour construire $f_\Sigma$,
  on peut définir une fonction $\hat g$ telle que
  \[\hat g (x_1,\ldots,x_p,y,z) = \widehat{g_{z\% n}}(x_1,\ldots,x_p,y//n,z)\]
  en imbricant des instructions conditionnelles.

  On définit alors la fonction
  \[h(x_1,\ldots,x_p,x) \defeq
  \langle\pi'_1(x) // n,\hat g(x_1,\ldots,x_p,\pi'_1(x),\pi'_2(x))\rangle\]
  On rappelle que la fonction $d$, définie dans une preuve précédente, associe
  à $m$ la taille de $f_\Sigma^{-1}(m)$. On remarque l'identité suivante, pour
  tous $x_1,\ldots,x_p,y,z\in \mathbb N, u \in \Gamma^\star, a \in \Gamma$~:
  \[h(x_1,\ldots,x_p,y,\langle f_\Gamma(u\star a),z\rangle) =
  \langle f_\Gamma(u),g_a(x_1,\ldots,x_p,u,z)\rangle\]

  On définit alors la fonction $F$ à $p+2$ arguments suivante~:
  \[F \defeq
  \rec(\alpha\circ (\pi_{p+1}^{p+1},f\circ (\pi_1^{p+1},\ldots,\pi_{p+1}^{p+1})),
  h\circ (\pi_1^{p+3},\ldots,\pi_p^{p+3},\pi_{p+3}^{p+3}))\]
  Cette fonction va retourner des codes de couples. L'intérêt est de pouvoir
  utiliser l'appel récursif pour faire réduire l'argument autrement que d'un
  seul cran par un seul cran en stockant l'argument dans l'appel récursif,
  avec la valeur retournée. On montre donc d'abord par récurrence sur $k$ que
  pour tout $u = u_1\cdots u_N\in\Gamma^\star$,
  \[F(x_1,\ldots,x_p,f_\Gamma(u),k) = \langle f_\Gamma(u_1\cdots u_{N-k}),
  \widehat{\rec(f,(g_a))}(x_1,\ldots,x_p,f_\Gamma(u_N\cdots u_{N-k+1}))\rangle\]
  \begin{itemize}
  \item dans le cas où $k = 0$, les deux membres de l'équation sont
    effectivement égaux à
    $\langle f_\Gamma(u_1\cdots u_N),\hat f(x_1,\ldots,x_p)\rangle$.
  \item supposons que l'égalité est vraie pour $k$, alors
    \begin{align*}
      F(x_1,\ldots,x_p,f_\Gamma(u),k + 1) &=
      h(x_1,\ldots,x_p,F(x_1,\ldots,x_p,f_\Gamma(u),k))\\
      &= h(x_1,\ldots,x_p,\langle f_\Gamma(u_1\cdots u_{N-k}),\\
      &\widehat{\rec(f,(g_a))}(x_1,\ldots,x_p,f_\Gamma(u_N\cdots u_{N-k+1}))
      \rangle)\\
      &= \langle f_\Gamma(u_1\cdots u_{N-k-1}),\\
      &g_{u_{N-k}}(\widehat{\rec(f,(g_a))}(x_1,\ldots,x_p,
      f_\Gamma(u_N\cdots u_{N-k+1})))\\
      &= \langle f_\Gamma(u_1\cdots u_{N-k-1}),\\
      &\widehat{\rec(f,(g_a))}(x_1,\ldots,x_p,f_\Gamma(u_N\cdots u_{N-k}))
    \end{align*}
  \end{itemize}
  D'où le résultat par récurrence.

  On laisse en exercice le soin de vérifier qu'il est possible de transformer
  $f_\Gamma(u_N\cdots u_1)$ en $f_\Gamma(u_1\cdots u_N)$ par une fonction RP.
  En utilisant cette fonction, qu'on notera simplement $a \mapsto \overline a$,
  on construit la fonction~:
  \[G(x_1,\ldots,x_p,m) \defeq
  \pi'_2(F(x_1,\ldots,x_p,\overline m,d(m)))\]
  En utilisant l'identité précédente, on sait que
  \[G(x_1,\ldots,x_p,f_\Gamma(u)) =
  \widehat{\rec(f,(g_a))}(x_1,\ldots,x_p,f_\Gamma(u))\]
  d'où le résultat.

  Ainsi $\widehat{\rec(f,(g_a)_a)}$ est aussi une fonction RP, donc par
  induction sur les fonction RPG, pour toute fonction RPG $f$ la fonction
  $\hat f$ associée est RP.
\end{proof}

\begin{exercise}
  Vérifier que la fonction $a \mapsto \overline a$ est effectivement une
  fonction RP.
\end{exercise}

\begin{theorem}
  Soit une fonction $f : \mathbb N^n \to \mathbb N$. Cette fonction est
  RPG si et seulement si elle est RP.
\end{theorem}

\begin{proof}
  Il est clair que si $f$ est RP alors elle est RPG. Dans le sens réciproque,
  on sait que $\hat f$ est RP, donc
  \[\id\circ f \circ (\id,\ldots,\id) \in \RecP\]
  ce qui revient à dire que $f \in \RecP$, d'où le résultat.
\end{proof}

\begin{remark}
  Si nous avons jusque là donné des termes explicites, il doit être clair pour
  le lecteur que ces termes explicites sont de moins en moins lisibles, et les
  exemples précédents suffisent pour donner une idée fiable des manipulations
  autorisées dans le cadre des fonctions RP et RPG. Aussi, nous emploierons
  désormais des arguments moins formels, en essayant malgré tout d'être
  suffisamment précis pour ne pas oublier des subtilités techniques.
\end{remark}

Ainsi, les fonctions RPG n'apportent pas plus de puissance de calcul, puisque
les fonctions sur les entiers restent les mêmes. De plus, comme on possède un
codage bijectif bi-RPG entre chaque $\Sigma^\star$ et $\mathbb N$, on en conclut
que le pouvoir expressif de $\mathbb N$ est suffisant. L'utilité des fonctions
RPG est de permettre de facilement passer de fonctions sur des langages
algébriques à leur codage. Montrons d'abord qu'un
langage algébrique est une partie RPG.

Pour ce faire, nous introduisons un algorithme de \foreignexpr{parsing} de
langage algébrique. Un algorithme de \foreignexpr{parsing} est un algorithme
qui, étant donnée une grammaire $G$ et un mot $u$, retourne à la fois un test
du fait que $u$ dérive de la grammaire $G$, mais aussi une dérivation
produisant $u$ le cas échéant. L'algorithme de \foreignexpr{parsing} que nous
utilisons est celui d'Earley.

Donnons d'abord le formalisme adapté pour présenter l'algorithme.

\begin{definition}[Item d'Earley]
  Soit $\Sigma$ un alphabet et $G = (\Gamma, \to, S)$ une grammaire hors
  contexte sur $\Sigma$. On appelle item d'Earley les triplets
  $(R,i,j)$ où $R$ est une règle de $G$ (c'est-à-dire un couple
  $(A, u)$ tel que $A \to u$) et $i$ et $j$ sont des entiers~: on manipule
  des items d'Earley pour un mot global $u$ fixé, et ces deux indices sont
  alors inférieurs à $|u|$.

  On note $A \to \alpha \bullet \beta, i$ pour représenter l'item
  $(A \to \alpha\beta , i,j)$ où $j = |\alpha|$.
\end{definition}

L'idée derrière un item d'Earley est d'analyser les occurrences d'une règle
de production en considérant des facteurs du mot $u$ analysé~: un item
$(A \to \alpha\bullet \beta, i)$ s'interprète comme le fait que le sous-mot
$u_i\cdots u_{j-1}$ vaut $\alpha$, et qu'il permet donc de construire le
début de la règle $A \to u$.

L'algorithme de \foreignexpr{parsing} est un algorithme de programmation
dynamique~: on considère un tableau $T$ de $n$ lignes (où $n$ est la taille
du mot $u$ analysé) où chaque ligne contient une liste d'items d'Earley,
de sorte que la lecture du mot $u$ jusqu'à la lettre $j$ s'effectue dans la
ligne $T[i]$. On donne donc l'\cref{fig.Earley}.

\begin{figure}[h]
  \centering
  \begin{algorithm}[H]
    \KwData{\\
      un vecteur $T[\;]$ de taille $n$ contenant des ensembles d'items
      d'Earley, initialisé par $T[i] = \varnothing$ pour tout $i$\\
      un mot $u \in \Sigma^\star$ de longueur $n$ \\
      une grammaire $G = (\Gamma, \to, S)$ \\
      une liste $\ell$ contenant la suite des dérivations sous la forme
      d'un parcours en profondeur de l'arbre de dérivation
    }
    \KwResult{\\
      si $u \in \mathcal L_G$, la dérivation permettant de construire
      $u$ dans la grammaire $G$\\
      si $u \notin\mathcal L_G$, la constante $\bot$}
    \For{$\alpha$ tel que $S\to \alpha$}{
      $T[0] \leftarrow (S\to \bullet \alpha, 0)$\;
    }
    \For{$j = 1$ \KwTo $n$}{
      \For{$(A \to \alpha \bullet \beta, i) \in T[j-1]$}{
        \If{$\beta = u_{j-1} \gamma$}{
          $T[j] \leftarrow (A \to \alpha u_{j-1} \bullet \gamma, i)$\; }
      }
      \For{$(A \to \alpha\bullet \beta, i) \in T[j]$}{
        \If{$\beta = B\gamma$, $B \in \Gamma$}{
          \For{$(B, \delta) \in \to$}{
            $T[j]\leftarrow (B \to \bullet\delta, j)$\;
        }}
      }
      \For{$(A \to \alpha \bullet \beta, i) \in T[j]$}{
        \If{$\beta = \varepsilon$}{
          \For{$(B \to \gamma \bullet \delta, k) \in T[i]$}{
            \If{$\delta = A \delta'$}{
              $T[j]\leftarrow (B \to \gamma A \bullet \delta', k)$\;
            }
          }
        }
      }
    }
    \For{$(A \to \alpha\bullet\beta, i)\in T[n-1]$}{
      \If{$(A = S)\land (\beta = \varepsilon) \land (i = 0)$}{
        $\ell \leftarrow S$\;
        $\ell \leftarrow \alpha$\;
        \For{A FAIRE}{
          A FAIRE
        }
      }
    }
    \KwRet $\bot$\;
    \caption{Algorithme d'Earley}
    \label{fig.Earley}
  \end{algorithm}
  %\label{fig.Earley}
\end{figure}

\begin{proposition}
  Soit $\Sigma$ un alphabet et $G$ une grammaire hors contexte. Alors
  $\mathcal L_G$ est une partie de $\Sigma^\star$ dont l'indicatrice est
  une fonction RPG (à valeurs dans $\btwo$).
\end{proposition}

\begin{proof}
  Il nous suffit donc d'implémenter l'\cref{fig.Earley} avec des fonctions
  RPG. A FAIRE
\end{proof}

Cette proposition permet de donner du sens à la définition suivante.

\begin{definition}[Fonction RPG de langage]
  Soit $\Sigma,\Gamma$ deux alphabets et $\mathcal L \subseteq \Sigma^\star$ un
  langage RPG sur $\Sigma$. Une fonction $f : \mathcal L \to \Gamma^\star$
  est dite RPG s'il existe une fonction RPG
  $\tilde f : \Sigma^\star \to \Gamma^\star$ qui coïncide avec $f$ sur
  $\mathcal L$.
\end{definition}

\begin{remark}
  Nous décidons de contraindre $\mathcal L$ à être lui-même RPG car sinon,
  notre fonction $\tilde f$ n'aurait pas de raison de définir une fonction dont
  le domaine est $\mathcal L$~: appeler $\tilde f$ sur une valeur ne nous
  permet pas de savoir si l'application de la fonction était autorisée. Au
  contraire, s'il est possible d'avoir un processus préalable nous disant
  quelles valeurs peuvent être effectivement appelées, alors la fonction définie
  sur $\mathcal L$ est calculable par des fonctions RPG dans tous ses aspects.
\end{remark}

Un exemple essentiel de fonction RPG de langage est celui des fonctions
définies par inductions à partir de fonctions RPG.

\begin{proposition}
  Soient $\Sigma,\Gamma$ deux alphabets et $G = (\Sigma',\to,S)$ une
  grammaire non ambigüe. On suppose que pour toute règle
  $A \to u$ dans $G$, on dispose d'une
  fonction RPG $g_{A,u} : (\Gamma^\star)^n \to \Gamma^\star$ où $n$ est le nombre
  de non terminaux apparaissant dans $u$. Alors la fonction
  $g : \mathcal L_G \to \Gamma^\star$ donnée par la \cref{prop.PU.gram} est une
  fonction RPG.
\end{proposition}

\begin{proof}
  On utilise l'implémentation de l'\cref{fig.Earley}. On a donc une fonction
  $\Sigma^\star \to (\Sigma' \sqcup \{``[",'']"\})^\star$ donnant, pour un
  mot $u \in \Sigma^\star$, le mot $\varepsilon$ si $u \notin \mathcal L_G$ et
  la liste $[u_1,u_2,\cdots,u_k]$ obtenue en effectuant un parcours en
  profondeur de l'arbre de dérivation de $u$, si $u \in \mathcal L_G$.

  A FAIRE
\end{proof}

\begin{remark}
  De la même manière qu'on peut généraliser la \cref{prop.PU.gram} à des cas
  où les ensembles associés à chaque non terminal peuvent différer, cette
  proposition s'étend aussi à ces cas.
\end{remark}

Ces résultats nous permettent donc de voir les langages algébriques comme une
sous-classe des langages RPG. On voit aussi que des opérations syntaxiques
simples sont RPG. Par exemple, étant donnée une signature du premier ordre
$\Delta$, un terme $t$ sur $\Delta$, une variable du premier ordre $x_n$ avec
$n \in \mathbb N$ et une formule $\varphi$ sur $\Delta$, il est possible de
définir la substitution $\varphi[t/x_n]$ par induction sur la grammaire des
formules sur $\Delta$~:
\begin{itemize}
\item on définit d'abord la substitution pour un terme $t'$. Si $t' = x_m$ où
  $m \neq n$, alors $t'[t/x_n] = t'$ ; si $t' = x_n$ alors $t'[t/x_n] = t$ et
  si $t' = f(t_1,\ldots,t_p)$ alors
  $t'[t/x_n] = f(t_1[t/x_n],\ldots,t_p[t/x_n])$.
\item si $\varphi = R(t_1,\ldots,t_p)$, alors
  $\varphi[t/x_n]=R(t_1[t/x_n],\ldots,t_p[t/x_n])$.
\item le cas de $\top$ et $\bot$ est simple~: la substitution ne fait rien.
\item dans le cas de $\lor,\land,\to$, on applique simplement la substitution
  récursivement aux sous-formules.
\item si $\varphi = \exists x,\psi$ (respectivement
  $\varphi = \forall x, \psi$), alors $\varphi[t/x_n] = \exists x, \psi[t/x_n]$
  (respectivement $\varphi[t/x_n] = \forall x, \psi[t/x_n]$).
\end{itemize}

En réalité, le cas des quantificateurs est un peu particulier. On peut se
demander comment est représentée une variable liée telle que le $x$ dans
$\exists x, \psi$. La façon la plus simple de faire est de considérer que
cette quantification lie $x_0$, et donc que la variable libre $x_0$ dans
$\exists x, \psi$ est la variable libre $x_s(x_0)$ dans $\psi$. Dans ce cas,
la définition récursive reste simple~:
$\varphi[t/x_n] = \exists x, \psi[t/x_{n+1}]$.

Malheureusement, toutes les fonctions qui peuvent s'exprimer par un algorithme
ne sont pas des fonctions RP. Comme on l'a dit au début, une fonction RP peut
se voir comme un programme impératif n'utilisant que la boucle \texttt{for}. La
boucle \texttt{while}, elle, permet de faire des itérations arbitrairement
grandes, sans savoir \latinexpr{a priori} si on aura un nombre fini
d'itérations. Dans l'autre sens, il est possible de définir la boucle
\texttt{while} à partir de la boucle \texttt{for}, dans les cas où la première
boucle converge. Si l'on décide d'écrire
\[\texttt{while}\;\varphi(x) \;\texttt{do}\; f ; x \coloneq x + 1\;
\texttt{done}\]
par exemple, pour appliquer une procédure $f$ pour tous les $x$ tels que
$\varphi(x)$, on peut tout aussi bien écrire, en fait,
\[\texttt{for}\; i = 0 \;\texttt{to}\; \mu(\lnot \varphi)\;\texttt{do}\; f\;
\texttt{done}\]
Ainsi, la minimisation non bornée nous permet de simuler les boucles
arbitrairement grandes.

Nous définissons donc une classe de fonctions plus générales, les fonctions
récursives (et récursives généralisées) en ajoutant cet opérateur. Plutôt que de
le présenter par minimisation d'un prédicat, on va le présenter à partir d'une
fonction, en considérant le prédicat \og la fonction s'annule en ce point\fg qui
ne fait pas perdre de généralité (puisqu'un prédicat RP est déjà une fonction
valant $0$ ou $1$).

Comme nous allons introduire des classes de fonctions partielles, nous donnons
d'abord des notations pour manipuler ces fonctions.

\begin{notation}
  Soit $f : E \partialto F$ une fonction partielle. Pour tout $x \in E$, on
  notera $f(x)\convcal$ dans le cas où $f$ est définie en $x$, et on
  notera $f(x)\divcal$ dans le cas où $f$ n'est pas définie~:
  \[f(x) \convcal \defeq \exists y \in F, f(x) = y \qquad
  f(x) \divcal \defeq \lnot(f(x)\convcal)\]
  On dira donc que $f$ est totale si $f(x)\convcal$ pour tout $x \in E$.

  Etant données deux fonctions partielles $f,g : E \partialto F$, on notera
  $f \convcal = g\convcal$ pour signifier que $f$ et $g$ sont définies
  exactement sur les mêmes valeurs et sont égales sur ces valeurs.
\end{notation}

\begin{definition}[Fonction récursive, récursive généralisée]
  On définit la classe $\Rec$ des fonctions récursives comme la plus petite
  classe de fonctions partielles stables par les mêmes opérations
  que la classe $\RecP$ et telle que si
  $f : \mathbb N^{n+1} \partialto \mathbb N$ est une fonction partielle
  récursive, alors
  c'est aussi le cas de la fonction
  \[\begin{array}{ccccc}
  \mu(f) & : & \mathbb N^n & \xrightharpoonup{\quad} & \mathbb N\\
  & & (x_1,\ldots,x_n) &\longmapsto & \min \left\{y \in \mathbb N \left\vert
  \begin{array}{c} f(x_1,\ldots,x_n,y)\convcal = 0 \\ \land \\
  \forall z < y, f(x_1,\ldots,x_n,z)\convcal \neq 0\end{array}\right.\right\}
  \end{array}\]
  où la fonction $\mu(f)$ est indéfinie là où l'ensemble dont on prend le
  minimum est infini.

  La classe $\RecG$ des fonctions récursives généralisées est la plus petite
  classe de fonctions partielles contenant la classe $\RPG$ et telle que si
  \[f : \left(\prod_{i = 1}^n (\Sigma_i)^\star\right)
  \times \mathbb N \to \Sigma^\star\]
  est une fonction récursive généralisée, alors
  \[\begin{array}{ccccc}
  \mu(f) & : & \displaystyle\prod_{i = 1}^n (\Sigma_i)^\star &\longrightarrow &
  \mathbb N\\
  & & (u_1,\ldots,u_n) &\longmapsto &\min\{y \in \mathbb N \mid
  f(u_1,\ldots,u_n,y) = \varepsilon\}
  \end{array}\]
  est aussi une fonction récursive généralisée.
\end{definition}

\begin{exercise}
  Montrer que pour les fonctions $f : \mathbb N^n \to \mathbb N$, appartenir à
  $\Rec$ est équivalent à appartenir à $\RecG$.
\end{exercise}

La classe $\Rec$ est en fait la classe des fonctions calculables, au sens des
fonctions que l'on peut écrire comme des programmes informatiques. Dans la
section qui suit, on s'intéresse à un autre formalisme~: celui des machines de
Turing. Nous reviendrons à la classe $\Rec$ dans la dernière sous-section de la
section suivante, pour prouver son équivalence avec la classe que l'on définira
grâce aux machines de Turing.

\section{Machines de Turing}

Les machines de Turing, introduites dans \cite{turing1936a}, sont un formalisme
très proche de celui des automates. La différence principale est qu'au lieu
de lire linéairement un mot, comme dans le cas d'un automate fini ou d'un
automate à pile, une machine de Turing va agir sur un ruban de données. Cette
action permet d'avoir des calculs arbitrairement longs, et le ruban de données
est infiniment grand pour permettre d'effectuer des calculs aussi grands que
souhaité. L'action sur le ruban, elle, sera déterminée par une structure
quasiment identique à celle d'un automate~: à chaque instant, une tête de
lecture indique à quelle place sur le ruban on effectue l'action, et l'action
dépend de la lettre lue et de l'état de la machine. La différence principale est
que la machine peut alors décider de continuer la lecture (vers la droite) ou
bien de revenir en arrière (vers la gauche). Ce formalisme est un prototype de
langage impératif, qui agit sur une bande de mémoire selon un programme écrit à
l'avance.

Dans cette section, on définit le formalisme élémentaire pour traiter des
machines de Turing. Nous montrons ensuite que différents choix sont possibles
pour définir les machines de Turing~: utiliser plusieurs rubans, considérer des
états acceptant/rejetant, utiliser des machines non déterministes\ldots Enfin,
nous montrons que les fonctions calculables au sens des machines de Turing sont
équivalentes aux fonctions récursives.

\subsection{Premiers pas dans le formalisme des machines de Turing}

Donnons d'abord une définition simple de machine de Turing, permettant de plus
rapidement travailler avec.

\begin{definition}[Machine de Turing]
  Soit $\Sigma$ un alphabet. Une machine de Turing $M$ sur $\Sigma$ est la
  donnée de~:
  \begin{itemize}
  \item un ensemble fini $Q$ d'états
  \item un état initial $q_0$
  \item un ensemble d'états d'arrêt $F$
  \item un alphabet $\Gamma$ de travail, tel que $\Sigma \subseteq \Gamma$
  \item un symbole blanc $\square \in \Gamma$
  \item une fonction de transition partielle
    \[\delta : Q \times \Gamma \longrightarrow Q \times \Gamma \times
    \{\lhd,\rhd\}\]
  \end{itemize}

  On note $\TM(\Sigma)$ (pour \foreignexpr{Turing machine}) l'ensemble des
  machines de Turing sur $\Sigma$.
\end{definition}

Décrivons le fonctionnement d'une machine de Turing à partir de ses
composantes~:
\begin{itemize}
\item la machine de Turing va agir sur un ruban qui est une suite de symboles
  de $\Gamma$ de la forme $u\square^\infty$, c'est-à-dire ayant un nombre
  fini d'élément différents de $\square$. A chaque étape, la machine pointe à
  un certain indice du ruban, lit le symbole à cet indice et agit en
  conséquence.
\item au début de l'exécution, la machine point à l'incide $0$ et est dans
  l'état $q_0$.
\item à chaque étape lors de l'exécution, en supposant que le ruban est de la
  forme $uav\square^\infty$ et que la machine pointe sur le symbole $a$, la
  machine étant en un état $q$, on regarde $\delta(q,a) = q',b,m$~: le symbole
  $a$ est remplacé par le symbole $b$, la machine entre dans l'état b et elle
  décale sa tête de lecture d'une case vers la gauche si $m = \lhd$, vers la
  droite si $m = \rhd$.
\item lorsque la machine entre dans un état $q \in F$, l'exécution s'arrête.
\end{itemize}

Donnons un exemple de machine et d'exécution d'une machine.

\begin{example}[Miroir d'un mot]
  On fixe l'alphabet $\Sigma \defeq \btwo$ et on donne une machine de Turing
  qui, étant donné un mot en entrée, réécrit ce mot en miroir.

  On en donne d'abord une description intuitive~:
  \begin{itemize}
  \item on commence par déplacer le mot $u$ vers la droite et on le
    précède d'un $0$, pour avoir $\square^{|u|}0u\square^\infty$ sur la
    bande
  \item on va tout à droite du mot $u$ de droite
  \item on répète la procédure suivante~:
    \begin{itemize}
    \item on garde en mémoire le caractère sur lequel on est, qu'on
      notera $a$ (on peut multiplier les états pour avoir une suite
      d'état distinct pour chaque symbole $a$ de l'alphabet), on
      efface de caractère
    \item on se déplace à gauche jusqu'à atteindre un $\square$, puis
      une lettre ($0$ ou $1$)
    \item on se déplace à droite d'une case (de sorte à être sur le
      premier $\square$) et on y place $a$
    \item on se déplace à droite jusqu'à lire un $0$
    \item on se déplace d'une case à droite~: si la case contient
      $\square$ alors on arrête la procédure~; sinon on la recommence
    \end{itemize}
  \item on efface le $0$
  \end{itemize}
\end{example}

A FAIRE : SCHEMA DE LA SITUATION EN TIKZ

Rendons maintenant ce fonctionnement formel. On définit d'abord la notion de
configuration, qui est un objet permettant de retenir simultanément l'état d'une
machine, de son ruban et l'emplacement de sa tête de lecture.

\begin{definition}[Configuration d'une machine sur un ruban]
  Soit une machine de Turing $M$ sur un alphabet $\Sigma$. Une configuration
  pour $M$ est une suite $u \in (\Gamma\cup Q)^\mathbb N$ telle que~:
  \begin{itemize}
  \item toutes les valeurs de $u$ sauf un nombre fini sont $\square$
  \item il existe exactement un indice $i$ tel que $u_i \in Q$.
  \end{itemize}
  Etant donnée une configuration $u$, on appel état de $M$ en $u$ l'unique
  valeur $q \in Q$ prise par $u$.
\end{definition}

\begin{notation}
  On notera une configuration $uqv\square^\infty$ pour indiquer que
  $u \in \Gamma^\star$, $v \in \Gamma^\star$ et $q \in Q$.
\end{notation}

Il faut lire la configuration $uqv\square^\infty$ comme le fait que la tête
de lecture de la machine se trouve à la première lettre de $v$, dans l'état $q$,
et que tous les symboles différents de $\square$ sont contenus dans $uv$.

Une machine de Turing agit alors naturellement sur les configurations grâce à sa
fonction de transition.

\begin{definition}[Action d'une machine sur une configuration]
  Soit $M$ une machine de Turing sur un alphabet $\Sigma$ et
  $uqav\square^\infty$ une configuration pour $M$
  (où $u,v \in \Gamma^\star, a\in \Gamma$ et $q \in Q$). Soit
  $q',b,m \defeq \delta(q,a)$, on définit l'action de $M$ en un pas sur
  $uqbv\square^\infty$, par
  \[
  M\cdot uqav\square^\infty \defeq
  \begin{cases}
    u'q'a'bv\square^\infty\text{ si } m = \lhd\text{ et si } u = u'\star a'\\
    ubq'v\square^\infty\text{ si } m = \rhd \\
    \text{indéfini sinon}
  \end{cases}\]
\end{definition}

\begin{remark}
  Si $\delta$ n'est pas définie en un certain couple $a,q$, l'action n'est pas
  définie non plus, comme l'indique l'impossibilité de décomposer $\delta(q,a)$.
\end{remark}

Cette action sur les configurations représente ce qu'il se déroule lors de
l'exécution de $M$ pendant une seule unité de temps de calcul. Il nous faut
ensuite, bien sûr, itérer cette action pour créer un vrai calcul de la part de
$M$. Comme dans le cas des automates, on va appeler trace d'un calcul la suite
des configurations parcourues.

\begin{definition}[Trace d'une exécution d'une machine de Turing]
  Soit une machine de Turing $M$ sur un alphabet $\Sigma$ et
  $uqv\square^\infty$ une configuration pour $M$. On appelle trace de
  l'exécution de $M$ sur $uqv\square^\infty$ la plus grande suite finie ou
  infinie de configurations $p_0,\ldots,p_i,\ldots$ telle que
  \begin{itemize}
  \item $p_0 = uqv\square^\infty$
  \item pour tout $i$, $p_{i+1} = M\cdot p_i$
  \item s'il existe $p_i$ tel que l'état de $M$ en $p_i$ appartient à $F$,
    alors la trace n'est pas définie pour $j > i$
  \end{itemize}

  Pour tout mot $u \in \Sigma^\star$, on appelle trace de l'exécution de $M$
  la trace de l'exécution de $M$ sur $q_0u\square^\infty$.
\end{definition}

L'exécution d'une machine sur un mot $u$ peut ainsi mener à plusieurs cas~:
\begin{itemize}
\item la trace est infinie, ce qui correspond à un calcul qui ne se termine pas,
  tel un programme exécutant une boucle de la forme \texttt{while true} ;
\item la trace est finie mais ne contient pas d'état d'arrêt, ce qui correspond
  à une dernière configuration pour laquelle l'action de $M$ est indéfinie ;
\item la trace est finie et contient un état d'arrêt, ce qui correspond à un
  calcul qui s'exécute correctement et s'arrête~: le ruban dans la configuration
  finale est alors de la forme $vw\square^\infty$ où $v \in \Sigma^\star$ et
  $w \in \Gamma^\star$ ($v$ peut être vide, et $w$ contient potentiellement
  des lettres dans $\Sigma)$. On a alors associé à $u\in\Sigma^\star$ un mot
  $v \in \Sigma^\star$. On notera dans la suite $M(u)$ ce mot $v$.
\end{itemize}

Une machine de Turing définit donc une fonction partielle
$\Sigma^\star \partialto \Sigma^\star$~: on dira que cette fonction est calculée
par la machine de Turing considérée, ce qui nous mène à la notion de fonction
calculable.

\begin{definition}[Fonction calculée par une machine de Turing]
  Soit $M$ une machine de Turing sur un alphabet $\Sigma$. On définit la
  fonction partielle calculée par $M$ comme~:
  \[\begin{array}{ccccc}
  f_M & : & \Sigma^\star & \longrightarrow & \Sigma^\star\\
  & & u & \longmapsto & M(u)
  \end{array}\]
  où $M(u)$ est indéfini si la trace de l'exécution de $M$ sur $u$ est infinie
  ou ne se termine pas sur un état d'arrêt.
  
  On définit la classe des fonctions calculables sur $\Sigma$ comme
  \[\Calc(\Sigma)\defeq \{ f_M\mid M \in \TM(\Sigma)\}\]
\end{definition}

\subsection{\'Equivalence de machines de Turing}

A FAIRE

\subsection{\'Equivalence avec les fonctions récursives}

Pour montrer que les fonctions calculables sont équivalentes aux fonctions
récursives généralisées, deux sens sont à faire~: montrer qu'il est possible
d'écrire une machine de Turing qui calcule une fonction $\RecG$, et montrer
qu'il est possible de simuler l'exécution d'une machine de Turing par une
fonction $\RecG$.

Le premier sens demande de raisonner par induction~: on montre que les
constructions définissant la classe $\RecG$ peuvent se faire avec des machines
de Turing. Le sens réciproque, lui, va se faire en deux temps. Tout d'abord, on
prouve qu'il existe deux fonctions $T,U \in \RPG$ qui, étant donnés un code
$e$ donnant la description d'une machine de Turing $M$, une entrée $x$ et un
temps $t$, retourne respectivement si la machine de Turing s'est arrêtée en
moins de $t$ étapes de calcul sur l'entrée $x$ et l'état du ruban après
l'exécution de $M$ sur $x$ pendant $t$ étapes de calcul. En utilisant alors la
minimisation non bornée, on obtient une simulation des machines de Turing par
les fonctions $\RecG$, et on remarque alors qu'une fonction $\RecG$ ne
nécessite qu'une utilisation de la minimisation non bornée par rapport à une
fonctions $\RPG$.

On montre d'abord le sens direct.

\begin{lemma}\label{lem.recG.MT}
  Pour toute fonction récursive généralisée $f$, il existe une machine de
  Turing telle que $f_M = f$.
\end{lemma}

\begin{proof}
  \'Etant donné que tous les formalismes de machines de Turing sont équivalents,
  on en choisit un adapté au lemme que l'on souhaite prouver~: une machine de
  Turing est prise avec $k$ rubans d'entrée en lecture seule, chacun sur un
  alphabet distinct, un ruban de sortie en écriture seule sur un alphabet encore
  distinct et un nombre arbitraire de rubans de travail en lecture et écriture
  sur un alphabet contenant tous les alphabets précédents.

  On prouve par induction sur la classe des fonctions $\RecG$ la proposition~:
  \begin{itemize}
  \item pour une fonction $\pi_k^n$, il suffit de considérer la machine qui
    recopie le contenu du $\ordinalnumeralmale{k}$ ruban d'entrée sur le ruban
    de sortie.
  \item pour un alphabet $\Sigma$ et $a \in \Sigma$, la fonction $\cons_a$ est
    implémentée en recopiant le mot d'entrée jusqu'au premier blanc et, au
    premier blanc, en écrivant $a$.
  \item la fonction s'arrêtant immédiatement sans lire l'entrée retourne
    la constante $\varepsilon$.
  \item on suppose données des fonctions $f_k$ d'arité
    $\prod_{i = 1}^n (\Sigma_i)^\star \to (\Gamma_k)^\star$ encodées par des
    machines à $n$ rubans d'entrée et $a_k$ rubans de travail, et une fonction
    $g : \prod_{i = 1}^p (\Gamma_i)^\star \to \Sigma$ encodée par une machine à
    $p$ rubans d'entrée et $a$ rubans de travail, et on souhaite montrer qu'il
    existe une machine de Turing à $n$ rubans d'entrées et
    $\sum a_k + a + p$ rubans de travail encodant $g\circ (f_1,\ldots,f_p)$.

    On décrit le comportement de la machine, sans en donner explicitement les
    états. Tout d'abord, on numérote les rubans de travail~: on a pour chaque
    $k \in \{1,\ldots,n\}$ les rubans $T_{k,j}$ correspondant aux rubans de
    travail de la machine simulant $f_k$, le ruban $T_j$ correspondant au
    ruban de travail de la machine simulant $g$, et les rubans
    $S_k$ correspondant aux rubans de sortie de la machine simulant $f_k$.
    On construit alors la machine de sorte à avoir le comportement suivant~:
    \begin{itemize}
    \item pour tout $k \in \{1,\ldots,p\}$, la machine implémente $f_k$ en
      utilisant les rubans d'entrée, les rubans $T_{k,j}$ pour ses rubans de
      travail, et écrit sa sortie dans le ruban $S_k$
    \item la machine implémente $g$ en utilisant les rubans $S_k$ comme rubans
      d'entrée, les rubans $T_j$ comme rubans de travail, et écrit sa sortie
      dans le ruban de sortie
    \end{itemize}

    Dans le déroulement d'une telle machine, la première étape assure que
    dans le ruban $S_k$ se trouve la valeur de $f_k(u_1,\ldots,u_n)$, et que
    la valeur de sortie est l'image par $g$ des valeurs contenus dans les
    rubans $S_k$, d'où le fait que notre machine simule
    $g\circ(f_1,\ldots,f_p)$.
  \item soit une fonction $f : \prod_{i =1}^p (\Sigma_i)^\star \to \Sigma^\star$
    et, pour chaque $a \in \Gamma$,
    $g_a : \prod_{i = 1}^p (\Sigma_i)^\star \times
    \Gamma^\star \times \Sigma^\star \to \Sigma^\star$.
    On suppose par hypothèse d'induction que toutes ces fonctions sont simulées
    par des machines de Turing. On souhaite montrer que la machine encodant
    $\rec(f,(g_a)_{a \in \Gamma})$ est simulée par une machine de Turing.

    Explicitons encore les rubans de travail dont nous avons besoin. On fixe
    d'abord des rubans $T_j$ correspondant aux rubans de travail de $f$, et
    des rubans $T_{a,j}$ correspondant aux rubans de travail de chaque
    $g_a$, ainsi qu'un ruban de travail supplémentaire $T$. On ajoute, de plus,
    un ruban de comptage $C$, un ruban de mémoire $M$, un ruban d'appel
    récursif $AR$ et un ruban de copie $CC$.
    On décrit le comportement de la machine simulant cette fonction~:
    \begin{itemize}
    \item on recopie le dernier ruban d'entrée, contenant l'argument sur lequel
      on effectue le calcul récursif, dans le ruban $C$.
    \item on simule la fonction $f$ sur les rubans d'entrée à l'exception du
      dernier, avec comme rubans de travail $T_j$ et comme ruban de sortie
      $AR$.
    \item on va tout à gauche du ruban $C$, et on effectue les opérations
      suivantes tant qu'on ne lit pas un $\square$ sur $C$~:
      \begin{itemize}
      \item on lit le caractère dans $C$, qu'on fixera comme valant $a$, et
        qu'on efface de $C$
      \item on simule la fonction $g_a$ en prenant comme rubans d'entrée~:
        \begin{itemize}
        \item les rubans d'entrée à l'exception du dernier,
        \item le ruban de mémoire $M$ et
        \item le ruban d'appel récursif $AR$
        \end{itemize}
        comme rubans de travail les rubans $T_{a,j}$ et comme ruban de sortie le
        ruban $CC$.
      \item on recopie le contenu de $CC$ dans $AR$ (en ayant effacé $AR$ avant)
        et on efface le contenu de $CC$.
      \item on ajoute le caractère $a$ à $M$.
      \item on se déplace d'une case à droite dans $C$.
      \end{itemize}
    \item on recopie le contenu de $AR$ sur le ruban de sortie.
    \end{itemize}
    
    L'itération sur $C$ commence en ayant dans le ruban $AR$ la valeur de
    $f(u_1,\ldots,u_p)$ (où $u_1,\ldots,u_p$ sont le contenu des premiers rubans
    d'entrée). A chaque étape de cette itération, on déplace une lettre de
    $C$ dans $M$, de sorte que l'appel sur $g_a$ se fait en ayant en paramètre
    le mot $u$ lu jusqu'à présent et dans la bande $AR$ le résultat de
    $\rec(f,(g_a)_{a\in \Gamma})$ sur le mot $u$. Ainsi, la machine simule bien
    la fonction $\rec(f,(g_a)_{a\in \Gamma})$.
    
  \item On suppose donnée une fonction
    $f : \prod_{i = 1}^n (\Sigma_i)^\star \times \bN \to \Sigma^\star$
    simulée par une machine à $n+1$ rubans d'entrée et $k$ rubans de travail.
    Montrons que $\mu(f)$ peut être simulé par une machine à $n$ rubans
    d'entrée et $k + 2$ rubans de travail.

    Décrivons les rubans. Tout d'abord, on a les rubans $T_i$ correspondant aux
    rubans de travail de $f$. On ajoute à cela le ruban de comptage $C$ et un
    ruban de mémoire $M$.
    Le comportement de la machine est le suivant~:
    \begin{itemize}
    \item on effectue la boucle suivante (dont on précisera dans la boucle la
      condition pour en sortir)~:
      \begin{itemize}
      \item on simule $f$ en prenant comme entrée les rubans d'entrée et
        le ruban $C$, avec comme rubans de travail les rubans $T_i$ et comme
        ruban de sortie $M$.
      \item si $M$ est vide, on s'arrête en copiant $C$ dans le ruban de
        sortie.
      \item sinon, on ajoute un symbole dans $C$, on efface le contenu de
        $M$ et on revient au début de la boucle.
      \end{itemize}
    \end{itemize}

    Sur les $k$ premières itérations de la boucle, on obtient dans le ruban $M$
    la valeur de $f(u_1,\ldots,u_n,k)$ au moment de tester si la valeur en
    question est $0$. Ainsi, si la boucle se termine, alors on a trouvé une
    valeur d'annulation (la plus petite) de $f(u_1,\ldots,u_n,\_)$. Sinon,
    cela signifie au choix que la boucle est infinie ou que la simulation de
    $f$ est elle-même infinie.
  \end{itemize}

  Par induction sur la structure de $\RecG$, on en déduit que toute fonction
  récursive générale peut être simulée par une machine de Turing.
\end{proof}

Pour montrer le sens réciproque, on doit d'abord décider d'une façon de coder
une machine de Turing $M$ sur un alphabet $\Sigma$ par un mot.

D'après le LEMME NORMALISATION MACHINE DE TURING, il nous suffit de considérer
une machine à $k$ entrées sur les alphabets $\Sigma_1,\ldots,\Sigma_k$, un ruban
de sortie sur un alphabet $\Sigma$ et $p$ rubans de travail sur l'alphabet
$\{*\}$, dont l'ensemble des états est $\{0,\ldots,N\}$ pour un certain entier
$N\in \bN$ et tel que $q_0 = 0$ et $q_a = N$.

Un code d'une telle machine est donc la donnée des nombres $N,p$ (les alphabets
$\Sigma_1,\ldots,\Sigma_k,\Sigma$ étant fixés ici) et de la table de transition
encodée en tant que mot sur
$\displaystyle\left(\bigcup_{i = 1}^k \Sigma_i\right)\cup \Sigma$. En réalité,
il nous suffit de donner la table de transition de la machine par des entiers,
puis composer des fonctions $f_\Sigma$ telles que décrites dans la
\cref{def.trad.RPG} On donne donc un codage de la fonction de transition par un
entier.

\begin{definition}[Codage d'une fonction de transition]
  Soient $\Sigma_1,\ldots,\Sigma_k, \Sigma$ des alphabets. Soit $M$ une machine
  de Turing sur ces alphabets à $p$ rubans de travail sur $\{*\}$ et d'états
  $\{0,\ldots,N\}$.

  On se donne une énumération de la fonction de
  transition
  \[\delta : Q \times \left(\prod_{i = 1}^{k} (\Sigma_i \sqcup \{\square\})\right)
  \times \{*,\square\}^p \longrightarrow
  Q \times \{*,\square\}^p \times \Sigma \times \{\lhd,\rhd\}^{k+p+1}\]
  par la famille
  \[\delta = \{\langle q_i,a_{i,1},\ldots,a_{i,k},b_{i,1},\ldots,b_{i,p}\rangle
  \longrightarrow \langle q'_i, b'_{i,1},\ldots,b'_{i,p}, a',
  d_{i,1},\ldots,d_{i,k+p+1}\rangle \}_{i \in I} \]
  où $I$ est un ensemble fini. On considère le mot obtenu en concaténant tous
  les éléments de cette famille, pris comme des mots sur
  \[\mathcal L_{\mathrm{code}} \defeq
  Q \cup \left(\bigcup_{i = 1}^k \Sigma_i\right) \cup \Sigma \cup
  \{``,", ``\langle",``\rangle"\} \cup\{\lhd,\rhd\}\cup \{\longrightarrow\}\]
  où chaque terme de cette union est supposé (sans perte de généralité)
  distinct des autres termes, à l'exception potentielle des $\Sigma_i$ et de
  $\Sigma$. On note ce mot $\godcod\delta$.
\end{definition}

\begin{definition}[Codage d'une machine de Turing]
  Soit $\Sigma_1,\ldots,\Sigma_k,\Sigma$ des alphabets. Une machine de Turing
  $M$ sur ces alphabets, dont les rubans de travail sont $p$ rubans sur $\{*\}$
  et dont les états sont $\{0,\ldots,N\}$, $0$ étant l'état initial et
  $N$ l'état d'arrêt, est encodée par le nombre
  \[\godcod M \defeq \langle N,p,
  f_{\mathcal L_{\mathrm{code}}}(\godcod \delta)\rangle\]
\end{definition}

Le point central de ce codage est le suivant~: toutes les fonctions simulant
l'exécution de $M$ sur des rubans donnés peuvent s'écrire de façon RPG. On peut
résumer ce fait par la mise sous forme normale de Kleene~: on peut trouver deux
fonctions RPG qui, respectivement, détermine si l'état de la machine en un
temps $t$ est l'état d'arrêt et affiche le contenu du ruban de sortie au bout
de $t$ étapes de calcul.

\begin{theorem}[Forme normale de Kleene]\label{thm.norm.Kleene}
  Soient des alphabets
  $\Sigma_1,\ldots,\Sigma_n, \Gamma$. Il existe deux fonctions RPG
  \[T : \bN \times \left(\prod_{i = 1}^n (\Sigma_i)^\star\right)
  \times\bN \to \btwo \qquad
  U : \bN \times \left(\prod_{i = 1}^n (\Sigma_i)^\star \right)
  \times \bN \to \Gamma^\star\]
  et une fonction
  \[\godcod - : \TM(\Sigma_1,\ldots,\Sigma_n,\Gamma) \to \bN\]
  telles que pour toute machine de Turing
  $M \in \TM(\Sigma_1,\ldots,\Sigma_n,\Gamma)$~:
  \begin{itemize}
  \item $T(\godcod M,u_1,\ldots,u_n,k) = 1$ si et seulement si la machine
    $M$ s'arrête sur les entrées $u_1,\ldots,u_n$ en moins de $k$ étapes
    de calcul
  \item $U(\godcod M,u_1,\ldots,u_n,k)$ est le contenu du ruban de sortie de
    $M$ au bout de $k$ étapes de calcul.
  \end{itemize}
\end{theorem}

\begin{proof}
  Tout d'abord, puisque les réciproques de la bijection de Cantor sont des
  fonctions RPG, on peut extraire de $\godcod M$ la valeur de $N,p$ et
  la table de transition $\delta$ prise comme un mot contenant chaque
  transition. Remarquons que, si $\delta$ est donnée sous la forme de
  $f_{\mathcal L_{\mathrm{code}}}(\godcod \delta)$, il est possible de retrouver
  $\godcod\delta$ à partir de cette valeur puisque la fonction réciproque est
  une fonction RPG.

  De plus, étant donné le mot $\godcod \delta$, un état $q \in Q$ des
  lettres $a_1,\ldots, a_k \in \prod \Sigma_i$ et des lettres
  $b_1,\ldots,b_p \in \{*,\square\}^p$, il est possible de récupérer le mot
  $\langle q,a_1,\ldots,a_k,b_1,\ldots,b_p \rangle \longrightarrow
  \langle v \rangle$
  de $\godcod \delta$ correspondant
  par une fonction RPG~: il suffit de lire $\godcod\delta$ en essayant de
  reconnaître le mot de gauche.

  Il nous suffit donc d'établir comment coder une configuration, pour pouvoir
  simuler l'action de $M$ sur une configuration. \'Etant donnés les
  $N+p+1$ rubans, de la forme $u_i\square^\infty$, pour des positions
  $a_i$de la machines aux différents rubans et pour un état $q$, on écrit la
  configuration par
  \[\godcod C \defeq q|a_0|u_0|a_2|u_2|a_3|\cdots |a_{N+p}| u_{N+p}\]
  où $|$ est un nouveau symbole.

  L'action en une étape de calcul de $M$ sur une configuration consiste alors
  à extraire l'état $q$ et
  la \ordinalnumeralfeminin{$a_i$} lettre du mot $u_i$, puis de
  reconnaître le mot $v$ de $\godcod\delta$ obtenu à la fin de la transition
  contenant ces lettres. Si aucune telle transition est trouvée, on retourne un
  état de blocage (que l'on peut encoder en dur, par exemple en faisant en sorte
  que $T$ retourne la valeur $2$ plutôt que $0$ ou $1$). Ensuite, on modifie
  les configurations de façon adéquate avec les symboles donnés en sortie de
  la transition, en augmentant ou diminuant les $a_i$ en fonction de si le
  caractère $d_i$ est $\lhd$ ou $\rhd$.

  On peut ainsi obtenir une fonction qui, pour une configuration $C$ donnée,
  calcule l'action de $M$ sur $C$ en $t$ étapes de calcul. On peut, de plus,
  construire de façon RPG une configuration pour des mots $u_1,\ldots,u_k$
  donnés en entrée. Cela nous permet donc d'obtenir une fonction $RPG$ qui,
  étant donnés des mots d'entrée et un temps de calcul $t$, retourne la
  configuration résultante de $t$ étapes de calcul sur la configuration
  $0|u_1|0|u_1|\cdots |0|u_k|0|\varepsilon|0|\varepsilon|\cdots|0|\varepsilon$
  (configuration initiale pour le calcul de $u_1,\ldots,u_k$).

  La fonction $T$ est alors obtenue en récupérant l'état $q$ de la
  configuration et en testant si $q = 0$. La fonction $U$, elle, renvoie
  simplement le mot de la configuration, en supprimant les $\square$
  potentiels écrits lors du calcul.
\end{proof}

On en déduit donc le théorème de Church-Turing.

\begin{theorem}[Church-Turing]
  Pour tous alphabets $\Sigma_1,\ldots,\Sigma_n,\Gamma$, on a l'égalité
  \[\RecG(\Sigma_1,\ldots,\Sigma_n,\Gamma) =
  \Calc(\Sigma_1,\ldots,\Sigma_n,\Gamma)\]
\end{theorem}

\begin{proof}
  Par le \cref{lem.recG.MT}, on en déduit l'inclusion $\RecG\subseteq\Calc$.
  Pour la réciproque, on pose $M \in \Calc(\Sigma_1,\ldots,\Sigma_n,\Gamma)$.
  D'après le \cref{thm.norm.Kleene}, on peut donc considérer la fonction
  suivante, qui est $\RecG$ par construction~:
  \[(u_1,\ldots,u_n) \longmapsto U(\godcod M,u_1,\ldots,u_n,\mu(t \mapsto
  T(\godcod M,u_1,\ldots, u_n,t)))\]
  Qui retourne exactement $f_M(u_1,\ldots,u_n)$ si $f_M$ converge en ce point,
  et n'est pas définie sinon. Cette fonction calculable est donc une fonction
  récursive générale.
\end{proof}

Cette preuve apporte aussi un corollaire essentiel, que l'on énonce comme
théorème à cause de son importance.

\begin{theorem}[Machine universelle]\label{thm.MT.U}
  Pour tous alphabets $\Sigma_1,\ldots,\Sigma_n,\Gamma$, il existe une machine
  de Turing $\mathcal U_{\Sigma_1,\ldots,\Sigma_n,\Gamma}$ (que l'on écrira simplement
  $\mathcal U$) telle que
  \[\forall M \in \TM(\Sigma_1,\ldots,\Sigma_n,\Gamma),
  \forall (u_1,\ldots, u_n) \in \prod_{i = 1}^n (\Sigma_i)^\star,
  f_M(u_1,\ldots,u_n) = f_\mathcal U(\godcod M, u_1,\ldots,u_n)\]
\end{theorem}

\begin{proof}
  La fonction donnée précédemment étant une fonction récursive générale, on peut
  trouver une machine de Turing qui la simule~: cette machine est donc une
  machine $\mathcal U$ vérifiant la propriété souhaitée.
\end{proof}

\subsection{Thèse de Church-Turing}

Pour clore ce chapitre, nous partageons la thèse de Church-Turing, d'un ordre
plus philosophique que mathématique. Malgré son statut informel, cette thèse
est cruciale dans l'appréciation de la calculabilité et dans sa pratique.

\begin{thesis}[Church-Turing]\label{thesis.CT}
  Tout algorithme reposant sur des
  méthodes \emphexpr{finitaire}, et généralement toute fonction intuitivement
  calculable, peut s'exprimer sous la forme d'une machine de Turing (de façon
  équivalente, sous la forme d'une fonction récursive).
\end{thesis}

Historiquement, cette thèse a d'abord été formulée par Church à propos d'un
autre formalisme équivalent~: le $\lambda$-calcul (que nous étudierons en
détails dans le CHAPITRE LAMBDA CALCUL). Church \cite{Church1936AnUP}
propose déjà cette thèse en tant que postulat empiriquement vérifié~: comme
un postulat physique (\latinexpr{e.g.} les principes de la thermodynamique)
ce fait est avancé sans une preuve formelle, mais avec une forte conviction de
sa vérité basée sur de nombreuses tentatives d'invalidation qui ont échoué.
Ainsi, pour Church, il n'y avait aucune procédure finitaire dont il ne pouvait
trouver qu'elle était équivalente à un programme au sens de son
$\lambda$-calcul.

Au contraire, Turing \cite{turing1936a} donne un réel argument pour soutenir
cette thèse. Comme dit précédemment, cet argument est d'ordre philosophique,
mais il ne peut en être autrement~: aucune description mathématique
fiable existe pour définir la notion de
\og fonction intuitivement calculable\fg et la thèse propose justement
d'assimiler cette notion floue et informelle avec la définition rigoureuse de
machine de Turing (ou autre formalisme de calcul équivalent). Décrivons
dans les grandes lignes cet argument~:

\begin{quotation}
  Imaginons une méthode algorithmique qui, étant donnée une séquence de symboles
  en entrée, doit retourner une séquence de symboles en retour. La méthode
  algorithmique doit être exécutée par un opérateur, qu'il soit un ordinateur
  exécutant un programme ou un être humain lisant une feuille sur laquelle est
  écrite la liste des instructions. De plus, on suppose que l'opérateur a accès
  à une forme de mémoire arbitrairement grande (une mémoire informatique, ou
  bien simplement un très grand nombre de feuilles de papier pour un humain).

  L'opérateur est un objet fini de la réalité~: il a donc un nombre fini d'états
  d'esprit qu'il peut adopter, et il ne peut lire qu'une portion minime de la
  mémoire arbitrairement grande à chaque instant. Ainsi, son comportement à un
  instant donné lors de l'exécution de l'algorithme ne dépend que de deux
  facteurs~: son état d'esprit (un parmi une finitude) et le caractère qu'il
  lit (un parmi une finitude).

  Il se comporte donc comme une machine de Turing dont le nombre d'états est
  suffisamment grand et le nombre de symboles aussi~: il lit un caractère, passe
  d'un état d'esprit au suivant, et modifie ce qu'il a lu en conséquence. Ainsi,
  toute exécution algorithmique est équivalente à l'exécution d'une machine de
  Turing.
\end{quotation}

Si cette thèse est généralement acceptée, l'argument de Turing peut être
discuté. Malgré tout, en bientôt un siècle que cette thèse a été avancée, elle
n'a jamais été rejetée malgré de très nombreux langages de programmation
inventés, des façons originales de définir des calculs ou des tentatives de
construire des modèles plus forts. Tous ces modèles de calculs se sont toujours
retrouvés équivalents (ou moins forts) que le modèle des machines de Turing.

Cette thèse apporte aussi des effets concrets sur la pratique de la
calculabilité.
En effet, en acceptant cette thèse, on admet aussi que toute procédure
algorithmique peut s'écrire comme une machine de Turing, et qu'il suffit donc de
décrire des fonctions calculables par des algorithmiques et méthodes
suffisamment précises et finitaires (par exemple, on refuse quand même
d'imaginer une méthode qui testerait toutes les valeurs d'une fonction
$f : \bN \to \bN$). Ce point de vue est largement plus efficace, et permet
d'éviter de détailler inutilement des constructions techniques qui ne font
aucunement avancer la compréhension des objets en jeu. Cette pratique était déjà
présente par exemple dans la preuve du \cref{lem.recG.MT}, où les
constructions de machines de Turing étaient décrites de façon informelle.
Dans les chapitres suivants, nous proposerons des constructions encore moins
formelles, mais restant suffisamment précises pour que la \cref{thesis.CT}
s'applique.

Enfin, précisons un point important~: cette thèse ne s'applique que dans le
cadre de la calculabilité, c'est-à-dire pour définir ce qui est de l'ordre du
calculable, de ce qu'une méthode finitaire peut atteindre. Elle ne dit rien,
ou presque, sur le lien qui existe entre des modèles de calculs du point de vue
de leur complexité.
