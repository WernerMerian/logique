\chapter{Calcul des prédicats}
\label{chp.logpred}

\minitoc

\lettrine{L}{e} premier but de la logique mathématique est de rendre compte du
langage mathématique. A ce titre, la logique propositionnelle est clairement
insuffisante, et nous l'avons déjà présentée comme une simplification du langage
mathématique habituel. Dans ce chapitre, nous allons nous intéresser à la
formalisation de ce langage mathématique habituel, qui est le calcul des
prédicats de la logique du premier ordre. Le terme prédicat désigne le fait que
nos propositions dépendent de termes, qui représentent des objets mathématiques.
L'expression \og logique du premier ordre\fg{} désigne la capacité d'expression
de nos propositions : celles-ci ne peuvent parler que des objets mathématiques
désignés préalablement par l'univers de discours. Par contraste, la logique du
deuxième ordre permet de parler, en plus de ces objets, des propositions
elles-mêmes : on peut écrire par exemple $\forall P\in\Prop, P\to P$.

Nous nous attarderons d'abord sur la définition, à partir d'une signature du
premier ordre, des termes et des formules, ainsi que les notions syntaxiques de
variables libres, liées et de substitution. Ensuite, nous introduirons les
notions les plus élémentaires de la théorie des modèles, et la relation de
satisfaction $\models$. Enfin, comme dans le chapitre précédent, nous allons
définir une syntaxe pour le calcul des prédicats. La différence, cependant,
est que nous prouverons le théorème de compacité à partir de la complétude.

Ce chapitre peut être vu comme la base commune de la théorie de la démonstration
et de la théorie des modèles. A ce titre, nous donnerons avant tout les
définitions des concepts importants, mais n'allons pas nous attarder sur
ceux-ci, puisque nous les reverrons dans des chapitres dédiés. En particulier
nous allons donner un formalisme pour la syntaxe du calcul des prédicats et un
seul, alors que la partie dédiée à la théorie de la démonstration donnera un
résultat plus général sur toute une famille de systèmes de preuves, et
présentera plusieurs formalismes.

On se fixe pour tout ce chapitre un ensemble $\Var$ dénombrable de variables.

\section{Signatures, termes et formules}

\subsection{Définition d'une signature}

Reprenons l'exemple que nous avions donné au début du \cref{chp.logprop} :
\[\forall n \in \mathbb N, (\exists m \in \mathbb N, n = 2\times m) \lor
(\exists m \in \mathbb N, n = 2 \times m + 1)\]
Remarquons tout d'abord que l'on remplace \og ou\fg{} par le symbole $\lor$,
maintenant que nous connaissons le formalisme de la logique propositionnelle.
Il nous reste cependant plusieurs points à définir formellement : tout d'abord,
la phrase précédente contient des termes, comme $2$ ou $n$. Ceux-ci sont d'une
nature différente d'une variable propositionnelle par exemple, puisque dans le
premier cas, les formules ne relient pas directement des termes, mais des
relations entre termes. Nous devons donc tout d'abord construire un ensemble de
termes, qui représenterons les objets dont les formules parleront. Cependant,
comme nous cherchons en premier lieu à élaborer des phrases finies, nous
cherchons aussi à limiter les symboles que nous utiliserons. Cela s'explique par
le fait que pour lire une phrase, il est nécessaire de savoir à l'avance quels
sont les symboles constitutifs de ce langage. En particulier, nous devons savoir
ce que signifie chaque symbole.

\begin{definition}[Signature]
  Une signature, ou langage, du premier ordre, est un quadruplet $\mathcal L =
  (\mathcal F,\mathcal R, \alpha,\beta)$ où $\alpha : \mathcal F \to \mathbb N$
  et $\beta : \mathcal R \to \mathbb N$. On appelle les éléments de $\mathcal F$
  les symboles de fonction et les éléments de $\mathcal R$ les symboles de
  relation. Pour un symboles de fonction $f\in\mathcal F$, $\alpha(f)$ est
  appelée l'arité de $f$, et de même $\beta(r)$ est l'arité de $r$ pour
  $r\in\mathcal R$. Si $f\in\mathcal F$ est d'arité $0$, on dit que c'est une
  constante.
\end{definition}

\begin{example}
  Un premier exemple de langage est le langage des groupes, qui est
  \[\mathcal L_{\mathrm{Grp}} \defeq \{e^0,\times^2,((-)^{-1})^1\}\]
  où l'on indique par un exposant l'arité d'un symbole, et où tous les symboles
  sont des symboles de fonction. De même, comme on préfère la notation additive
  pour les groupes abéliens, on peut aussi définir
  \[\mathcal L_{\mathrm{Ab}} \defeq \{0^0,+^2,-^1\}\]
\end{example}

\begin{example}
  Un autre exemple est le langage des anneaux :
  \[\mathcal L_{\mathrm{Ring}}\defeq \{0^0,1^0,+^2,\times^2,-^1\}\]
\end{example}

\begin{example}
  Un autre exemple classique de langage est celui de l'arithmétique :
  \[\mathcal L_{\mathrm{Arith}}\defeq \{0^0, S^1, +^2,\times^2,\leq^2\}\]
  où $\leq$ est un symbole de relation, et les autres symboles sont des symboles
  de fonction.
\end{example}

L'exemple du langage de l'arithmétique permet de voir ce que nous entendons par
termes~: avec ce langage, nous avons envie de pouvoir écrire $0$ (qui est une
constante) mais aussi $1$, défini par $S\;0$, ou $S\;S\;0$. De plus, il doit
être possible d'écrire $(S\;S\;0) + (S\;0)$ par exemple : l'écriture est donc
naturellement donnée comme un ensemble inductif, où les arités des symboles de
fonction nous donnent les arités des constructeurs de l'ensemble.

\subsection{Termes et formules}

\begin{definition}[Termes]
  Soit une signature
  $\Sigma = (\mathcal F_\Sigma,\mathcal R_\Sigma, \alpha_\Sigma,\beta_\Sigma)$,
  on définit $\Term(\Sigma)$ comme l'ensemble inductif engendré par
  $\mathcal F_\Sigma\cup\Var$ où l'arité de $f\in\mathcal F_\Sigma$ est
  $\alpha_\Sigma(f)$ et où l'arité de $x\in \Var$ est $0$. On peut représenter
  cet ensemble par la grammaire suivante :
  \[t,u \Coloneq x \mid f(t_1,\ldots,t_{\alpha(f)})\]
  où $x\in \Var$ et $f\in \mathcal F_\Sigma$.
\end{definition}

Ainsi, les termes écrits dans notre langage vont représenter les objets
mathématiques sur lesquels porteront nos formules. Ces formules sont définies
par induction, d'une façon analogue aux propositions de la logique
propositionnelle. En l'absence de variables propositionnelles, les éléments
atomiques des formules seront construits à partir des termes.

Nous donnons la définition d'une proposition atomique comme une proposition de
la forme $r(t_1,\ldots,t_n)$ où $n$ correspond à l'arité de $r$. En plus des
symboles de relation de la signature, on considère le symbole $=$, représentant
l'égalité.

\begin{definition}[Proposition atomique]
  Soit une signature $\Sigma$. On définit l'ensemble $\Atom(\Sigma)$ des
  propositions atomiques par
  \begin{multline*}
    \Atom(\Sigma) \defeq \{(r,t_1,\ldots,t_k)\mid r\in\mathcal R_\Sigma,
    (t_1,\ldots,t_k) \in (\Term(\Sigma))^k, k = \beta_\Sigma(r)\}\\
    \cup\{(``=",t,u)\mid t,u\in \Term(\Sigma)\}
  \end{multline*}
  où $=$ est un symbole n'appartenant pas à $\mathcal R_\Sigma$.
\end{definition}

\begin{remark}
  L'égalité est une relation, mais celle-ci n'appartient pas formellement au
  langage, car son comportement est donné par les règles logiques, de la même
  façon que $\lor$ et $\land$ ont leur sens imposés. Certains auteurs
  considèrent au contraire que $=$ doit être ajouté au langage, en tant que
  symbole de relation binaire, et d'autres font la différence entre un langage
  égalitaire (incluant le symbole $=$) et un langage non égalitaire. Notre choix
  est motivé à la fois par la simplicité et par l'expressivité : l'égalité est
  clairement utile pour formaliser les mathématiques et raisonner dessus, mais
  chercher à préciser quand nous l'utilisons ne l'est pas, étant donné qu'elle
  sera toujours présente.
\end{remark}

\begin{notation}
  Pour un symbole de relation $r$ (pouvant être l'égalité), on écrira
  $r(t_1,\ldots,t_k)$ plutôt que $(r,t_1,\ldots,t_k)$ dans la suite. Cela est
  motivé par le fait que le sens donné à $r$ est d'être un prédicat atomique
  prenant en argument des termes.
\end{notation}

Nous pouvons maintenant définir l'ensemble des formules sur une signature
donnée.

\begin{definition}[Formules]
  Soit une signature $\Sigma$. On définit l'ensemble $\Formula(\Sigma)$ par
  la grammaire suivante :
  \[\varphi,\psi \Coloneq a\mid \top\mid\bot\mid\lnot\varphi
  \mid\varphi\lor\psi\mid \varphi\land\psi\mid \varphi\to\psi\mid
  \forall x,\varphi\mid \exists x,\varphi\]
  où $a\in \Atom(\Sigma)$ et $x\in \Var$.
\end{definition}

\subsection{Variables et substitution}

Maintenant que les formules sont définies, nous voulons définir les opérations
basiques sur celles-ci. Tout d'abord, nous devons introduire les notions
élémentaires liées aux variables.

\begin{definition}[Variable libre, formule close]
  On définit la fonction qui étant donné un terme (respectivement une formule),
  retourne l'ensemble des variables libres y apparaissant. La fonction $\VL$ est
  définie par induction sur $\Term(\Sigma)$ (respectivement $\Formula(\Sigma)$)
  par les équations suivantes :
  \begin{itemize}
  \item si $t = x\in \Var$, alors $\VL(t) = \{x\}$.
  \item si $t = f(t_1,\ldots,t_n)$ où $f\in \mathcal F_\Sigma$,
    $t_1,\ldots,t_n\in\Term(\Sigma)$, alors
    $\displaystyle\VL(t) =\bigcup_{k = 1}^n \VL(t_k)$.
  \item si $\varphi = r(t_1,\ldots,t_n)$ est une proposition atomique où
    $r\in\mathcal R_\Sigma, t_1,\ldots,t_n\in\Term(\Sigma)$, alors
    $\displaystyle\VL(\varphi) = \bigcup_{k = 1}^n \VL(t_k)$.
  \item si $\varphi = \top$, alors $\VL(\varphi) = \varnothing$.
  \item si $\varphi = \bot$, alors $\VL(\varphi) = \varnothing$.
  \item si $\varphi = \lnot \psi$, alors $\VL(\varphi) = \VL(\psi)$.
  \item si $\varphi = \psi\lor\chi$, $\varphi = \psi\land \chi$ ou
    $\varphi = \psi\to\chi$, alors $\VL(\varphi) = \VL(\psi)\cup\VL(\chi)$.
  \item si $\varphi = \forall x, \psi$ ou $\varphi = \exists x, \psi$, alors
    $\VL(\varphi) = \VL(\psi)\backslash\{x\}$.
  \end{itemize}
  On dit que $\varphi$ est une formule close (ou une proposition) si
  $\VL(\varphi) =\varnothing$. On note par $\Prop(\Sigma)$ l'ensemble des
  formules closes (ou propositions) sur la signature $\Sigma$. De même, un
  terme $t$ est appelé un terme clos s'il n'a pas de variables libres
  (c'est-à-dire s'il n'a pas de variable).
\end{definition}

\begin{remark}\label{rmk.alpha}
  Une variable non libre est dite liée : les variables liées sont muettes, elles
  n'ont pas d'importance en elle-même et seulement sur le quantificateur qui les
  lie. On considère implicitement que si $\varphi$ et $\psi$ diffèrent seulement
  en remplaçant la variable liée par un quantificateur et les variables que ce
  quantificateur lie, alors $\varphi = \psi$. Par exemple, $\forall x, x = x$ et
  $\forall y, y = y$ sont identifiées.
\end{remark}

Comme pour la logique propositionnelle, la valeur de vérité d'une formule va
dépendre de la valeur associée aux variables libres. Cependant, pour l'instant,
nous n'avons pas de système clair d'évaluation : nous verrons comment évaluer
une formule quand nous aborderons la notion de modèle. Au niveau syntaxique,
cependant, nous pouvons déjà introduire la substitution, que l'on peut voir
comme une évaluation syntaxique : on remplace une variable libre par un terme.

\begin{definition}[Substitution]
  Soient une signature $\Sigma$, un terme $t\in \Term(\Sigma)$ et une variable
  $x\in \Var$. On définit les deux fonctions
  \[\begin{array}{rcccc}
  -[t/x] &: & \Term(\Sigma) & \longrightarrow & \Term(\Sigma)\\
  & & u & \longmapsto & u[t/x] \\
  \\
  -[t/x] &: & \Formula(\Sigma) & \longrightarrow & \Formula(\Sigma)\\
  & & \varphi & \longmapsto & \varphi[t/x]
  \end{array}\]
  par induction sur la structure de $\Term(\Sigma)$ (respectivement sur la
  structure de $\Formula(\Sigma)$) :
  \begin{itemize}
  \item si $u = x$, alors $u[t/x] = t$.
  \item si $u = y\in \Var$ avec $y \neq x$, alors $u[t/x] = y$.
  \item si $u = f(u_1,\ldots,u_n)$, alors
    $u[t/x] = f(u_1[t/x],\ldots,u_n[t/x])$.
  \item si $\varphi = r(u_1,\ldots,u_n)$, alors
    $\varphi[t/x] = r(u_1[t/x],\ldots,u_n[t/x])$.
  \item si $\varphi = \top$ alors $\varphi[t/x] = \top$.
  \item si $\varphi = \bot$ alors $\varphi[t/x] = \bot$.
  \item si $\varphi = \lnot \psi$ alors $\varphi[t/x] = \lnot \psi[t/x]$.
  \item si $\varphi = \psi \lor \chi$ alors
    $\varphi[t/x] = \psi[t/x]\lor\chi[t/x]$.
  \item si $\varphi = \psi \land \chi$ alors
    $\varphi[t/x] = \psi[t/x]\land\chi[t/x]$.
  \item si $\varphi = \psi \to \chi$ alors
    $\varphi[t/x] = \psi[t/x]\to\chi[t/x]$.
  \item si $\varphi = \forall z, \psi$ où $z\notin \VL(t)$, alors
    $\varphi[t/x] = \forall z, \psi[t/x]$.
  \item si $\varphi = \exists z, \psi$ où $z\notin \VL(t)$, alors
    $\varphi[t/x] = \exists z, \psi[t/x]$.
  \end{itemize}
\end{definition}

\begin{remark}
  La condition de $z\notin\VL(t)$ dans les derniers cas peut toujours être
  réalisée quitte à renommer la variable liée $z$ : puisque $\Var$ est infini
  et que $\VL(t)$ est fini, on peut toujours trouver $a\notin\VL(t)$ et
  remplacer $\forall z, \psi$ par $\forall a, \psi[a/z]$ en utilisant
  l'identification de la \cref{rmk.alpha}.

  Si l'on veut être parfaitement formel, il conviendrait de procéder dans
  l'autre sens : on définit d'abord la substitution comme donnée précédemment,
  puis on définit la relation $\varphi \equiv \psi$ engendrée par
  $\forall x, \psi \equiv \forall y, \psi[y/x]$ et
  $\exists x,\psi \equiv\exists y, \psi[y/x]$ dont on prouve qu'elle est une
  relation d'équivalence, puis on définit le \og vrai\fg{} ensemble
  $\Formula(\Sigma)$ par $\Formula(\Sigma)/ \equiv$ (cela n'est pas nécessaire
  pour $\Term(\Sigma)$ puisque toute variable est libre, dans un terme), et que
  la fonction $-[t/x]$ est bien définie sur ce quotient. Ce processus est
  évidemment plus laborieux et n'apporte rien à la compréhension, c'est pourquoi
  nous ne le détaillons pas ici.
\end{remark}

On définit aussi la notion de substitution simultanée, permettant de remplacer
plusieurs variables à la fois par des termes.

\begin{definition}[Substitution simultanée]
    Soit une signature $\Sigma$, une fonction partielle
    $\rho : \Var \partialto \Term(\Sigma)$, un terme $t \in \Term(\Sigma)$ et 
    une formule $\varphi \in \Formula(\Sigma)$. On définit la substitution de $t$
    (respectivement $\varphi$) par $\rho$, notée  $\rho(t)$
    (respectivement $\rho(\varphi)$), par induction sur $\varphi$~:
    \begin{itemize}
        \item si $t = x \in \dom(\rho)$ alors $\rho(t) = \rho(x)$
        \item si $t = f(t_1,\ldots,t_n)$ alors
        $\rho(t) = f(\rho(t_1),\ldots,\rho(t_n))$.
        \item si $\varphi = r(t_1,\ldots,t_n)$ alors
        $\rho(\varphi) = r(\rho(t_1),\ldots,\rho(t_n))$.
        \item si $\varphi = \top$ alors $\rho(\varphi) = \top$.
        \item si $\varphi = \bot$ alors $\rho(\varphi) = \bot$.
        \item si $\varphi = \lnot \psi$ alors
        $\rho(\varphi) = \lnot \rho(\psi)$.
        \item si $\varphi = \psi \lor \chi$ alors
        $\rho(\varphi) = \rho(\psi)\lor\rho(\chi)$.
        \item si $\varphi = \psi \land \chi$ alors
        $\rho(\varphi) = \rho(\psi)\land\rho(\chi)$.
        \item si $\varphi = \psi \to \chi$ alors
        $\rho(\varphi) = \rho(\psi)\to\rho(\chi)$.
        \item si $\varphi = \forall z, \psi$ où
        $\displaystyle z \notin \bigcup_{x \in \dom(\rho)}\VL(\rho(x))$,
        alors $\rho(\varphi) = \forall z, \rho(\psi)$.
        \item si $\varphi = \exists z, \psi$ où
        $\displaystyle z \notin \bigcup_{x \in \dom(\rho)}\VL(\rho(x))$,
        alors $\rho(\varphi) = \exists z, \rho(\psi)$.
    \end{itemize}
\end{definition}

\begin{exercise}
  Soit $x\in \Var$, $t\in\Term(\Sigma)$ et $\varphi\in\Prop(\Sigma)$ pour une
  signature $\Sigma$ quelconque. Montrer que $\varphi[t/x] = \varphi$.
\end{exercise}

\begin{exercise}
  Soient $t,u,v\in\Term(\Sigma)$ et $x,y\in\Var$, montrer que
  \[t[u/x][v/y] = (t[v/y])[u[v/y]/x]\]
\end{exercise}

\begin{exercise}
  Soient $t,u\in\Term(\Sigma)$, $\rho : \Var \partialto \Term(\Sigma)$ et
  $x \notin \VL(\rho)$ (cet ensemble est l'abréviation de
  $\displaystyle\bigcup_{y \in \VL(\rho)} \VL(\rho(y))$). Montrer que
  \[\rho(t[u/x]) = \rho(t)[\rho(u)/x]\]
\end{exercise}

\section{Bases de théorie des modèles}

Maintenant que nous avons défini la syntaxe élémentaire, nous allons lui donner
un sens : une sémantique. Dans le cas de la logique propositionnelle, le sens
d'une proposition était simple à définir, puisqu'il s'agissait d'une valeur de
vérité en fonction des variables propositionnelles. Dans le cas de la logique du
premier ordre, les propositions parlent d'objets, et il faut donc fixer un
univers ambiant sur lequel porte le discours donné par les formules. Par
exemple, en prenant le langage de l'arithmétique, pour évaluer le terme
$S\;S\;0 + S\;S\;0$, on peut considérer $0$ comme l'entier naturel $0$, $S$
comme la fonction $n \mapsto n + 1$ et $+$ comme l'addition usuelle sur les
entiers, ce qui fait que notre évaluation du terme sera $4$ ;
mais on peut imaginer une autre interprétation de ce terme donnant par exemple
$5$ ou tout autre nombre.

\subsection{Structure et interprétation}

Nous travaillons donc sur l'interprétation d'une formule en deux parties : tout
d'abord, nous introduisons la notion de structure, qui offre une interprétation
claire du langage dans lequel la formule est écrite, et c'est seulement à partir
de cette interprétation que l'on peut évaluer une formule. Cela modifie notre
notion de vérité : les formules closes prennent une plus grande importance que
le reste des formules, mais il faut quantifier sur des structures en
contrepartie.

Donner un sens à notre langage signifie que l'on sélectionne en premier lieu un
univers de discours~: on désigne un ensemble $M$ dans lequel seront interprétées
les formules. Par exemple, quand on parle d'arithmétique, une formule telle que
\[\forall n,\exists m, n \leq m\]
s'interprète naturellement comme portant sur des entiers, c'est-à-dire que $n$
et $m$ ont alors vocation à être des éléments de $\mathbb N$. Il nous faut
ensuite donner un sens aux symboles~: ceux-ci n'ont, par défaut, pas de sens
défini. On a donc besoin, pour chaque symbole de fonctions, d'associer une
réelle fonction ensembliste, dont le domaine est l'univers de discours choisi.
De même, un symbole de relation sera associé à une vraie relation ensembliste
sur l'univers de discours.

\begin{definition}[Structure]
  Soit une signature $\Sigma$. On appelle $\Sigma$-structure (ou simplement
  structure) $\mathcal M$ un triplet
  $(|\mathcal M|,-^{\mathcal M}_\mathcal F,-^{\mathcal M}_\mathcal R)$
  (on notera les deux $-^{\mathcal M}$, sans indice) où :
  \begin{itemize}
  \item $|\mathcal M|$ est un ensemble.
  \item pour chaque $f\in \mathcal F_\Sigma$ d'arité $n$,
    $f^{\mathcal M} : |\mathcal M|^n \to |\mathcal M|$.
  \item pour chaque $r\in\mathcal R_\Sigma$ d'arité $n$,
    $r^{\mathcal M} \subseteq |\mathcal M|^n$.
  \end{itemize}

  On identifie $|\mathcal M|^0 \to |\mathcal M|$ à $|\mathcal M|$ : un symbole
  de constante est associé directement à un élément.
\end{definition}

\begin{remark}
  Si l'on définit un modèle comme un triplet, définir un modèle revient à lister
  toutes les assignations, c'est-à-dire donner un ensemble $|\mathcal M|$ puis
  une fonction $f^\mathcal M$ pour chaque symbole de fonction $f$, et
  une relation $r^\mathcal M$ pour chaque symbole de relation $r$. On
  notera donc, au lieu d'un triplet contenant deux fonctions, une liste de
  tous les éléments donnés par l'intérprétation. En règle générale (comme par
  exemple le modèle donné au paragraphe suivant) l'association de chaque symbole
  est évidente, car la notation du symbole de fonction évoque la fonction qui
  l'interprète ($0$ pour le neutre dans le langage des groupes abéliens, par
  exemple).
\end{remark}

\begin{example}
  En reprenant les différents langages définis précédemments, on peut voir que,
  par exemple, $(\mathbb Z,0,+,-)$ est une structure sur le langage des groupes.
  C'est même, en incluant $1$ et $\times$, une structure sur le langage des
  anneaux. De même, $(\mathbb N,0,n\mapsto n + 1,+,\times,\leq)$ est une
  structure sur le langage de l'arithmétique.
\end{example}

Avec ces nouveaux exemples, on voit qu'il devient naturel d'interpréter dans la
structure $(\mathbb N,0,n\mapsto n+1,+,\times,\leq)$ le terme
$S\;S\;0 + S\;S\;0$
par l'élément $4\in\mathbb N$. Nous pouvons donc généraliser ce résultat. Pour
cela, on définit d'abord la notion d'environnement. Un environnement permet de
gérer les variables libres d'un terme et d'une formule pour qu'une variable $x$
puisse être assignée à un réel objet de $|\mathcal M|$. Dans un environnement
$\rho$ donné, dont on suppose qu'il contient les variables libres d'un certain
terme $t$, on peut construire de façon canonique un élément
$t^\mathcal M_\rho \in |\mathcal M|$~: une variable libre est associée à son
image par $\rho$, et le reste se construit par l'interprétation des symboles
de fonction. De même, une formule est alors associée à une valeur de vérité,
dépendant des valeurs de $\rho$.

\begin{definition}[Environnement]
  Soit une signature $\Sigma$ et une structure $\mathcal M$. Un environnement
  $\rho$ est une fonction partielle $\rho : \Var\partialto |\mathcal M|$. On
  note $\mathcal E$ l'ensemble des environnements. Etant donnés un élément
  $m\in|\mathcal M$|, une variable $x$ et un environnement $\rho$, on note
  $\rho[x \mapsto m]$ l'environnement coïncidant avec $\rho$ sur
  $\Var\backslash\{x\}$ et valant $m$ en $x$.
\end{definition}

\begin{definition}[Interprétation, valuation]
  Soit une signature $\Sigma$, une structure $\mathcal M$ et un environnement
  $\rho$. On définit par induction sur $t$ (respectivement sur $\varphi$), en
  supposant que $\VL(t)\subseteq\dom(\rho)$ (respectivement
  $\VL(\varphi)\subseteq\dom(\rho)$), les fonctions suivantes~:
  \[\begin{array}{rcccc}
  -_\rho^\mathcal M & : & \Term(\Sigma) & \longrightarrow & |\mathcal M|\\
  \\
  \Val_\rho & : & \Formula(\Sigma) & \longrightarrow & \{0,1\}\\
  \end{array}\]

  \begin{itemize}
  \item si $t = x \in \Var$, alors $t^\mathcal M_\rho = \rho(x)$.
  \item si $t = f(t_1,\ldots,t_n)$, alors
    $t_\rho^\mathcal M =
    f^\mathcal M((t_1)^\mathcal M_\rho,\ldots,(t_n)^\mathcal M_\rho)$.
  \item si $\varphi = r(t_1,\ldots,t_n)$ alors
    $\Val_\rho(\varphi) =
    \chi_{r^\mathcal M}((t_1)^\mathcal M_\rho,\ldots,(t_n)^\mathcal M_\rho)$, l'image
    de $((t_i)^\mathcal M_\rho)_{i = 1,\ldots,n}$ par la fonction indicatrice de
    $r^{\mathcal M}$ (qui est, rappelons-le, une partie de $|\mathcal M|^n$).
    On interprète le symbole $=$ de la même façon, en prenant la partie
    $\{(m,m)\mid m\in |\mathcal M|\}$.
  \item si $\varphi = \top$, alors $\Val_\rho(\varphi) = 1$.
  \item si $\varphi = \bot$, alors $\Val_\rho(\varphi) = 0$.
  \item si $\varphi = \lnot \psi$, alors
    $\Val_\rho(\varphi) = 1 - \Val_\rho(\psi)$.
  \item si $\varphi = \psi \lor \chi$, alors
    $\Val_\rho(\varphi) = \max(\Val_\rho(\psi),\Val_\rho(\chi))$
  \item si $\varphi = \psi \land \chi$, alors
    $\Val_\rho(\varphi) = \min(\Val_\rho(\psi),\Val_\rho(\chi))$
  \item si $\varphi = \psi \to \chi$, alors
    $\Val_\rho(\varphi) = \max(1 - \Val_\rho(\psi),\Val_\rho(\chi))$
  \item si $\varphi = \forall x, \psi$, alors
    $\displaystyle\Val_\rho(\varphi) =
    \min_{m \in |\mathcal M|}(\Val_{\rho[x\mapsto m]}(\psi))$
  \item si $\varphi = \exists x, \psi$, alors
    $\displaystyle\Val_\rho(\varphi) =
    \max_{m \in |\mathcal M|}(\Val_{\rho[x\mapsto m]}(\psi))$
  \end{itemize}
  
  Si $t$ est un terme clos, alors $t^{\mathcal M}$ est un élément de
  $|\mathcal M|$. Si $\varphi$ est une formule close, alors $\Val(\varphi)$ est
  un élément de $\{0,1\}$. Ces deux éléments sont obtenus en interprétant le
  terme (respectivement la formule) dans le contexte vide, ou de façon
  équivalente dans n'importe quel contexte.
\end{definition}

Enfin, donnons un résultat statuant que la substitution et l'interprétation
commutent.

\begin{proposition}\label{prop.comm.subst.env}
  Soit une signature $\Sigma$, une structure $\mathcal M$, un environnement
  $\rho$, une variable $x$, deux termes $t,u$ et une formule $\varphi$. On
  suppose de plus les inclusions suivantes~: $\VL(t)\subseteq \dom(\rho)$,
  $\VL(u)\subseteq\dom(\rho)$ et $\VL(\varphi)\subseteq\dom(\rho)$. On a alors
  les deux égalités suivantes :
  \[(t[u/x])_\rho^\mathcal M = t^\mathcal M_{\rho[x \mapsto u^\mathcal M_\rho]} \qquad
  \Val_\rho(\varphi[u/x]) = \Val_{\rho[x\mapsto u^\mathcal M_\rho]}(\varphi)\]
\end{proposition}

\begin{proof}
  On montre la première égalité par induction sur $t$, et l'autre par induction
  sur $\varphi$ :
  \begin{itemize}
  \item si $t = x$ alors $(t[u/x])_\rho^\mathcal M = u^\mathcal M_\rho$ d'un côté,
    et de l'autre
    $t^\mathcal M_{\rho[x\mapsto u_\rho^\mathcal M]} =
    (\rho[x\mapsto u_\rho^\mathcal M])(x)$
    d'où l'égalité.
  \item si $t = y$ où $y\in\Var$ et $y\neq x$, alors
    $(t[u/x])_\rho^\mathcal M = \rho(y)$, qui est la même valeur pour l'autre
    partie de l'équation.
  \item si $t = f(t_1,\ldots,t_n)$ où pour tout $i\in\{1,\ldots,n\}$,
    $(t_i[u/x])^\mathcal M_\rho = (t_i)^\mathcal M_{\rho[x\mapsto u^\mathcal M_\rho]}$, alors
    \begin{align*}
      (t[u/x])^\mathcal M_\rho &= f((t_1[u/x])^\mathcal M_\rho,\ldots,
      (t_n[u/x])^\mathcal M_\rho)\\
      &= f((t_1)^\mathcal M_{\rho[x\mapsto u^\mathcal M_\rho]},\ldots,
      (t_n)^\mathcal M_{\rho[x\mapsto u^\mathcal M_\rho]})\\
      &= t^\mathcal M_{\rho[x\mapsto u^\mathcal M_\rho]}
    \end{align*}
  \item si $\varphi = \top$ ou $\varphi = \bot$, l'égalité est directe.
  \item si $\varphi = r(t_1,\ldots,t_n)$ est une proposition atomique, alors
    \begin{align*}
      \Val_\rho(r(t_1,\ldots,t_n)[u/x])
      &= \Val_\rho(r(t_1[u/x],\ldots,t_n[u/x]))\\
      &= \chi_r((t_1[u/x])^\mathcal M_\rho,\ldots,(t_n[u/x])^\mathcal M_\rho)\\
      &= \chi_r((t_1)^\mathcal M_{\rho[x\mapsto u^\mathcal M_\rho]},\ldots,
      (t_n)^\mathcal M_{\rho[x\mapsto u^\mathcal M_\rho]})\\
      &= \Val_{\rho[x\mapsto u^\mathcal M_\rho]}(r(t_1,\ldots,t_n))
    \end{align*}
  \item si $\varphi = \lnot \psi$, alors
    \begin{align*}
      \Val_\rho((\lnot \psi)[u/x]) &= \Val_\rho(\lnot (\psi[u/x]))\\
      &= 1 - \Val_\rho(\psi[u/x])\\
      &= 1 - \Val_{\rho[x\mapsto u^\mathcal M_\rho]}(\psi)\\
      &= \Val_{\rho[x\mapsto u^\mathcal M_\rho]}(\lnot\psi)\\
    \end{align*}
  \item si $\varphi = \psi \lor \chi$, alors
    \begin{align*}
      \Val_\rho((\psi \lor \chi)[u/x]) &= \Val_\rho(\psi[u/x] \lor \chi[u/x])\\
      &= \max (\Val_\rho(\psi[u/x]),\Val_\rho(\chi[u/x]))\\
      &= \max (\Val_{\rho[x\mapsto u^\mathcal M_\rho]}(\psi),
      \Val_{\rho[x \mapsto u^\mathcal M_\rho]}(\chi))\\
      &= \Val_{\rho[x\mapsto u^\mathcal M_\rho]}(\psi\lor \chi)
    \end{align*}
  \item si $\varphi = \psi \land \chi$, alors
    \begin{align*}
      \Val_\rho((\psi \land \chi)[u/x]) &= \Val_\rho(\psi[u/x] \land \chi[u/x])\\
      &= \min (\Val_\rho(\psi[u/x]),\Val_\rho(\chi[u/x]))\\
      &= \min (\Val_{\rho[x\mapsto u^\mathcal M_\rho]}(\psi),
      \Val_{\rho[x \mapsto u^\mathcal M_\rho]}(\chi))\\
      &= \Val_{\rho[x\mapsto u^\mathcal M_\rho]}(\psi\land \chi)
    \end{align*}
  \item si $\varphi = \psi \to \chi$, alors
    \begin{align*}
      \Val_\rho((\psi to \chi)[u/x]) &= \Val_\rho(\psi[u/x] \to \chi[u/x])\\
      &= \max (1 - \Val_\rho(\psi[u/x]),\Val_\rho(\chi[u/x]))\\
      &= \max (1 - \Val_{\rho[x\mapsto u^\mathcal M_\rho]}(\psi),
      \Val_{\rho[x \mapsto u^\mathcal M_\rho]}(\chi))\\
      &= \Val_{\rho[x\mapsto u^\mathcal M_\rho]}(\psi\to \chi)
    \end{align*}
  \item si $\varphi = \forall y, \psi$ (sans perte de généralité,
    $y\notin \VL(u)$), alors
    \begin{align*}
      \Val_\rho((\forall y,\psi)[u/x]) &= \Val_\rho(\forall y,\psi[u/x])\\
      &= \min_{m\in|\mathcal M|}(\Val_{\rho[y \mapsto m]}(\psi[u/x]))\\
      &= \min_{m\in|\mathcal M|}(\Val_{\rho[y\mapsto m]
        [x\mapsto u^\mathcal M_{\rho[y\mapsto m]}]}(\psi))\\
      &= \min_{m\in|\mathcal M|}(\Val_{\rho[y\mapsto m][x\mapsto u^\mathcal M_\rho]}(\psi))\\
      &= \min_{m\in|\mathcal M|}(\Val_{\rho[x\mapsto u^\mathcal M_\rho][y\mapsto m]}(\psi))\\
      &= \Val_{\rho[x\mapsto u^\mathcal M_\rho]}(\forall y, \psi)
    \end{align*}
  \item si $\varphi = \exists y, \psi$ (sans perte de généralité,
    $y\notin \VL(u)$), alors
    \begin{align*}
      \Val_\rho((\exists y,\psi)[u/x]) &= \Val_\rho(\exists y,\psi[u/x])\\
      &= \max_{m\in|\mathcal M|}(\Val_{\rho[y \mapsto m]}(\psi[u/x]))\\
      &= \max_{m\in|\mathcal M|}(\Val_{\rho[y\mapsto m]
        [x\mapsto u^\mathcal M_{\rho[y\mapsto m]}]}(\psi))\\
      &= \max_{m\in|\mathcal M|}(\Val_{\rho[y\mapsto m][x\mapsto u^\mathcal M_\rho]}(\psi))\\
      &= \max_{m\in|\mathcal M|}(\Val_{\rho[x\mapsto u^\mathcal M_\rho][y\mapsto m]}(\psi))\\
      &= \Val_{\rho[x\mapsto u^\mathcal M_\rho]}(\exists y, \psi)
    \end{align*}
  \end{itemize}
  D'où le résultat par induction.
\end{proof}

\subsection{Satisfaction, modèle}

La notion de valuation permet de définir la relation de satisfaction, $\models$,
d'une façon analogue à ce que nous avons fait pour la logique propositionnelle.

\begin{definition}[Satisfaction]
  Soit une signature $\Sigma$, une structure $\mathcal M$ et, une formule
  $\varphi$ et un environnement $\rho$ tel que
  $\VL(\varphi)\subseteq\dom(\rho)$.
  On définit $\mathcal M,\rho\models \varphi$ par
  \[\mathcal M,\rho\models \varphi \defeq \Val_\rho(\varphi) = 1\]

  Soit un ensemble $\mathcal F\subseteq\Formula(\Sigma)$ et un environnement
  $\rho$ tel que
  $\forall \varphi\in\mathcal F, \VL(\varphi)\subseteq\dom(\rho)$.
  On dit que $\mathcal M,\rho$ satisfont $\mathcal F$, ce que l'on écrit
  $\mathcal M,\rho\models\mathcal F$, lorsque pour toute formule
  $\varphi\in\mathcal F$, il est vrai que $\mathcal M,\rho\models \varphi$.

  Dans le cas de formules closes, on écrira directement
  $\mathcal M\models\varphi$ et $\mathcal M\models \mathcal F$.
\end{definition}

Cela permet alors de définir ce qu'est un modèle.

\begin{definition}[Modèle]
  Soit une signature $\Sigma$ et $\mathcal C\subseteq\Prop(\Sigma)$. On dit que
  $\mathcal M$ est un modèle de $\mathcal C$ si $\mathcal M\models \mathcal C$.
\end{definition}

Un modèle ne se définit qu'avec un ensemble de formules closes. On pourrait
imaginer une définition analogue avec une formule non close, mais faire cela
signifie qu'au lieu de donner une structure, il faudrait donner une structure et
un environnement en même temps. Le but des modèles est plutôt, ici, de
construire une classe particulière de structure vérifiant certaines conditions
que l'on peut exprimer au premier ordre.

Un exemple simple est la formule
\[\forall x,\forall y, x + y = y + x\]
Les modèles de cette formule, sur le langage $\{+^2\}$, sont les magmas
commutatifs (ensembles munis d'une loi de composition interne commutative) :
l'intérêt ici est de pouvoir décrire parmi tous les magmas possibles ceux qui
ont une lci commutative, et donc de le faire en quelque sorte uniformément parmi
les structures, ce qui n'est pas le cas si les formules n'étaient pas closes.

\subsection{Théories et conséquence logique}

Cela mène naturellement à deux notions connexes : celle de théorie, et celle
de conséquence logique. Une théorie permet de décrire des classes de modèles, et
la conséquence logique permet de créer des liens entre les formules, de la même
façon que nous avions $\vDash$ pour le calcul propositionnel.

\begin{definition}[Théorie]
  Une théorie axiomatique, ou simplement théorie, sur une signature $\Sigma$,
  est une partie $\mathcal T\subseteq\Prop(\Sigma)$.
\end{definition}

\begin{definition}[Conséquence logique, équivalence]
  Soit un ensemble $\mathcal F \subseteq \Formula(\Sigma)$ et une formule
  $F\in\Formula(\Sigma)$. On dit que $F$ est conséquence logique de
  $\mathcal F$, ce que l'on écrit $\mathcal F \vDash F$, lorsque pour toute
  structure $\mathcal M$, si $\mathcal M\models \mathcal F$ alors
  $\mathcal M\models F$. Si deux formules $F$ et $G$ sont telles que
  $F\vDash G$ et $G\vDash F$, alors $F$ et $G$ sont dites logiquement
  équivalentes, ce que l'on note $F\equiv G$.
\end{definition}

On peut relier ces deux notions par celle de théorie saturée.

\begin{definition}[Théorie saturée, clôture par conséquence]
  Soit une théorie $\mathcal T$ sur une signature $\Sigma$. On dit que
  $\mathcal T$ est saturée si pour toute formule $A\in\Formula(\Sigma)$,
  si $\mathcal T\vDash A$ alors $A\in \mathcal T$.

  Pour une théorie $\mathcal T$, on définit sa clôture par conséquence, notée
  $\vclose{\mathcal T}$, par
  \[\vclose{\mathcal T} \defeq \{ A \in \Formula(\Sigma)
  \mid \mathcal T \vDash A\}\]
\end{definition}

De plus, la conséquence logique permet directement de décrire une théorie
\og fausse\fg{} : une telle théorie est une théorie dans laquelle la proposition
fausse est considérée comme vraie. Au niveau des modèles, cela se traduit par le
fait qu'il n'existe pas de modèle de la théorie, puisqu'un tel modèle
associerait automatiquement à $\bot$ la valeur de vérité $0$.

\begin{proposition}
  Soit une signature $\Sigma$ et une théorie $\mathcal T$ sur $\Sigma$. Alors
  $\mathcal T$ admet un modèle si et seulement si $\mathcal T \not\vDash \bot$.
\end{proposition}

\begin{proof}
  Supposons que $\mathcal T$ admette un modèle $\mathcal M$. Alors par
  définition de $\Val$, $\Val(\bot) = 0$ donc $\mathcal M\not\models \bot$. On
  en déduit que $\mathcal T\nvDash\bot$.

  Dans le sens réciproque et par contraposée, supposons que $\mathcal T$ n'admet
  pas de modèle. Alors pour tout $\mathcal M$ tel que
  $\mathcal M\models \mathcal T$, $\mathcal M\models \bot$, par vacuité de la
  condition : on en déduit donc que $\mathcal T \vDash \bot$, donc que si
  $\mathcal T \nvDash\bot$, alors $\mathcal T$ a un modèle.
\end{proof}

\begin{definition}[Théorie cohérente, contradictoire]
  Soit une signature $\Sigma$ et une théorie $\mathcal T$ sur $\Sigma$. On dit
  que $\mathcal T$ est cohérente quand elle admet un modèle, et qu'elle est
  contradictoire si elle n'admet pas de modèle. De façon équivalente,
  $\mathcal T$ est cohérente si et seulement si elle n'est pas contradictoire,
  et si et seulement si $\bot\notin\vclose{\mathcal T}$.
\end{definition}

On voit donc qu'une théorie ne doit pas pouvoir prouver trop de choses.
Néanmoins, on peut vouloir une théorie la plus forte possible, qui reste malgré
tout cohérente. Une telle théorie est une théorie complète~: elle est une
théorie dans laquelle si un énoncé est faux, alors son contraire est vrai. Elle
peut donc décider tout énoncé, et puisqu'elle est cohérente elle ne peut pas
décider plus (sinon il serait possible de vérifier $A$ et $\lnot A$, ce qui est
impossible).

\begin{definition}[Théorie complète]
  Une théorie $\mathcal T$ sur une signature $\Sigma$ est dite complète si pour
  toute formule $A\in\Formula(\Sigma)$, soit $A\in \vclose{\mathcal T}$ soit
  $\lnot A \in \vclose{\mathcal T}$, mais pas les deux.
\end{definition}

On va voir plus tard que toute théorie cohérente, c'est-à-dire dont $\bot$ n'est
pas une conséquence logique, peut être étendue en une théorie complète.

\subsection{Morphismes de modèles}

Pour manipuler efficacement les modèles, il est utile de prendre du recul sur ce
qui est manipulé, et d'adopter un point de vue plus algébrique. Pour cela, nous
allons donner quelques propriétés basiques liées aux morphismes de modèles.

En mathématiques, un morphisme est en toute généralité une application qui
préserve la structure. C'est l'idée qui est formalisée dans la notion de
morphisme entre structures~: un morphisme commute avec les symboles de relation
et de fonction.

\begin{definition}[Morphisme de structures]
  Soit une signature $\Sigma$ et deux structures $\mathcal M,\mathcal N$, un
  morphisme $\varphi$ de $\mathcal M$ vers $\mathcal N$ est une application
  \[\varphi : |\mathcal M|\longrightarrow |\mathcal N|\]
  vérifiant les propositions suivantes~:
  \begin{itemize}
  \item pour tout symbole de fonction $f$ d'arité $n$ et tout tuple
    $(m_1,\ldots,m_n)\in|\mathcal M|^n$~:
    \[\varphi(f^{\mathcal M}(m_1,\ldots,m_n))
    = f^{\mathcal N}(\varphi(m_1),\ldots,\varphi(m_n))\]
  \item pour tout symbole de relation $r$ d'arité $n$ et tout tuple
    $(m_1,\ldots,m_n)\in|\mathcal M|^n$~:
    \[r^{\mathcal M}(m_1,\ldots,m_n) \implies
    r^{\mathcal N}(\varphi(m_1),\ldots,\varphi(m_n)) \]
  \end{itemize}
\end{definition}

\begin{exercise}
  Montrer que pour toute signature $\Sigma$ et toute structure $\mathcal M$ sur
  $\Sigma$, la fonction $\id_{|\mathcal M|}$ induit un morphisme de $\mathcal M$
  vers elle-même. On notera $\id_\mathcal M$ ce morphisme.
\end{exercise}

\begin{exercise}
  Montre que l'opération $\circ$, de composition, s'étend en une opération sur
  les morphismes de structures, c'est-à-dire que si $f$ et $g$ sont deux
  morphismes tels que $\dom(g) = \im(f)$, alors $g\circ f$ est aussi un
  morphisme.
\end{exercise}

\begin{exercise}\label{pred.exo.morph.val}
  Montrer que pour tout morphisme de structure
  $\varphi : \mathcal M \to \mathcal N$, terme $t$ sur la même signature et
  environnement $\rho$ tel que $\VL(t) \subseteq \dom(\rho)$, on a l'égalité
  suivante~:
  \[t_\rho^\mathcal M = t_{\varphi\circ\rho}^\mathcal N\]
\end{exercise}

Un renforcement de la notion de morphisme est celle de plongement~: un
plongement est un morphisme dont l'image est isomorphe au domaine, c'est-à-dire
que non seulement le morphisme est injectif, mais le comportement vis à vis
des propositions atomiques peut s'étudier dans l'image uniquement en étudiant le
modèle de départ.

\begin{definition}[Plongement]
  Un plongement d'une structure $\mathcal M$ vers une structure $\mathcal N$ est
  un morphisme pour lequel la deuxième condition n'est plus une implication mais
  une équivalence~:
  \[\forall(m_1,\ldots,m_n)\in|\mathcal M|^n,r^{\mathcal M}(m_1,\ldots,m_n) \iff
  r^{\mathcal N}(\varphi(m_1),\ldots,\varphi(m_n))\]
\end{definition}

\begin{remark}
  Puisque nous avons toujours la relation $=$ dont l'interprétation est
  l'égalité dans son sens naturel, on en déduit qu'un plongement est injectif.
\end{remark}

Enfin, la notion d'isomorphisme est celle à laquelle on s'attend.

\begin{definition}[Isomorphisme]
  Un isomorphisme $\varphi : \mathcal M \cong \mathcal N$ est un morphisme de
  $\mathcal M$ vers $\mathcal N$ tel qu'il existe un morphisme $\psi$ de
  $\mathcal N$ vers $\mathcal M$ vérifiant
  \[\begin{cases}
  \varphi\circ \psi = \id_{\mathcal N}\\
  \psi\circ\varphi = \id_{\mathcal M}
  \end{cases}\]
\end{definition}

Assez naturellement, un isomorphisme préserve les formules. Cela se voit par
exemple en théorie des groupes, où on n'étudie les groupes qu'à isomorphisme
près puisque toute formule qu'on voudrait énoncer reste stable en passant à
travers un isomorphisme.

\begin{proposition}
  Si $\varphi : \mathcal M \cong \mathcal N$ alors pour toute formule close $A$,
  $\mathcal M\models A$ si et seulement si $\mathcal N\models A$.
\end{proposition}

\begin{proof}
  On montre par induction sur la structure de $A$ que pour tout environnement
  $\rho : \Var\partialto |\mathcal M|$ tel que $\VL(A) \subseteq\dom(\rho)$,
  on a l'équivalence
  \[\mathcal M,\rho\models A \iff \mathcal N,\varphi\circ \rho\models A\]
  \begin{itemize}
  \item si $A$ est de la forme $r(t_1,\ldots,t_n)$ avec un symbole de relation
    $r$, alors comme $\varphi$ est un morphisme on sait que si
    $(t_{1,\rho}^\mathcal M,\ldots,t_{n,\rho}^\mathcal M)\in r^\mathcal M$, alors
    $(t_{1,\varphi\circ \rho}^\mathcal N,\ldots,
    t_{n,\varphi\circ\rho}^\mathcal N)\in r^{\mathcal N}$, comme on sait qu'on a
    un morphisme réciproque $\psi : \mathcal N \to \mathcal M$, on peut
    écrire $t_{i,\rho}^\mathcal M = t_{i,\psi\circ\varphi\circ\rho}^\mathcal M$, donc
    l'argument précédent appliqué à $\psi$ permet de déduire que si
    $\mathcal N,\varphi\circ \rho\models A$ alors $\mathcal M,\rho\models A$.
  \item si $A$ est $\top$ ou $\bot$, le résultat est direct puisqu'indépendant
    de la structure dans laquelle on l'évalue.
  \item si $A$ est de la forme $\lnot B$, alors
    \[\mathcal M, \rho\models \lnot B\iff \mathcal M, \rho\not\models B
    \iff \mathcal N,\varphi\circ\rho\not\models B
    \iff \mathcal N,\varphi\circ\rho\models \lnot B\]
  \item si $A$ est de la forme $B\lor C$, alors
    \begin{align*}
      \mathcal M, \rho \models B\lor C &\iff (\mathcal M,\rho\models B)\lor
      (\mathcal M,\rho\models C) \\
      &\iff (\mathcal N,\varphi\circ\rho\models B)\lor
      (\mathcal N,\varphi\circ\rho\models C)\\
      &\iff \mathcal N,\varphi\circ \rho \models B\lor C
      \end{align*}
  \item si $A$ est de la forme $B\land C$, alors
    \begin{align*}
      \mathcal M, \rho \models B\land C &\iff (\mathcal M,\rho\models B)\land
      (\mathcal M,\rho\models C) \\
      &\iff (\mathcal N,\varphi\circ\rho\models B)\land
      (\mathcal N,\varphi\circ\rho\models C)\\
      &\iff \mathcal N,\varphi\circ \rho \models B\land C
      \end{align*}
  \item si $A$ est de la forme $B\to C$, alors
    \begin{align*}
      \mathcal M,\rho\models B\to C &\iff
      \mathcal M,\rho\models (\lnot B)\lor C\\
      &\iff \mathcal N,\varphi\circ\rho\models (\lnot B) \lor C\\
      &\iff \mathcal N,\varphi\circ\rho\models B \to C
    \end{align*}
  \item si $A$ est de la forme $\forall x,B$, on montre le résultat par double
    implication. Si pour tout
    $m \in |\mathcal M|, \mathcal M,\rho[x\leftarrow m]\models B$, alors
    pour tout $n \in |\mathcal N|$, par surjectivité de $\varphi$, on trouve
    $m \in |\mathcal M|$ tel que $\varphi(m) = n$. On sait alors que
    $\mathcal M,\rho[x\leftarrow m]\models B$ donc, par hypothèse d'induction,
    que $\mathcal N,\varphi\circ(\rho[x\leftarrow m])\models B$, ce qui revient
    à $\mathcal N,(\varphi\circ\rho)[x\leftarrow \varphi(m)]\models B$, donc
    $\mathcal N,(\varphi\circ\rho)[x\leftarrow n]\models B$, d'où le résultat.
    Dans le sens réciproque, si pour tout
    $n\in|\mathcal N|, \mathcal N,(\varphi\circ\rho)[x\leftarrow n]\models B$,
    on peut faire le même raisonnement en écrivant tout $m \in |\mathcal N|$
    comme $\varphi(\varphi^{-1}(m))$ et utiliser l'hypothse d'induction pour en
    déduire que $\mathcal M,\rho\models \forall x,B$.
  \item le cas où $A$ est de la forme $\exists x, B$ est analogue au précédent.
  \end{itemize}
  D'où le résultat par induction. On a le résultat dans le cas particulier d'une
  formule close.
\end{proof}

On a montré qu'un isomorphisme était un morphisme bijectif~: il est faux en
toute généralité de considérer que cette propriété caractérise un isomorphisme,
comme le montre l'exercice suivant.

\begin{exercise}
  On considère les deux ensembles ordonnés suivants~:
  \begin{itemize}
  \item $\{0,1\}$ où $0$ et $1$ ne sont pas comparables
  \item $\{\bot,\top\}$ où $\bot \leq \top$
  \end{itemize}
  Construire un morphisme bijectif entre les deux ensembles ordonnés qui n'est
  pas un isomorphisme.
\end{exercise}

Cependant, pour un plongement, ce résultat devient valide.

\begin{proposition}
  Un morphisme $\varphi : \mathcal M \to \mathcal N$ est un isomorphisme si et
  seulement si c'est un plongement surjectif.
\end{proposition}

\begin{proof}
  Si $\varphi$ est un isomorphisme, alors c'est en particulier une bijection,
  le rendant surjectif. De plus, comme $\varphi$ préserve toutes les
  propositions dans les deux sens, ce morphisme préserve en particulier les
  propositions atomiques, c'est donc un plongement.

  Si $\varphi$ est un plongement surjectif, alors c'est une bijection~: on
  peut donc considérer la réciproque $\psi$ de $\varphi$. Il nous reste à
  prouver que $\psi$ est bien un morphisme~: la préservation des termes est
  automatique, et il nous faut encore montrer que pour tout symbole de relation
  $r$ d'arité $p$ et $n_1,\ldots,n_p\in|\mathcal N|$~:
  \[r^\mathcal N(n_1,\ldots,n_p) \implies r^\mathcal M
  (\psi(n_1),\ldots,\psi(n_p))\]
  mais chaque $n_1,\ldots,n_p$ peut se réécrire
  $\varphi(m_1),\ldots,\varphi(m_p)$, donnant
  \[r^\mathcal N(\varphi(m_1),\ldots,\varphi(m_p)) \implies r^\mathcal M
  (\psi(\varphi(m_1)),\ldots,\psi(\varphi(m_p)))\]
  Or on sait que $\psi\circ \varphi = \id$, donc la condition revient à la
  réciproque de celle disant que $\varphi$ est un morphisme~: c'est justement
  ce que nous donne le fait que $\varphi$ est un plongement. Donc $\varphi$ est
  un isomorphisme.
\end{proof}

\subsection{Agrandir des modèles}

Pour conclure cette partie introductive à propos des modèles, nous allons
définir la notion d'enrichissement.

\begin{definition}[Enrichissement]
  Soit deux signatures $\Sigma\subseteq\Sigma'$, c'est-à-dire telles que tout
  symbole de $\Sigma$ est un symbole de $\Sigma'$ de même arité. Soit une
  structure $\mathcal M'$ sur $\Sigma'$, alors on dit que $\mathcal M'$ est un
  enrichissement d'une structure $\mathcal M$ sur $\Sigma$ si~:
  \begin{itemize}
  \item $|\mathcal M| = |\mathcal M'|$
  \item pour tout symbole de fonction $f\in \Sigma$,
    $f^{\mathcal M} = f^{\mathcal M'}$
  \item pour tout symbole de fonction $r\in \Sigma$,
    $r^{\mathcal M} = r^{\mathcal M'}$
  \end{itemize}

  On définit l'appauvrissement de $\mathcal M'$ sur $\Sigma$ comme l'unique
  structure sur $\Sigma$ dont $\mathcal M'$ est l'enrichissement.
\end{definition}

\begin{remark}
  On peut toujours appauvrir une strucutre, mais il n'est pas toujours possible
  d'enrichir (ou du moins naturellement) une structure sur une extension de sa
  signature.
\end{remark}

Par exemple, le langage des anneaux est un enrichissement du langage des
groupes. On remarque qu'un anneau est toujours, en particulier, un groupe
additif~: nous allons généraliser ce résultat pour une structure sur un
enrichissement.

\begin{proposition}
  Soit deux signatures $\Sigma\subseteq\Sigma'$, $\mathcal M'$ une structure
  sur $\Sigma'$ et $\mathcal M$ son appauvrissement sur $\Sigma$, alors pour
  toute formule $A\in\Formula(\Sigma)$ et tout environnement $\rho$, on a
  \[\mathcal M,\rho\models A \iff \mathcal M',\rho\models A\]
\end{proposition}

\begin{proof}
  On montre d'abord que pour tout terme $t\in\Term(\Sigma)$,
  $t_\rho^\mathcal M = t_\rho^{\mathcal M'}$. Cela se fait par induction~: pour une
  variable $x$, on a $\rho(x) = \rho(x)$ et si l'on suppose que pour tout
  $i\in\{1,\ldots,n\}$ on a $(t_i)_\rho^\mathcal M = (t_i)_\rho^{\mathcal M'}$ alors
  \[(f(t_1,\ldots,t_n))_\rho^\mathcal M = (f(t_1,\ldots,t_n))_\rho^{\mathcal M'}\]
  d'où le résultat pour tout terme $t$.
  
  On procède maintenant par induction sur la structure de $A$~:
  \begin{itemize}
  \item si $A = r(t_1,\ldots,t_n)$ alors
    \begin{align*}
      \Val_\rho^\mathcal M(r(t_1,\ldots,t_n)) &= r^\mathcal M((t_1)_\rho^\mathcal M,
      \ldots,(t_n)_\rho^\mathcal M) \\
      &= r^{\mathcal M'}((t_1)_\rho^{\mathcal M'},\ldots,(t_n)_{\rho}^{\mathcal M'})
    \end{align*}
  \item si $A$ est un connecteur logique, alors le résultat est direct.
  \item si $A = \forall x, B$ ou $A = \exists x, B$ alors la quantification se
    fait sur le même ensemble pour $\mathcal M$ et $\mathcal M'$ donc le
    résultat est le même dans les deux cas (en utilisant l'hypothèse
    d'induction).
  \end{itemize}
  Donc par induction $\Val_\rho^\mathcal M(A) = \Val_\rho^{\mathcal M'}(A)$,
  ce qui est le résultat attendu.
\end{proof}

\begin{corollary}
  Soit deux signatures $\Sigma\subseteq\Sigma'$, une structure $\mathcal M'$
  et une théorie $\mathcal T$ sur $\Sigma$. Si $\mathcal M'\models \mathcal T$,
  alors en prenant l'appauvrissement $\mathcal M$ de $\mathcal M'$ sur $\Sigma$,
  $\mathcal M\models \mathcal T$.
\end{corollary}

La notion d'enrichissement est fondamentale en théorie des modèles~: elle
permet d'avoir un meilleur contrôle sur les modèles qu'on manipule en y ajoutant
des informations par enrichissement, pour ensuite les appauvrir de nouveau vers
le langage d'origine. C'est par exemple l'idée menant à la méthode des
diagrammes, développés dans la partie dédiée à la théorie des modèles.

\section[Syntaxe de preuves]{Système de démonstration du calcul des prédicats}

Nous avons défini la sémantique des formules à travers la notion de modèle. Pour
suivre ce que nous avons fait dans le chapitre précédent, nous allons maintenant
introduire un formalisme pour décrire mathématiquement des preuves en calcul des
prédicats.

Nous avons alors plusieurs choix, car plusieurs formalismes existent. Les trois
principaux sont les sytèmes à la Hilbert, le calcul des séquents et la déduction
naturelle. Nous avons eu un aperçu du calcul des séquents dans le chapitre
précédent, et il pourrait être pertinent de continuer à l'utiliser, mais le
choix fait ici est d'introduire un nouveau formalisme~: celui de la déduction
naturelle. Ce choix se justifie par deux raisons principales~:
\begin{itemize}
\item Tout d'abord, il permet d'aborder d'autres systèmes de preuves, puisque
  nous avons déjà exploré le calcul des séquents. Cet argument est en même temps
  un contre-argument, puisque cela signifie aussi que l'on peut trouver là une
  occasion de réexplorer le calcul des séquents pour mieux le comprendre, c'est
  donc seulement une raison mineure qui nous pousse à le choisir. De plus, le
  formalisme des systèmes à la Hilbert ne sera pas exploré du tout dans cet
  ouvrage, car s'il est très simple à définir, il possède peu de propriétés
  intéressantes à explorer contrairement aux deux autres formalismes et n'a rien
  de naturel à manipuler.
\item La deuxième raison, plus importante, est que la déduction naturelle est en
  quelque sorte le raisonnement le plus primitif d'un mathématicien. Chaque
  règle exprime une règle tout à fait évidente à l'intuition, particulièrement à
  celle du mathématicien, et on peut trouver une correspondance importante
  entre une preuve utilisant la déduction naturelle et une preuve en langage
  naturel (à la différence évidente que la première est illisible pour un
  profane quand la deuxième est\ldots illisible aussi pour un profane, mais il y
  a moins de profanes des mathématiques que de profanes de la déduction
  naturelle).
\end{itemize}

Il y a de nombreux avantages à définir une syntaxe pour nos preuves en calcul
des prédicats. Le premier est évident~: avoir, comme dans le chapitre précédent,
un système efficace pour prouver des relations entre formules, ne passant pas
par une interprétation et une quantification sur toutes les valuations (et ici,
en plus~: tous les modèles). Le deuxième, moins évident, sera plus lourd de
conséquences~: une syntaxe nous permet de voir l'ensemble de l'activité
mathématique comme un processus finitaire. Nous n'avons le droit que d'employer
des phrases finies en des textes finis pour étudier des objets potentiellement
infinis. Si ce fait semble en premier lieu purement philosophique (et il est
effectivement important, philosophiquement parlant) il mène aussi à une
conséquence importante~: le théorème de compacité. Ce théorème est un analogue
au \cref{thm.compac.prop} dans le cas du calcul des prédicats. Simplement, au
lieu de quantifier sur les valuations, nous quantifions sur les modèles, et
puisque nous nous occupons en priorité des formules closes cela nous donne un
énoncé parlant uniquement de modèles.

\subsection{Déduction naturelle}

Commençons par définir la relation $\vdash$ de conséquence syntaxique. Pour
cela, comme pour le calcul des séquents, on va définir un système travaillant
sur des listes (par nature finies) plutôt que sur des ensembles. Une différence
importante~: nous n'allons pas relier deux listes, mais une liste avec une
proposition. Ainsi un séquent $\Gamma\vdash A$ signifie directement que, sous
les hypothèses listées dans $\Gamma$, la proposition $A$ est vraie.

\begin{definition}[Déduction naturelle]
  Soit une signature $\Sigma$. On définit la relation
  $\vdash\subseteq \List(\Formula(\Sigma))\times \Formula(\Sigma)$ par induction
  par les règles suivantes~:
  \begin{center}
    \AxiomC{$A \in \Gamma$}
    \RightLabel{Ax}
    \UnaryInfC{$\Gamma\vdash A$}
    \DisplayProof
    \qquad
    \AxiomC{}
    \RightLabel{$\top$}
    \UnaryInfC{$\Gamma\vdash \top$}
    \DisplayProof
    \qquad
    \AxiomC{$\Gamma,\lnot A\vdash \bot$}
    \RightLabel{$\bot_\mathrm c$}
    \UnaryInfC{$\Gamma\vdash A$}
    \DisplayProof

    \vspace{0.5cm}
    \AxiomC{$\Gamma,A\vdash \bot$}
    \RightLabel{$\lnot_\mathrm i$}
    \UnaryInfC{$\Gamma\vdash \lnot A$}
    \DisplayProof
    \qquad
    \AxiomC{$\Gamma\vdash \lnot A$}
    \AxiomC{$\Gamma\vdash A$}
    \RightLabel{$\lnot_\mathrm e$}
    \BinaryInfC{$\Gamma\vdash \bot$}
    \DisplayProof

    \vspace{0.5cm}
    \AxiomC{$\Gamma\vdash A$}
    \RightLabel{$\lor_\mathrm i^\mathrm g$}
    \UnaryInfC{$\Gamma\vdash A\lor B$}
    \DisplayProof
    \quad
    \AxiomC{$\Gamma\vdash B$}
    \RightLabel{$\lor_\mathrm i^\mathrm d$}
    \UnaryInfC{$\Gamma\vdash A\lor B$}
    \DisplayProof
    \qquad
    \AxiomC{$\Gamma\vdash A\lor B$}
    \AxiomC{$\Gamma,A\vdash C$}
    \AxiomC{$\Gamma,B\vdash C$}
    \RightLabel{$\lor_\mathrm e$}
    \TrinaryInfC{$\Gamma\vdash C$}
    \DisplayProof

    \vspace{0.5cm}
    \AxiomC{$\Gamma\vdash A$}
    \AxiomC{$\Gamma\vdash B$}
    \RightLabel{$\land_\mathrm i$}
    \BinaryInfC{$\Gamma\vdash A\land B$}
    \DisplayProof
    \qquad
    \AxiomC{$\Gamma\vdash A\land B$}
    \RightLabel{$\land_\mathrm e^\mathrm g$}
    \UnaryInfC{$\Gamma\vdash A$}
    \DisplayProof
    \quad
    \AxiomC{$\Gamma\vdash A\land B$}
    \RightLabel{$\land_\mathrm e^\mathrm d$}
    \UnaryInfC{$\Gamma\vdash B$}
    \DisplayProof

    \vspace{0.5cm}
    \AxiomC{$\Gamma,A\vdash B$}
    \RightLabel{$\to_\mathrm i$}
    \UnaryInfC{$\Gamma\vdash A\to B$}
    \DisplayProof
    \qquad
    \AxiomC{$\Gamma\vdash A\to B$}
    \AxiomC{$\Gamma\vdash A$}
    \RightLabel{$\to_\mathrm e$}
    \BinaryInfC{$\Gamma\vdash B$}
    \DisplayProof

    \vspace{0.5cm}
    \AxiomC{$\Gamma\vdash A[v/x]$}
    \RightLabel{$\forall_\mathrm i^\dagger$}
    \UnaryInfC{$\Gamma\vdash \forall x, A$}
    \DisplayProof
    \qquad
    \AxiomC{$\Gamma\vdash \forall x, A$}
    \RightLabel{$\forall_\mathrm e$}
    \UnaryInfC{$\Gamma\vdash A[t/x]$}
    \DisplayProof

    \vspace{0.5cm}
    \AxiomC{$\Gamma\vdash A[t/x]$}
    \RightLabel{$\exists_\mathrm i$}
    \UnaryInfC{$\Gamma\vdash \exists x, A$}
    \DisplayProof
    \qquad
    \AxiomC{$\Gamma\vdash \exists x, A$}
    \AxiomC{$\Gamma, A[v/x]\vdash B$}
    \RightLabel{$\exists_\mathrm e^\dagger$}
    \BinaryInfC{$\Gamma\vdash B$}
    \DisplayProof

    \vspace{0.5cm}
    \AxiomC{}
    \RightLabel{$=_\mathrm i$}
    \UnaryInfC{$\Gamma\vdash t = t$}
    \DisplayProof
    \qquad
    \AxiomC{$\Gamma\vdash A[t/x]$}
    \AxiomC{$\Gamma\vdash t = u$}
    \RightLabel{$=_\mathrm e$}
    \BinaryInfC{$\Gamma\vdash A[u/x]$}
    \DisplayProof
  \end{center}
  Où $x,v\in \Var$ et où $t,u\in \Term(\Sigma)$.
  
  Les règles avec $^\dagger$ signifient que
  $v\notin\VL(\Gamma)\cup\VL(A)\cup\VL(B)$.

  De plus, on définit la relation
  $\vdash\subseteq\powerset(\Formula(\Sigma))\times\Formula(\Sigma)$ par
  $\mathcal F\vdash A$ si et seulement s'il existe $\Gamma\in\List(\mathcal F)$
  tel que $\Gamma\vdash A$.
\end{definition}

\begin{exercise}[Sur la négation]
  Montrer que l'on peut prouver les deux séquents suivants~:
  \[\vdash \lnot A \to (A \to \bot) \qquad \vdash (A \to \bot) \to \lnot A\]

  Montrer de plus que $\lnot A \vDash A \to \bot$ et que
  $A\to \bot\vDash \lnot A$.
\end{exercise}

On peut donc, sans perdre d'expressivité, redéfinir $\lnot$ comme l'opération
$A \mapsto A \to \bot$. Cela nous permet alors de réduire le nombre de règles
dans notre système.

\begin{exercise}[Sur l'implication]
  Montrer que l'équivalence suivante est prouvable~:
  \[A \to B \dashv\vdash \lnot A \lor B\]
  où $A \dashv\vdash B$ signifie que $A\vdash B$ et $B\vdash A$.
\end{exercise}

\begin{exercise}[De Morgan]
  Montrer que les lois de De Morgan sont dérivables~:
  \begin{itemize}
  \item $\lnot (A \lor B) \dashv\vdash \lnot A \land \lnot B$
  \item $\lnot (A \land B) \dashv\vdash \lnot A \lor \lnot B$
  \item $\lnot (\exists x, A) \dashv\vdash \forall x, \lnot A$
  \item $\lnot (\forall x, A) \dashv\vdash \exists x, \lnot A$
  \item $\lnot\lnot A \dashv\vdash A$
  \item $\lnot (A\to B) \dashv\vdash A \land \lnot B$
  \end{itemize}
\end{exercise}

\begin{exercise}[Tiers exclu et non contradiction]
  Montrer les deux équivalences suivantes~:
  \begin{itemize}
  \item $A \lor \lnot A \dashv\vdash \top$
  \item $A \land \lnot A \dashv\vdash \bot$
  \end{itemize}
\end{exercise}

\begin{remark}
  A partir des exercices précédents, on peut être tenté de réduire l'ensemble
  des formules et des règles à un fragment tel que les propositions atomiques,
  $\lnot$, $\lor$ et $\exists$. En effet, toute formule est équivalent à une
  formule écrite avec ce fragment : on peut donc imaginer que toute autre
  formule est en fait une simple écriture plus lisible de celles constituées
  uniquement du fragment restreint.

  Nous n'emploierons pas cette méthode de restriction car, structurellement,
  il n'est pas évident par exemple que $\lnot (\lnot A \lor \lnot B)$, qui code
  $A \land B$, s'utilise de la même façon au niveau des règles. Si l'on sait que
  l'on peut se ramener en utilisant certaines règles de l'un à l'autre, il
  faudrait travailler à montrer en plus que les règles à propos de $A\land B$
  peuvent se dériver des règles de $\lnot$ et $\lor$ sur
  $\lnot (\lnot A \lor \lnot B)$. Mais cela motive aussi un déroulement plus
  lent des preuves par induction et des différents cas, qui s'ils sont
  laborieux peuvent pour autant être instructifs pour une première lecture.

  Le cas de $\lnot$, que l'on peut remplacer par $\to \bot$, est différent~:
  les règles à propos de $\lnot$ sont exactement celles de $\to \bot$.
\end{remark}

\begin{remark}
    Avec l'écriture actuelle des formules, deux écritures utiles nous
    manquent~:
    \begin{itemize}
        \item le connecteur $\leftrightarrow$, qui est traité dans
        l'\cref{exo.equiv}
        \item le quantificateur $\exists ! x, \varphi(x)$. Celui-ci peut
        être encodé par
        \[\exists!x, \varphi(x) \defeq \exists x, \varphi(x) \land
        (\forall y, \varphi(y) \to x = y)\]
    \end{itemize}
    On utilisera donc ces deux écritures, permettant une meilleure lisibilité
    mais ne changeant pas l'expressivité de notre formalisme.
\end{remark}

\paragraph{A propos de la vacuité}
Notre formalisme a un problème essentiel : on peut prouver la proposition
$\forall x, A \implies \exists x, A$, qui est fausse dans le modèle vide. Deux
façons permettent de régler cet écart entre la syntaxe et la sémantique :
changer la syntaxe, ou changer la sémantique. Dans notre cas, nous changeons
alors la sémantique en considérant que toute structure (et donc tout modèle)
est non vide. Cette restriction n'est pas très limitante, puisque le modèle vide
est inintéressant en général. Cependant, celle-ci parait particulièrement
artificielle. Pour contrer cela, on peut à la place considérer des séquents
enrichis de la forme $\Gamma\mid \Theta \vdash \varphi$ où $\Gamma$ va être un
contexte de variables, $\Theta$ un contexte logique et $\varphi$ la conclusion.
Dans ce formalisme, les règles avec $\dagger$ ont, plutôt qu'une restriction,
une action sur le contexte des variables, avec par exemple
\begin{prooftree}
  \AxiomC{$\Gamma, v\mid \Theta\vdash \varphi[v/x]$}
  \RightLabel{$\forall_\mathrm i$}
  \UnaryInfC{$\Gamma\mid \Theta\vdash \forall x, \varphi$}
\end{prooftree}
où $\Gamma\mid\Theta\vdash \forall x, \varphi$ doit être une proposition bien
typée, imposant ansi que $v\notin\VL(\Theta)\cup\VL(\varphi)$. On peut par
exemple citer \cite{JacobsCLTT} pour trouver un traitement de la syntaxe qui
prend en compte les variables introduites.

Le fait de gérer les variables est bien plus naturel, étant donné qu'une preuve
en langage naturel va toujours tenir compte des variables (en particulier, il
semblerait incongru de mentionner une variable non déjà introduite dans le
contexte), et dans un contexte avec plusieurs sortes, c'est-à-dire où les
variables du premier ordre peuvent appartenir à différents ensembles, ce
formalisme gagne en utilité. Dans notre cas, il parait trop lourd de devoir
gérer les variables pour simplement pouvoir inclure le cas du modèle vide,
c'est pourquoi nous préférons simplement modifier notre sémantique.

Plutôt que de prouver directement des résultats, nous allons nous attarder sur
le sens de chaque règle, pour montrer en quoi elles sont intuitives (et donc
robustes au niveau de l'évidence qu'elles énoncent) et permettent de
retranscrire n'importe quelle preuve (en particulier, toute preuve en langage
naturel est virtuellement équivalente à un arbre de preuve en déduction
naturelle).
\begin{itemize}
\item La règle Ax est surement la plus évidente : si $A$ est une hypothèse,
  alors on peut en déduire $A$.
\item La règle $\top$ dit simplement que $\top$ est toujours prouvable.
\item La règle $\bot_\mathrm c$ dit que pour prouver $A$, on peut supposer
  $\lnot A$ est aboutir à une contradiction : c'est le raisonnement par
  l'absurde, d'où l'indice \og c\fg{} exprimant que cette règle est propre à la
  logique classique (nous le verrons, remplacer cette règle par une autre plus
  faible a des conséquences particulièrement intéressantes).
\item La règle $\lnot_\mathrm i$ dit que pour prouver $\lnot A$, il suffit de
  prouver que $A$ aboutit à une absurdité.
\item La règle $\lnot_\mathrm e$ dit que prouver $A$ et $\lnot A$ en même temps
  est une absurdité.
\item Les règles $\lor_\mathrm i$ montrent que si on prouve $A$ (respectivement
  $B$) alors on prouve $A\lor B$.
\item La règle $\lor_\mathrm e$ montre que pour prouver $C$ à partir de
  $A\lor B$, il suffit de montrer $A\to C$ et $B\to C$~; ou, comme nous
  l'avons écrit, que l'on peut prouver $C$ à la fois sous l'hypothèse $A$ et
  sous l'hypothèse $B$. C'est un raisonnement par disjonction de cas.
\item La règle $\land_\mathrm i$ dit que pour prouver $A\land B$, il suffit de
  prouver $A$ d'une part, et $B$ d'autre part.
\item Les règles $\land_\mathrm e$ permettent d'affaiblir une preuve de
  $A\land B$ en une preuve de $A$ (respectivement de $B$).
\item La règle $\to_\mathrm i$ dit que prouver $A\to B$ signifie prouver $B$ en
  ajoutant l'hypothèse $A$.
\item La règle $\to_\mathrm e$ dit que si l'on a prouvé $A\to B$ et $A$, alors on
  peut en déduire $B$. C'est la règle du \latinexpr{modus ponens}.
\item La règle $\forall_\mathrm i$ signifie que pour prouver $\forall x, A$, il
  suffit de prouver $A$ pour une variable $v$ quelconque à la place de $x$. La
  nécessité que $v\notin\VL(\Gamma)\cup\VL(A)$ exprime que ce $v$ est
  quelconque~: aucune hypothèse n'est faite sur celui-ci.
\item La règle $\forall_\mathrm e$ signifie qu'à partir d'une preuve de
  $\forall x, A$ on peut instancier $x$ à un terme $t$ quelconque pour obtenir
  une preuve de $A[t/x]$.
\item La règle $\exists_\mathrm i$ permet de déduire une preuve de $\exists x,A$
  à partir d'une preuve de $A[t/x]$, pour n'importe quel terme $t$.
\item La règle $\exists_\mathrm e$ dit qu'à partir d'une proposition de la forme
  $\exists x, A$, on peut déduire une proposition $B$ en ajoutant dans le
  contexte $A[v/x]$, où $v$ est quelconque (ce qui se traduit par la condition
  de $v\notin\VL(\Gamma)\cup\VL(A)\cup\VL(B)$).
\item La règle $=_\mathrm i$ est la réflexivité de l'égalité : un terme est
  égal à lui-même.
\item La règle $=_\mathrm e$, parfois appelée principe de Leibniz, exprime que
  si deux termes $t$ et $u$ sont égaux, alors ils vérifient les mêmes formules.
  On appelle aussi ce principe \og indiscernabilité des identités\fg{}.
\end{itemize}

\subsection[Théorème de complétude]{Complétude de la déduction naturelle}

Nous allons maintenant montrer que $\vDash$ et $\vdash$ coïncident. Un sens est
élémentaire : montrer que $\vdash \subseteq\vDash$, nous allons donc le traiter
en premier. C'est la propriété qu'on appelle correction. Elle énonce que ce que
notre syntaxe dérive est valide. L'autre sens, disant que tout ce qui est
valide est dérivable, est très souvent plus technique~: ce fut le cas pour
la logique propositionnelle, c'est encore le cas pour le calcul des prédicats.

On appelle en général, par abus de langage, complétude du système la propriété
que $\vdash = \vDash$, plutôt que simplement la propriété
$\vDash\subseteq\vdash$. Il est en effet peu pratique de devoir citer deux
théorèmes différents lorsque l'on parle de la correspondance des deux relations,
c'est pourquoi on préfère englober les deux en un résultat, et comme le sens le
plus technique est celui de complétude, c'est celui qu'on utilise pour nommer le
théorème.

Comme $\vdash$ est définie par induction, la preuve de correction est
directement une induction sur sa structure. Remarquons cependant qu'il est
nécessaire de pouvoir prendre en compte les variables, et donc d'introduire des
environnements en plus. Nous allons donc utiliser la \cref{prop.comm.subst.env}.

\begin{theorem}[Correction]\label{thm.correction}
  Soit une signature $\Sigma$, une liste $\Gamma\in\List(\Formula(\Sigma))$ et
  une formule $A\in\Formula(\Sigma)$. Si $\Gamma\vdash A$ alors, en notant
  $X_\Gamma$ l'ensemble des formules dans $\Gamma$, pour toute structure
  $\mathcal M$, tout environnement $\rho$ sur $\mathcal M$, si
  $\mathcal M,\rho\models X_\Gamma$ alors $\mathcal M,\rho\models A$.
\end{theorem}

\begin{proof}
  Nous allons prouver ce résultat par induction sur $\Gamma\vdash A$, en
  supposant introduits $\mathcal M$ et $\rho$ :
  \begin{itemize}
  \item Ax~: Si $A\in \Gamma$, alors il est évident que
    $\mathcal M,\rho\models A$.
  \item $\top$~: Peu importe les prémisses, $\mathcal M,\rho\models \top$.
  \item $\bot_\mathrm c$~: Supposons qu'aucun modèle $\mathcal M$ et aucun
    environnement $\rho$ ne sont tels que
    $\mathcal M,\rho\models X_\Gamma\cup\{\lnot A\}$. Alors si
    $\mathcal M,\rho\models X_\Gamma$, on ne peut pas avoir
    $\mathcal M,\rho\models \lnot A$, donc $\Val_\rho(\lnot A) = 0$, donc
    $\Val_\rho(A) = 1$, d'où $\mathcal M,\rho\models A$.
  \item $\lnot_\mathrm i$~: Supposons qu'aucun $(\mathcal M,\rho)$ ne vérifie que
    $\mathcal M,\rho\models X_\Gamma\cup\{A\}$, alors si
    $\mathcal M,\rho\models X_\Gamma$, $\Val_\rho(A) = 0$ donc
    $\Val_\rho(\lnot A) = 1$, d'où $\mathcal M,\rho\models \lnot A$.
  \item $\lnot_\mathrm e$~: Supposons qu'un modèle de $\Gamma$ est un modèle à la
    fois de $A$ et de $\lnot A$. Alors $\Val_\rho(A) = 1$ et
    $\Val_\rho(\lnot A) = 1$, mais $\Val_\rho(\lnot A) = 1 - \Val_\rho(A)$, donc
    $0 = 1$~: c'est absurde, donc il n'y a pas de modèle de $\Gamma$.
  \item $\lor_\mathrm i^\mathrm g$~: Supposons que si
    $\mathcal M,\rho\models X_\Gamma$ alors $\mathcal M,\rho\models A$. Soit
    $\mathcal M,\rho\models X_\Gamma$, par
    hypothèse $\Val_\rho(A) = 1$, donc
    $\Val_\rho(A\lor B) = \max(1,\Val_\rho(B))$, donc
    $\mathcal M,\rho\models A\lor B$.
  \item $\lor_\mathrm i^\mathrm d$~: L'argument précédent fonctionne exactement de
    la même manière.
  \item $\lor_\mathrm e$~: A partir de maintenant, nous n'expliciterons plus les
    hypothèses d'induction, ni ce que l'on cherche à prouver, pour gagner de la
    place. Supposons que $\mathcal M,\rho\models X_\Gamma$. Alors par hypothèse
    d'induction, $\mathcal M,\rho\models A\lor B$, donc
    $\max(\Val_\rho(A),\Val_\rho(B)) = 1$~: on en déduit qu'au moins l'un des
    deux entre $A$ et $B$ est tel que $\Val_\rho = 1$. Sans perte de généralité,
    supposons que $\Val_\rho(A) = 1$. On sait donc que
    $\mathcal M,\rho\models X_\Gamma\cup\{A\}$, donc par hypothèse d'induction
    $\mathcal M,\rho\models C$.
  \item $\land_\mathrm i$~: Supposons que $\mathcal M,\rho\models X_\Gamma$,
    alors par hypothèse d'induction $\Val_\rho(A) = 1$ et $\Val_\rho(B) = 1$,
    donc $\Val_\rho(A\land B) = \min(1,1) = 1$, donc
    $\mathcal M,\rho\models A\land B$.
  \item $\land_\mathrm e^\mathrm g$~: Supposons que
    $\mathcal M,\rho\models X_\Gamma$, alors par hypothèse d'induction
    $\Val_\rho(A\land B) = 1$, donc $\Val_\rho(A) = 1$ car cette valeur est
    supérieure à $\Val_\rho(A\land B)$~: donc $\mathcal M,\rho\models A$.
  \item $\land_\mathrm e^\mathrm d$~: L'argument précédent fonctionne de la même
    manière.
  \item $\to_\mathrm i$~: Supposons que $\mathcal M,\rho\models X_\Gamma$. On
    travaille par disjonction de cas sur la valeur de $\Val_\rho(A)$~:
    \begin{itemize}
    \item si $\Val_\rho(A) = 0$ alors $\Val_\rho(A\to B) = 1$ donc
      $\mathcal M,\rho\models A \to B$.
    \item si $\Val_\rho(A) = 1$ alors $\mathcal M,\rho\models X_\Gamma\cup\{A\}$,
      donc par hypothèse d'induction $\mathcal M,\rho\models B$, donc
      $\Val_\rho(A\to B) = 1$, donc $\mathcal M,\rho\models A \to B$.
    \end{itemize}
  \item $\to_\mathrm e$~: Supposons que $\mathcal M,\rho\models X_\Gamma$. On sait
    donc que $\Val_\rho(A\to B) = 1$ et $\Val_\rho(A) = 1$. Ainsi,
    $\max(0,\Val_\rho(B)) = 1$~: on en déduit que $\Val_\rho(B) = 1$,
    c'est-à-dire que $\mathcal M,\rho\models B$.
  \item $\forall_\mathrm i$~: Supposons que $\mathcal M,\rho\models X_\Gamma$.
    On veut montrer que $\Val_\rho(\forall x, A) = 1$. Pour cela, soit
    $m \in |\mathcal M|$~: on remarque que, par hypothèse d'induction,
    $\mathcal M,\rho\models A[v/x]$, mais comme $v$ est libre dans $\Gamma$
    (c'est-à-dire que $v\notin\VL(\Gamma)$), on remarque que
    $\mathcal M,\rho[v\mapsto m]\models X_\Gamma$, soit
    $\mathcal M,\rho[v\mapsto m]\models A[v/x]$, d'où
    $\Val_{\rho[v\mapsto m]}(A[v/x]) = 1$, mais l'expression de gauche vaut
    $\Val_{\rho[x\mapsto m]}(A)$. Comme cela fonctionne pour tout $m$, on en
    déduit que $\Val_\rho(\forall x, A) = 1$.
  \item $\forall_\mathrm e$~: Supposons que $\mathcal M, \rho\models X_\Gamma$, on
    sait donc que $\mathcal M,\rho\models \forall x, A$, ce qui signifie en
    particulier que pour tout $m\in\mathcal M$,
    $\mathcal M,\rho[x\mapsto m]\models A$.
    Mais alors, en considérant $m = t^\mathcal M_\rho$, on trouve que
    $\Val_{\rho[x\mapsto t^\mathcal M_\rho]}(A) = 1$, mais l'expression de gauche
    correspond exactement à $\Val_\rho(A[t/x])$, donc
    $\mathcal M,\rho\models A[t/x]$.
  \item $\exists_\mathrm i$~: Supposons que $\mathcal M,\rho\models X_\Gamma$,
    alors $\mathcal M,\rho\models A[t/x]$, mais comme on sait que
    $\Val_\rho(A[t/x])\leq\Val_\rho(\exists x,A)$, on en déduit que
    $\Val_\rho(\exists x,A) = 1$, donc $\mathcal M,\rho\models \exists x,A$.
  \item $\exists_\mathrm e$~: Supposons que $\mathcal M,\rho\models X_\Gamma$,
    alors $\mathcal M,\rho\models \exists x,A$, donc on peut trouver un élément
    $m\in|\mathcal M|$ tel que $\Val_{\rho[x\mapsto m]}(A) = 1$. On a
    $\Val_{\rho[v\mapsto m]}(A[v/x]) = \Val_{\rho[x\mapsto m]}(A)$ car
    $v \notin\VL(A)$, et cette expression vaut $1$. On en déduit que
    $\mathcal M,\rho[v\mapsto m]\models A[v/x]$. De plus, comme
    $v\notin\VL(\Gamma)$, on a $\mathcal M,\rho[v\mapsto m]\models X_\Gamma$,
    donc par hypothèse d'induction $\mathcal M,\rho[v\mapsto m]\models B$.
    Mais $v\notin\VL(B)$, donc $\mathcal M,\rho\models B$.
  \item $=_\mathrm i$~: Pour tout modèle $\mathcal M$ et environnement $\rho$, on
    a forcément $\Val_\rho(t=t) = 1$ puisque
    $(t^\mathcal M_\rho,t^\mathcal M_\rho)\in\{(m,m)\mid m\in|\mathcal M|\}$.
  \item $=_\mathrm e$~: Supposons que $\mathcal M,\rho\models X_\Gamma$, alors
    $\mathcal M,\rho\models A[t/x]$ et $\mathcal M,\rho\models t = u$. On en
    déduit que $\Val_\rho(t = u) = 1$, c'est-à-dire que
    $t^\mathcal M_\rho = u^\mathcal M_\rho$, donc
    $\Val_{\rho[x\mapsto t^\mathcal M_\rho]}(A) = \Val_{\rho[x\mapsto u^\mathcal M_\rho]}(A)$,
    et à partir du fait que $\mathcal M,\rho\models A[t/x]$ on en déduit donc
    que $\mathcal M,\rho\models A[u/x]$.
  \end{itemize}

  Ainsi, par induction, si $\mathcal M,\rho\models X_\Gamma$, alors
  $\mathcal M,\rho\models A$. En particulier, si $\mathcal M\models X_\Gamma$,
  alors $\mathcal M\models A$, et si pour
  $\mathcal F\subseteq \Formula(\Sigma)$, on a
  $\mathcal M\models \mathcal F$ et $\mathcal F\vdash A$, cela implique donc que
  $\mathcal M\models A$. Ainsi $\vdash\subseteq\vDash$.
\end{proof}

Il nous reste à prouver le sens réciproque. En réalité, le point critique pour
la démonstration est celui de l'existence d'un modèle. Plutôt que de montrer
réellement que $\vDash\subseteq\vdash$, nous allons montrer qu'une théorie
consistante, c'est-à-dire une théorie $\mathcal T$ telle que
$\mathcal T\nvdash \bot$, possède un modèle. On va donc commencer par montrer
que notre résultat suffira à prouver $\vDash\subseteq\vdash$.

\begin{lemma}\label{lem.vDashvdashabs}
  Supposons que pour toute théorie $\mathcal T$ telle que
  $\mathcal T\nvdash\bot$, il existe une structure $\mathcal M$ telle que
  $\mathcal M\models \mathcal T$. Alors $\vDash\subseteq\vdash$.
\end{lemma}

\begin{proof}
  Supposons que $\mathcal F\vDash A$ pour $A\in\Formula(\Sigma)$ et
  $\mathcal F\subseteq\Formula(\Sigma)$. On voit donc que
  $\mathcal F\cup\{\lnot A\}$ n'a pas de modèle : par contraposée de notre
  hypothèse, cela signifie que $\mathcal F,\lnot A\vdash \bot$. En appliquant
  simplement $\bot_\mathrm c$, on en déduit que $\mathcal F\vdash A$. Ainsi
  $\vDash\subseteq\vdash$.
\end{proof}

\begin{remark}
  Nous avons défini une théorie comme un ensemble de formules closes, donc la
  démonstration précédente n'est pas tout à fait exacte. Une façon de corriger
  cela est d'enrichir le langage~: pour chaque variable $x$ libre dans
  $\mathcal F$ ou $A$, on ajoute un symbole de constante $c_x$, et on considère
  ensuite la théorie où $x$ est remplacé par $c_x$. On a une correspondance
  entre l'existence d'un modèle avec $x$ ou avec $c_x$, puisque notre théorie
  sans $c_x$ pourra s'évaluer dans le modèle avec $x\mapsto c_x$.
\end{remark}

Il nous reste donc à démontrer qu'une théorie consistante admet bien un modèle.
Cette construction est technique, c'est pourquoi on va commencer par donner
l'idée de la preuve.

L'idée principale est de construire un modèle syntaxique, c'est-à-dire un modèle
dont les éléments sont exactement les termes (clos) du langage. Ceux-ci seront
quotientés par l'égalité~: si $\mathcal T \vdash t = u$, alors les termes $t$ et
$u$ seront identifiés. En construisant un tel modèle $\mathcal M$, on va
chercher à ce que $\mathcal T \vdash \varphi$ si et seulement si
$\mathcal M\vDash \varphi$ puisque ce que vérifie $\mathcal M$ est exactement ce
que $\mathcal T$ peut prouver. On a alors besoin de deux caractéristiques
essentielles :
\begin{itemize}
\item tout d'abord, il nous faut résoudre un problème quant à la quantification
  existentielle. Supposons que $\exists x,P$ soit vraie~: on veut pouvoir
  exhiber un élément $m$ tel que $P[m/x]$ est vrai, et nos éléments sont des
  termes. On en déduit donc qu'il faut pour chaque proposition $P$ avoir un
  terme $t_P$ correspondant tel que $\exists x, P \implies P[t_P/x]$. Cela n'est
  pas vrai \latinexpr{a priori}, on va donc chercher à élargir notre langage
  pour ajouter à chaque fois des constantes témoignant pour une proposition
  $\exists x,P$ vraie qu'un terme correspond~: c'est ce que l'on appelle la
  méthode des témoins de Henkin.
\item ensuite, $\mathcal T$ doit être complète. L'ensemble des énoncés
  vrais dans un modèle est une théorie complète, puisqu'un énoncé est vrai dans
  un modèle si et seulement si sa négation est fausse. Ainsi, si $\mathcal T$
  n'est pas complète, ça ne peut pas être la théorie d'un modèle. Moralement, on
  peut voir ça comme le fait qu'au moment de construire un modèle, il faut faire
  des choix parmi les propositions, car certains modèles de $\mathcal T$ peuvent
  vérifier telle proposition ou telle autre, si $\mathcal T$ n'est pas complète.
  Nous avons donc besoin d'étendre notre théorie en une théorie complète.
\item une fois cela fait, il ne nous restera plus qu'à appliquer la complétion
  de la théorie enrichie par témoins de Henkin pour obtenir une théorie
  $\overline{\mathcal T}$ contenant $\mathcal T$ et qui nous permettra de
  construire un modèle.
\end{itemize}

Historiquement, le théorème de complétude de la logique du premier ordre date de
Gödel, dans sa thèse de doctorat \cite{GödelCompleteness}, mais la méthode que
nous présentons ici (et celle généralement utilisée de nos jours) a été
introduite par Henkin, dans \cite{HenkinCompleteness}.

Pour pouvoir effectuer les deux premières étapes, il est important que celles-ci
soient compatibles entre elles.

\begin{definition}[Propriété de Henkin]
  On dit qu'une théorie $\mathcal T$ sur une signature $\Sigma$ a la propriété
  de Henkin si pour toute formule $F$ à une variable libre $x$ telle que
  $\mathcal T\vdash\exists x, F$, il existe un terme $t_F\in\Term(\Sigma)$ tel
  que $\mathcal T\vdash P[t_F/x]$.
\end{definition}

Le point important de cette propriété est qu'elle peut être vérifiée, au prix
d'une augmentation de la théorie et du langage. Il faut cependant vérifier,
alors, que la nouvelle théorie ne peut toujours pas prouver $\bot$.

Pour créer notre nouvelle théorie, on va ajouter artificiellement, pour chaque
formule $\exists x, F(x)$ où $x$ est la seule variable libre de $F$, un témoin
$c_F$ ainsi qu'une formule dans notre théorie disant que, dans le cas où
$\exists x, F(x)$ est vérifiée, il vient directement que $F(c_F)$ est vérifiée
aussi.

\begin{definition}[Clôture par témoins]
  Soit $\Sigma$ une signature, et $\mathcal T$ une théorie sur $\Sigma$. On
  construit de façon itérative la suite $\Sigma_n$ et $\mathcal T_n$ de
  signatures et de théories, où $\mathcal T_n$ est une théorie sur $\Sigma_n$ :
  \begin{itemize}
  \item $\Sigma_0 = \Sigma$ et $\mathcal T_0 = \mathcal T$.
  \item Si $\Sigma_n$ et $\mathcal T_n$ sont construits, on définit la signature
    $\Sigma_{n+1}$ en ajoutant les éléments suivants~: pour chaque formule
    $F \in \Formula(\Sigma_n)$ avec une unique variable libre
    (qu'on notera $x$), on ajoute une constante $c_F$ à $\Sigma_{n+1}$ et la
    formule close $(\exists x, F(x))\to F[c_F/x]$ à $\mathcal T_{n+1}$.
  \end{itemize}
  On définit alors
  \[\hclose{\Sigma}\defeq \bigcup_{n\in\mathbb N}\Sigma_n
  \qquad\hclose{\mathcal T} \defeq \bigcup_{n \in \mathbb N}\mathcal T_n\]
  et $\hclose{\mathcal T}$ est une théorie sur $\hclose{\Sigma}$.
\end{definition}

\begin{property}
  La théorie $\hclose{\mathcal T}$ a la propriété de Henkin.
\end{property}

\begin{proof}
  Supposons que $\hclose{\mathcal T}\vdash\exists x,F$ où
  $\exists x, F\in\Prop(\hclose{\Sigma})$. Remarquons que $F$ ne peut contenir
  qu'un nombre fini de symboles de fonctions et de relations~: on en déduit
  qu'il existe $n\in \mathbb N$ tel que $F\in\Formula(\Sigma_n)$. Cela signifie
  donc que
  \[(\exists x,F(x))\to F(c_F) \in \mathcal T_{n+1}\]
  On en déduit que $\mathcal T_{n+1}\vdash F(c_F)$.
\end{proof}

De plus, cette construction est stable par extension (ce qui nous servira
lorsque nous compléterons notre théorie).

\begin{property}\label{prop.henkin.ext}
  Si $\hclose{\mathcal T}\subseteq \mathcal S$ pour une certaine théorie
  $\mathcal S$, alors $\mathcal S$ a la propriété de Henkin.
\end{property}

\begin{proof}
  Comme $(\exists x,F)\to F(c_F)$ appartient à $\mathcal S$, la dérivation
  syntaxique précédente fonctionne encore en remplaçant $\hclose{\mathcal T}$
  par $\mathcal S$.
\end{proof}

Enfin, on veut montrer que la clôture par témoins conserve la cohérence. Pour
prouver cela, on a besoin d'abord de montrer un lemme important~: une constante
sur laquelle on ne fait aucune hypothèse revient à considérer une variable
libre.

\begin{lemma}[Simulation d'une variable par une constante]\label{lem.var.const}
  Soit $\Sigma$ une signature contenant une constante $c$,
  $A_1,\ldots,A_n\in\Formula(\Sigma\setminus\{c\})$, $P\in\Formula(\Sigma)$ et
  une variable $x\notin \VL(A_1,\ldots,A_n)$.
  Les deux propositions sont alors équivalentes~:
  \begin{itemize}
  \item $A_1,\ldots,A_n\vdash P$
  \item $A_1,\ldots,A_n\vdash P[c/x]$
  \end{itemize}
\end{lemma}

\begin{proof}
  On raisonne par double implication. Par induction sur
  $A_1,\ldots,A_n\vdash P$ (on notera $\Gamma \defeq A_1,\ldots,A_n$)~:
  \begin{itemize}
  \item si $P\in \Gamma$, alors $x\notin\VL(P)$ donc $P[c/x]=P$ et
    le résultat est évident.
  \item si $P = \top$ alors $P[c/x]=P$ encore une fois.
  \item pour toutes les règles $\bot_\mathrm c,\lnot,\lor,\land,\to$, la
    validité est immédiate puisque l'hypothèse d'induction est stable dans
    toutes les règles (les variables ne changent jamais).
  \item si la dernière règle est $\forall_\mathrm i$, on suppose alors que
    $P = \forall y, A$ et que $\Gamma\vdash A[v/y]$. Par hypothèse d'induction,
    on en déduit que $\Gamma\vdash A[v/y][c/x]$, mais on remarque que
    $A[v/y][c/x] = A[c/x][v/y]$ car $v$ est une variable différente de $x$,
    donc $\Gamma\vdash A[c/x][v/y]$, d'où $\Gamma\vdash \forall y, A[c/x]$
    puisque $v$ reste libre dans $\Gamma,A$.
  \item si la dernière règle est $\forall_\mathrm e$, on suppose alors que
    $P = A[t/y]$ pour un certain terme $t$, donc $P[c/x] = A[t/y][c/x]$ ou
    encore $P[c/x] = A[c/x][t[c/x]/y]$. Par hypothèse d'induction, on sait que
    $\Gamma\vdash \forall y, A[c/x]$, donc $\Gamma\vdash A[c/x][t[c/x]/y]$
    d'où le résultat.
  \item si la dernière règle est $\exists_\mathrm i$, on suppose alors que
    $P = \exists y, A$ et l'hypothèse d'induction nous donne
    $\Gamma\vdash A[t/y][c/x]$, mais on peut réécrire
    $A[t/y][c/x] = A[c/x][t[y/x]/y]$ et appliquer la règle $\exists_\mathrm i$
    pour avoir le résultat.
  \item si la dernière règle est $\exists_\mathrm e$, on suppose que
    $\Gamma\vdash \exists y, A$ et $\Gamma, A[v/x]\vdash P$ où $v$ n'apparaît
    pas dans $\Gamma$ ou dans $P$. On utilise notre hypothèse d'induction sur
    la première hypothèse pour en déduire que $\Gamma\vdash \exists y, A[c/x]$,
    puis sur notre deuxième hypothèse réécrite en $\Gamma\vdash A[v/y]\to B$,
    pour obtenir $\Gamma\vdash A[v/y][c/x]\to B[c/x]$. On peut réécrire
    $A[v/y][c/x] = A[c/x][v/y]$, et on peut alors déduire que
    $\Gamma,A[c/x][v/y]\vdash B[c/x]$ donc en utilisant $\exists_\mathrm e$, que
    $\Gamma\vdash B[c/x]$.
  \item si la dernière règle est $=_\mathrm i$, le résultat est direct.
  \item si la dernière règle est $=_\mathrm e$, alors on suppose que $P = A[u/y]$
    et, en utilisant l'hypothèse d'induction, que $\Gamma\vdash A[t/y][c/x]$
    et $\Gamma\vdash t[c/x] = u[c/x]$. On peut alors montrer que
    $A[t/y][c/x] = A[c/x][t[c/x]/y]$, d'où $\Gamma\vdash A[c/x][u[c/x]/y]$, ce
    qui est exactement $\Gamma\vdash A[u/y][c/x]$.
  \end{itemize}
  Ainsi si $\Gamma\vdash P$ alors $\Gamma\vdash P[c/x]$. Le sens réciproque est
  une induction sur $\Gamma\vdash P[c/x]$ et est analogue, nous ne la détaillons
  donc pas.
\end{proof}

On peut maintenant prouver que la clôture par témoins de Henkin préserve la
cohérence.

\begin{property}
  Si $\mathcal T\nvdash \bot$, alors $\hclose{\mathcal T}\nvdash\bot$.
\end{property}

\begin{proof}
  On suppose que $\mathcal T\nvdash\bot$, on montre alors par récurrence que
  $\mathcal T_n\nvdash\bot$ :
  \begin{itemize}
  \item Comme $\mathcal T_0 =\mathcal T$, le résultat est direct.
  \item Supposons par l'absurde que $\mathcal T_{n+1}\vdash \bot$, et par
    hypothèse de récurrence que $\mathcal T_n\nvdash \bot$. On sait donc qu'il
    existe $A_1,\ldots,A_n \in \mathcal T_n$ et
    $B_1,\ldots,B_p \in \mathcal T_{n+1}\setminus \mathcal T_n$
    tels que
    \[A_1,\ldots,A_n,B_1,\ldots,B_p\vdash \bot\]
    En itérant la règle $\to_\mathrm i$ et en utilisant l'équivalence
    $A\land B \to \bot \dashv\vdash A \to (B \to \bot)$ (que le lecteur peut
    montrer en exercice), on obtient le séquent équivalent
    \[A_1,\ldots,A_n\vdash \lnot\left(\bigwedge_{i = 1}^p B_i\right)\]
    On sait de plus que les $B_i$ sont de la forme
    \[B_i \defeq (\exists x, F_i) \to F_i[c_{F_i}/x]\]
    Or, comme chaque $B_i$ est dans $\mathcal T_{n+1}$ mais pas dans
    $\mathcal T_n$, on en déduit que chaque $c_{F_i}$ n'est pas dans $\Sigma_n$.
    Cela signifie donc, en utilisant le \cref{lem.var.const}, que prouver notre
    séquent est équivalent, en choisissant de nouvelles variables $y_i$, libres
    dans $A_1,\ldots,A_n,B_1,\ldots,B_p$, à prouver
    \[A_1,\ldots,A_n\vdash \lnot \left(\bigwedge_{i = 1}^p
    (\exists x, F_i) \to F_i[y_i/x]\right)\]
    On peut alors généraliser chaque $y_i$ pour obtenir
    \[A_1,\ldots,A_n\vdash \forall y_1,\ldots,y_p, \lnot \left(
    \bigwedge_{i=1}^p(\exists x, F_i) \to F_i[y_i/x]\right)\]
    On a ensuite besoin de deux équivalences relativement naturelles à
    considérer~:
    \[\exists x_i, \bigwedge_{i = 1}^p A_i\dashv\vdash
    \bigwedge_{i = 1}^p(\exists x_1, A_i)\qquad
    \exists x, (A \to B) \dashv\vdash A \to (\exists B)\]
    si $x$ n'apparaît pas dans $A$ et dans aucun $A_j$ pour $j \neq i$.

    En utilisant les lois de De Morgan on obtient donc d'abord que le séquent
    suivant est équivalent à notre séquent d'origine~:
    \[A_1,\ldots,A_n\vdash \lnot\left(\exists y_1,\ldots,\exists y_p,
    \bigwedge_{i = 1}^p (\exists x, F_i) \to F_i[y_i/x]\right)\]
    puis 
    \[A_1,\ldots,A_n\vdash \lnot\left(\bigwedge_{i = 1}^p
    \exists y_i,(\exists x, F) \to F_i[y_i/x]\right)\]
    d'où, en utilisant la deuxième équivalence~:
    \[A_1,\ldots,A_n\vdash \lnot\left(\bigwedge_{i = 1}^p
    (\exists x, F) \to (\exists y_i, F_i[y_i/x])\right)\]
    Or il est clair que chaque $(\exists x, F)\to (\exists y_i, F_i[y_i/x])$
    est prouvable puisque de la forme $A \to A$ (à renommage des variables liées
    près). On en déduit donc que $A_1,\ldots,A_n\vdash \bot$, ce qui est
    absurde. Ainsi $\mathcal T_{n+1}\nvdash \bot$.
  \end{itemize}

  Ainsi, $\mathcal T_n\nvdash\bot$ pour tout $n\in \mathbb N$. Ceci suffit à
  notre preuve, car si $\hclose{\mathcal T}\vdash \bot$, alors il existe une
  liste (finie) $\Gamma\in\List(\hclose{\mathcal T})$ telle que
  $\Gamma\vdash\bot$, mais $\Gamma\in\List(\mathcal T_n)$ pour un certain $n$,
  puisque cette liste est finie. Comme $\mathcal T_n\nvdash\bot$, cela est
  absurde. Ainsi $\hclose{\mathcal T}\nvdash\bot$.
\end{proof}

On va maintenant énoncer qu'on peut compléter une théorie pour $\vdash$. Pour
prouver ce théorème, on va utiliser un contexte plus adapté qui est celui du
\cref{chp.ordres}.

\begin{theorem}[Extension complète d'une théorie]\label{thm.completion}
  Soit $\mathcal T$ une théorie telle que $\mathcal T \nvdash \bot$. Alors il
  existe une théorie $\mathcal T' \supseteq \mathcal T$ complète pour $\vdash$,
  c'est-à-dire telle que pour tout formule $\varphi$, on a
  $\mathcal T'\vdash \varphi$ ou $\mathcal T'\vdash \lnot \varphi$, et
  $\mathcal T'\nvdash\bot$.
\end{theorem}

\begin{proof}
  Retardée à la \cref{sbsct.ultrafiltre}.
\end{proof}

En combinant les deux propriétés sur la complétion de Henkin avec le
\cref{thm.completion}, nous obtenons une extension de $\mathcal T$ complète et
vérifiant la propriété de Henkin.

\begin{lemma}
  Si $\mathcal T$ est une théorie sur $\Sigma$ telle que
  $\mathcal T\nvdash\bot$, alors il existe une signature
  $\Sigma'\supseteq\Sigma$ et une théorie
  $\mathcal T'\supseteq\mathcal T$ sur $\Sigma'$, complète et possédant la
  propriété de Henkin.
\end{lemma}

\begin{proof}
  On applique le \cref{thm.completion} à $\hclose{\mathcal T}$~: comme on sait
  que $\hclose{\mathcal T}\nvdash\bot$, on peut effectivement compléter cette
  théorie pour $\vdash$. Avec la \cref{prop.henkin.ext}, on sait que cette
  complétion vérifie aussi la propriété de Henkin.
\end{proof}

On fixe maintenant la théorie $\mathcal T'$ construite à partir de $\mathcal T$.
On fixe aussi la signature $\Sigma'$ comme étant $\hclose{\Sigma}$.

\begin{definition}[Modèle syntaxique]
  On définit le modèle syntaxique $\mathcal M_{\mathcal T'}$ par :
  \begin{itemize}
  \item $|\mathcal M_{\mathcal T'}|$ défini comme l'ensemble des termes sur
    $\Sigma'$ (la signature définie plus tôt) qui n'ont aucune variable libre,
    quotienté par $\equiv$ où $\equiv$ est défini par
    \[t\equiv u \defeq \mathcal T'\vdash t = u\]
  \item pour chaque symbole de fonction $f\in\Sigma'$ d'arité $n$, on associe la
    fonction
    \[\begin{array}{ccccc}
    f^{\mathcal T'} & : & |\mathcal M_{\mathcal T'}|^n & \longrightarrow &
    |\mathcal M_{\mathcal T'}|\\
    & & (\overline{t_1},\ldots,\overline{t_n}) & \longmapsto &
    \overline{f(t_1,\ldots,t_n)}
    \end{array}\]
  \item pour chaque symbole de relation $r\in\Sigma'$ d'arité $n$, on définit la
    relation $r^{\mathcal T'}$ par
    \[r^{\mathcal T'}(\overline{t_1},\ldots,\overline{t_n}) \defeq
    \mathcal T'\vdash r(t_1,\ldots,t_n)\]
  \end{itemize}
\end{definition}

\begin{proof}
  Pour que cette définition ait du sens, il convient de montrer que $\equiv$ est
  une relation d'équivalence, et qu'elle est compatible avec les symboles de
  fonction et de relation (c'est-à-dire que nos définitions de
  $f^{\mathcal T'}$ et $r^{\mathcal T'}$ ne dépendent pas du représentant choisi).

  Montrons d'abord que $\equiv$ est une relation d'équivalence :
  \begin{itemize}
  \item Grâce à $=_\mathrm i$, on sait que $\mathcal T'\vdash t = t$ pour tout
    $t\in \Term(\Sigma')$, donc $\equiv$ est réflexive.
  \item Supposons que $t\equiv u$, c'est-à-dire que $\mathcal T'\vdash t = u$,
    on peut alors construire l'arbre de preuve suivant :
    \begin{prooftree}
      \AxiomC{}
      \RightLabel{$=_\mathrm i$}
      \UnaryInfC{$\mathcal T' \vdash t = t$}
      \AxiomC{$\mathcal T' \vdash t = u$}
      \RightLabel{$=_\mathrm e$}
      \BinaryInfC{$\mathcal T' \vdash u = t$}
    \end{prooftree}
    donc $u\equiv t$ ($t=t$ peut se lire comme $(x=t)[t/x]$).
  \item Supposons que $t\equiv u$ et $u\equiv v$, montrons alors que
    $t\equiv v$ :
    \begin{prooftree}
      \AxiomC{$\mathcal T'\vdash u = v$}
      \AxiomC{}
      \RightLabel{$=_\mathrm i$}
      \UnaryInfC{$\mathcal T' \vdash t = t$}
      \AxiomC{$\mathcal T' \vdash t = u$}
      \RightLabel{$=_\mathrm e$}
      \BinaryInfC{$\mathcal T' \vdash u = t$}
      \RightLabel{$=_\mathrm e$}
      \BinaryInfC{$\mathcal T'\vdash t = v$}
    \end{prooftree}
  \end{itemize}
  Ainsi $\equiv$ est bien une relation d'équivalence.

  Soit $f$ un symbole de fonction d'arité $n$. Pour simplifier la preuve, on
  suppose que $f$ est d'arité $1$ (il suffit ensuite de faire une récurrence
  sur $n$ pour généraliser la preuve que nous allons faire). Pour que $\equiv$
  soit compatible avec $f$, il faut que l'image de $f$ ne dépende pas du choix
  du représentant, c'est-à-dire que si $t\equiv u$ alors
  $f^{\mathcal T'}(\overline t) = f^{\mathcal T'}(\overline u)$, c'est-à-dire que
  $\mathcal T'\vdash f(t) = f(u)$, ce que l'on peut prouver par
  \begin{prooftree}
    \AxiomC{}
    \RightLabel{$=_\mathrm i$}
    \UnaryInfC{$\mathcal T' \vdash f(t) = f(t)$}
    \AxiomC{$\mathcal T'\vdash t = u$}
    \RightLabel{$=_\mathrm e$}
    \BinaryInfC{$\mathcal T'\vdash f(t) = f(u)$}
  \end{prooftree}
  donc $f^{\mathcal T'}$ est bien définie.

  De même, pour une relation $r$ prise pour simplifier d'arité $1$, il convient
  de montrer que si $t\equiv u$ alors
  $r^{\mathcal T'}(\overline t)\to r^{\mathcal T'}(\overline u)$ (il faudrait une
  équivalence, mais il suffit en fait de montrer l'implication puisque $\equiv$
  a été montrée symétrique). L'arbre de preuve suivant nous le montre :
  \begin{prooftree}
    \AxiomC{$\mathcal T'\vdash r(t)$}
    \AxiomC{$\mathcal T'\vdash t = u$}
    \RightLabel{$=_\mathrm e$}
    \BinaryInfC{$\mathcal T'\vdash r(u)$}
  \end{prooftree}
  donc $r^{\mathcal T'}$ est bien définie.
\end{proof}

Afin de montrer le théorème de complétude, on va vouloir montrer que les
formules vérifiées par $\mathcal M_{\mathcal T'}$ sont exactement les résultats
montrés par la théorie $\mathcal T'$. Comme l'induction se fera sur la structure
de la formule vérifiée, il nous faut considérer des possibles variables libres.
On définit donc d'abord une notation pour définir un environnement sur
$\mathcal M_{\mathcal T'}$.

\begin{definition}[Valuation syntaxique]
    Soit $\rho$ une valuation sur $\mathcal M_{\mathcal T'}$, on la notera
    $\overline \rho$ et on notera $\rho$ une fonction
    $\Var \partialto \Term(\Sigma')$ telle que
    $\overline{\rho(x)} = \overline\rho(x)$.
\end{definition}

On peut alors démontrer le lemme qui permettra de prouver le théorème de
complétude.

\begin{lemma}
  Pour toute formule $\varphi \in \Formula(\Sigma')$ et toute valuation
  $\overline\rho$ telle que $\VL(\varphi)\subseteq \dom(\rho)$, on a
  l'équivalence suivante~:
  \[\mathcal T' \vdash \rho(\varphi) \iff
  \mathcal M_{\mathcal T'},\overline \rho\models \varphi\]
\end{lemma}

\begin{proof}
  On peut prouver par induction sur la structure des termes que pour tout terme
  $t \in \Term(\Sigma')$, on a $\overline{\rho(t)} = \overline \rho(t)$
  (l'exercice est laissé au lecteur, mais il s'agit d'une induction qui ne
  présente aucune difficulté).
  
  On démontre le résultat par induction sur la structure de $\varphi$ :
  \begin{itemize}
  \item si $\varphi$ est $\top$ ou $\bot$, le résultat est évident (en
    particulier $\mathcal T'\nvdash\bot$).
  \item si $\varphi = r(t_1,\ldots,t_n)$ alors la proposition
    $\mathcal T'\vdash r(\rho(t_1),\ldots,\rho(t_n))$ signifie exactement
    \[r^{\mathcal T'}(\overline{\rho(t_1)},\ldots,\overline{\rho(t_n)})\]
    ce qui correspond à
    \[\mathcal M_{\mathcal T'}\models 
    r(\overline \rho (t_1),\ldots,\overline \rho(t_n))\]
    en utilisant le résultat sur la substitution des termes.
    Ce fait est une équivalence puisqu'il provient de la définition même de
    $r^{\mathcal T'}$.
  \item si $\varphi = \lnot \psi$, montrons que
    $\mathcal T'\vdash\rho(\varphi)\implies \mathcal M_{\mathcal T'},
    \overline\rho \models \varphi$.
    Comme $\mathcal T'\vdash \lnot \rho(\psi)$, on en déduit que
    $\mathcal T'\nvdash \rho(\psi)$ (car $\mathcal T'\nvdash\bot$), donc par
    hypothèse d'induction
    $\mathcal M_{\mathcal T'},\overline \rho\not\models \psi$, donc
    $\mathcal M_{\mathcal T'},\overline \rho\models \varphi$.
    Montrons la réciproque~: on suppose que
    $\mathcal M_{\mathcal T'},\overline\rho\models \varphi$, c'est-à-dire que
    $\mathcal M_{\mathcal T'},\overline\rho\not\models \psi$. Par hypothèse
    d'induction, cela signifie que $\mathcal T'\nvdash \rho(\psi)$, mais
    $\mathcal T'$ est complète donc $\mathcal T'\vdash\lnot\rho(\psi)$. On a
    donc l'équivalence.
  \item si $\varphi = \psi \land \chi$, on suppose que
    $\mathcal T'\vdash \rho(\psi)\land\rho(\chi)$. Comme $\mathcal T'$ est
    complète, cela est équivalent à dire que $\mathcal T'\vdash \rho(\psi)$ et
    $\mathcal T'\vdash \rho(\chi)$ (puisque ni $\lnot\rho(\psi)$ ni
    $\lnot \rho(\chi)$ ne sont prouvables), ce qui est équivalent par hypothèse
    d'induction à dire que
    $\mathcal M_{\mathcal T'},\overline\rho\models\psi$ et
    $\mathcal M_{\mathcal T'},\overline\rho\models\chi$, donc équivalent à
    $\mathcal M_{\mathcal T'},\overline\rho\models \psi\land \chi$.
  \item les cas de $\to$ et $\lor$ se ramènent directement à $\land$ et $\lnot$
    grâce aux lois de De Morgan (sémantiquement on peut voir facilement
    l'équivalence par l'égalité des valeurs de vérité).
  \item le cas de $\forall$ se ramène à celui de $\exists$ et $\lnot$ grâce
    aux lois de De Morgan, nous traitons donc simplement $\exists$.
  \item si $\varphi = \exists x, \psi$, où $x \notin \VL(\rho)$; alors supposons
    que $\mathcal T'\vdash \exists x, \rho(\psi)$~: comme chaque terme substitué
    par $\rho$ devient un terme clos et que toutes les variables libres de
    $\varphi$ sont dans le domaine de $\rho$, on sai que seul $x$ est une
    variable libre de $\rho(\psi)$, la propriété de Henkin sur $\mathcal T'$
    nous donne done une constante $c$ telle que
    $\mathcal T'\vdash \rho(\psi[c/x])$
    et donc un élément $\overline c \in |\mathcal M_{\mathcal T'}|$ tel que
    $\mathcal M_{\mathcal T'}, \overline\rho[x\mapsto \overline c]
    \models \psi$,
    donc
    $\mathcal M_{\mathcal T'},\overline\rho\models \exists x, \psi$.
    Réciproquement, s'il existe $\overline t$ tel que
    $\mathcal M_{\mathcal T'},\overline\rho[x\mapsto \overline t]\models \psi$,
    alors par hypothèse d'induction
    $\mathcal T'\vdash \rho[x\mapsto \overline t](\psi)$ donc
    $\mathcal T'\vdash \exists x, \psi$.
  \end{itemize}

  D'où le résultat par induction.
\end{proof}

On peut maintenant prouver le théorème de complétude.

\begin{theorem}[Complétude de la déduction naturelle \cite{GödelCompleteness}]
  Soit une signature $\Sigma$ et une théorie $\mathcal T$ sur $\Sigma$ telle que
  $\mathcal T\nvdash\bot$. Alors il existe un modèle
  $\mathcal M\models \mathcal T$.
\end{theorem}

\begin{proof}
  En reprenant notre modèle $\mathcal M_{\mathcal T'}$, on sait à partir du lemme
  précédent que $\mathcal M_{\mathcal T'}\models \mathcal T'$ grâce à la règle
  d'axiome, et au fait que $\mathcal T'$ ne contient que des formules closes.
  Comme $\mathcal T\subseteq\mathcal T'$, on en déduit donc que
  $\mathcal M_{\mathcal T'}\models \mathcal T$.
\end{proof}

\begin{theorem}[Complétude, deuxième version]\label{thm.completude}
  Soit $\Sigma$ une signature. Alors les deux relations
  $\vdash,\vDash\subseteq \powerset(\Formula(\Sigma))\times\Formula(\Sigma)$
  coïncident.
\end{theorem}

On récupère en tant que conséquence un théorème essentiel de la théorie des
modèles~: le théorème de compacité.

\begin{theorem}[Théorème de compacité]\label{thm.compacite}
  Soit $\Sigma$ une signature, $\mathcal F \subseteq\Formula(\Sigma)$ et
  $F\in\Formula(\Sigma)$, alors $\mathcal F\vDash F$ si et seulement s'il existe
  $\mathcal A\subseteq_{\mathrm{fin}} \mathcal F$ tel que
  $\mathcal A\vDash F$.
\end{theorem}

\begin{proof}
  En effet, si $\mathcal F\vDash F$ alors $\mathcal F\vdash F$, d'où par
  définition l'existence de $\Gamma\in\List(\mathcal F)$ tel que
  $\Gamma\vdash A$. Il n'y a qu'un nombre fini de propositions dans $\Gamma$,
  donc on peut trouver $\mathcal A\subseteq_{\mathrm{fin}}\mathcal F$ tel que
  $\Gamma\in\List(\mathcal A)$ : on en déduit donc que $\mathcal A\vdash F$,
  d'où par correction que $\mathcal A\vDash F$.
\end{proof}

En étudiant un peu en détail le modèle syntaxique que nous avons construit, il
est possible d'étudier son cardinal. En fait, comme le modèle est construit
comme un quotient de l'ensemble des termes, il nous suffit de dénombrer cet
ensemble. Cela mène au théorème de Löwenheim-Skolem descendant.

\begin{theorem}[Löwenheim-Skolem descendant]
  Soit $\mathcal T$ une théorie cohérente sur une signature $\Sigma$ qui
  n'admet aucun modèle fini. Alors il existe un modèle de $\mathcal T$ de
  cardinal inférieur à $\max(|\Sigma_{\mathcal F}|,\aleph_0)$.
\end{theorem}

\begin{proof}
  En effet, soit le modèle $\mathcal M$ de $\mathcal T$ syntaxique, construit
  précédemment. Comme $\mathcal T$ n'admet aucun modèle fini, le modèle est de
  cardinal au moins $\aleph_0$. De plus, on a une surjection
  $\Term(\hclose\Sigma) \to \mathcal M$ par $t\mapsto \overline t$. L'ensemble
  $\Term(\hclose\Sigma)$ est de cardinal $\max(|\Sigma_{\mathcal F}|,\aleph_0)$.
  En effet, en notant à nouveau $\Sigma_n$ chaque signature utilisée à chaque
  étape de la construction de $\hclose\Sigma$, on voit qu'on ajoute un nombre
  de $\max(|\Sigma_\mathcal F|,\aleph_0)$ de constantes $c_F$. La signature
  $\hclose\Sigma$ est donc l'union dénombrable d'ensembles de cardinal
  $\max(|\Sigma_\mathcal F|,\aleph_0)$, donc c'est un ensemble dénombrable.
  On voit enfin que l'ensemble des termes sur $\hclose\Sigma$ est de même
  cardinal, avec le même argument.

  En conclusion, on a une surjection d'un ensemble de cardinal
  $\max(|\Sigma_\mathcal F|,\aleph_0)$ vers le modèle syntaxique, qui est donc
  un modèle vérifiant l'affirmation voulue.
\end{proof}

Le théorème de compacité, lui, nous permet de déduire la version ascendante du
théorème.

\begin{theorem}[Löwenheim-Skolem ascendant]
  Soit $\mathcal T$ une théorie admettant un modèle infini, et
  soit $\kappa$ un cardinal infini. Alors il existe un modèle
  de $\mathcal T$ de cardinal supérieur à $\kappa$.
\end{theorem}

\begin{proof}
  On ajoute à $\Sigma$ l'ensemble $\{c_a\mid a \in \kappa\}$ de constantes, et
  on définit la théorie $\mathcal T'$ sur cette signature par
  \[\mathcal T'\defeq\mathcal T\cup \bigcup_{a,b\in \kappa}(c_a\neq c_b)\]
  ainsi un modèle de $\mathcal T'$ est naturellement un modèle de $\mathcal T$,
  et il a au moins $\kappa$ éléments. Par le théorème de compacité, pour
  que $\mathcal T'$ soit cohérent, il suffit que toute partie finie de
  $\mathcal T'$ soit cohérente. Mais le modèle de $\mathcal T$, infini, est
  un modèle de toute partie finie de $\mathcal T'$ en associant aux éléments
  $c_a$ des éléments distincts (seul un nombre fini est considéré par la partie
  finie de $\mathcal T'$, les autres peuvent être associés au même élément). Ainsi
  on a un modèle de $\mathcal T'$.
\end{proof}

On peut ainsi énoncer le théorème de Löwenheim-Skolem sous une forme générale,
utilisant la version ascendante et descendante. Ce théorème, dû à Löwenheim
pour sa forme descendante, a été prouvé rigoureusement par Skolem ensuite.

\begin{theorem}[Löwenheim-Skolem\cite{Löwenheim1915}\cite{skolem1920logisch}]
  \label{thm.LS}
  Soit $\mathcal M$ une structure infinie sur une signature $\Sigma$ et
  $\kappa \geq \max(|\Sigma_{\mathcal F}|,\aleph_0)$. Alors il existe une
  structure $\mathcal N$ élémentairement équivalente à $\mathcal M$,
  c'est-à-dire telle que pour tout énoncé clos $\varphi\in\Prop(\Sigma)$, on a
  \[\mathcal M \models \varphi \iff \mathcal N \models \varphi\]
  et telle que $|\mathcal N| = \kappa$.
\end{theorem}

\begin{proof}
  On combine les deux résultats précédents~: on considère tout d'abord la
  théorie $\mathcal T_{\mathcal M}$ des propositions vraies dans $\mathcal M$.
  Cette théorie étant complète (un modèle vérifie un énoncé ou sa négation),
  construire un modèle de $\mathcal T_{\mathcal M}$ de cardinal $\kappa$
  suffit à prouver ce qu'on veut. On définit alors, comme précédemment, la
  théorie $\mathcal T_\mathcal M'$ constituée des énoncés $c_a \neq c_b$
  (on enrichit la signature en conséquences).

  On sait donc qu'il existe un modèle syntaxique de cette théorie, qui est
  donc de cardinal inférieur à $\kappa$ par la version descendante du
  théorème, et de cardinal supérieur à $\kappa$ par la version ascendante
  du théorème~: le modèle syntaxique est de cardinal $\kappa$.

  On en déduit donc le résultat.
\end{proof}

\section{Des éléments de méta-mathématiques}

Cette section annexe a pour objectif de donner des résultats qui permettent de rapprocher
notre objet d'étude (la logique formelle) de la pratique mathématique.
En effet, il manque un élément essentiel des mathématiques à notre système syntaxique
actuel~: la notion de définition.

Il est possible d'éviter ce problème en considérant qu'aucune définition n'existe. Dans
une telle optique, on utilise uniquement des raccourcis du langage lorsqu'on désigne un
objet défini~: par exemple, il faudrait non pas considérer un objet à part nommé
$\varnothing$, mais un objet $x$ quelconque auquel on a donné la propriété définissant
$\varnothing$, c'est-à-dire
\[\forall y, y \notin x\]

L'autre choix possible est de considérer que définir un objet $A$ avec une propriété
$\varphi(A)$ revient à ajouter à notre signature une constante $A$ et l'axiome
$\varphi(A)$.

En réalité, ces deux procédés sont équivalents, ce qui reviendra au théorème central de
cette section. Pour pouvoir l'énoncer et le montrer, on va d'abord s'intéresser aux
conséquences d'une extension de langage sur les théories.

\subsection{Extension et conservativité}

Supposons donnée une signature $\Sigma$ et un enrichissement $\Sigma' \supseteq \Sigma$.
Il existe une façon naturelle d'associer une théorie sur $\Sigma$ à une théorie sur
$\Sigma'$ et une façon naturelle de faire l'inverse.

\begin{definition}[Trace d'une théorie dans un enrichissement]
  Soient $\Sigma\subseteq \Sigma'$ deux signatures. Soit $\mathcal T$ une théorie sur
  $\Sigma$ et $\mathcal T'$ une théorie sur $\Sigma'$. On appelle trace de $\mathcal T$
  (respectivement trace de $\mathcal T'$) la théorie sur $\Sigma'$ (respectivement
  sur $\Sigma$) suivante~:
  \[\tr_{\Sigma'}(\mathcal T) \defeq \mathcal T
  \qquad \tr_{\Sigma}(\mathcal T') \defeq \mathcal T' \cap \Prop(\Sigma)\]
\end{definition}

Les traces permettent de facilement passer d'un modèle pour une théorie à un modèle sur
une sous-signature ou sur un enrichissement~:

\begin{proposition}
  Soient $\Sigma\subseteq \Sigma'$ deux signature. Soit $\mathcal T$ une théorie sur
  $\Sigma$ et $\mathcal M$ un modèle de $\mathcal T$, alors tout enrichissement de
  $\mathcal M$ sur $\Sigma'$ est un modèle de $\tr_{\Sigma'}(\mathcal T)$.
\end{proposition}

\begin{proposition}\label{prop.ext.trace2}
  Soient $\Sigma\subseteq \Sigma'$ deux signature. Soit $\mathcal T'$ une théorie sur
  $\Sigma'$ et $\mathcal M$ un modèle de $\mathcal T'$, alors l'affaiblissement de
  $\mathcal M$ à $\Sigma$ est un modèle de $\tr_{\Sigma}(\mathcal T')$.
\end{proposition}

\begin{exercise}
  Prouver les deux propositions précédentes.
\end{exercise}

\begin{example}
  Considérons la signature des groupes commutatifs et celle des anneaux, ainsi que leurs
  théories associées. On peut remarquer que la trace de la théorie des anneaux à la
  signature des groupes est exactement la théorie des groupes~: ainsi, un anneau contient
  toujours un groupe pour sa structure additive. Cependant, dans l'autre sens, un
  groupe qu'un munit d'une seconde opération binaire et d'une seconde constante (ce qui
  correspond donc à prendre un modèle de la trace de la théorie des groupes dans la
  signature des anneaux) n'est pas \latinexpr{a priori} un anneau.
\end{example}

On remarque donc, par l'exemple précédent, que la trace d'une théorie dans une plus
grande théorie fait généralement perdre une forme d'information.

Notre définition de trace, cependant, est particulièrement syntaxique~: si l'on considère
par exemple la théorie sur la signature des anneaux donnée par le seul axiome
\[(\forall x, x + x = 0) \land (\forall x, x \times x = 0)\]
alors sa trace sur la signature des groupes sera simplement $\varnothing$.
Ce résultat n'est pas souhaitable, puisque l'axiome donne une information sur le
comportement du groupe additif avec sa moitié gauche.

On peut régler ce problème directement en considérant, au lieu d'une théorie $\mathcal T$
dont on regarde la trace, la trace $\tr_{\Sigma}(\vdclose{\mathcal T})$. Dans un tel cas,
l'énoncé $\forall x, x + x = 0$ appartient effectivement à la trace.

Ainsi, la façon naturelle de plonger une théorie dans une signature plus petite est
d'en regarder sa clôture par la relation de prouvabilité~: cela correspond donc à
considérer l'ensemble des théorèmes qui s'écrivent dans $\Sigma$ mais peuvent se
prouver dans $\mathcal T$ en tant que théorie sur $\Sigma'$. C'est cette vision qui
motive la notion d'extension conservative.

\begin{definition}[Extension conservative]
  Soient $\Sigma\subseteq \Sigma'$ deux signatures, et $\mathcal T$ une théorie sur
  $\Sigma$.
  Une extension conservative (on dit aussi conservatrice) de $\mathcal T$ est
  une théorie $\mathcal T'$ sur $\Sigma'$ telle que
  \[\tr_\Sigma(\vdclose{\mathcal T'}) = \vdclose{\mathcal T}\]
\end{definition}

Une extension conservative est un outil important pour la pratique mathématique. En
effet, on ne conserve pas en pratique la même signature tout au long d'une démonstration,
et encore moins pour développer toute une théorie~: on a besoin d'ajouter de nouveaux
objets qu'on définit généralement à partir des anciens objets. Prouver qu'une extension
est conservative signifie que l'on prouve qu'en ajoutant la possibilité de parler de
nouveaux objets, on ne change pour autant pas les formules que l'on souhaite prouver.

\begin{example}
  On le verra dans le CHAPITRE QUINZE~: il est possible d'ajouter la fonction
  \[\begin{array}{ccccc}
  \exp & : & \mathbb N^2 & \longrightarrow & \mathbb N\\
  & & (n,m) & \longmapsto & n^m
  \end{array}\]
  à la théorie de l'arithmétique de Peano en gardant une extension conservative.
  Dans un tel cas, on peut alors écrire des propositions telles que
  \[\forall x, \forall y, \forall z, \forall n, n > 2 \implies x^n + y^n \neq z^n\]
  et les utiliser dans des démonstrations, pour prouver des énoncés impliquant uniquement
  les symboles $+, \times, 0, S$.
\end{example}

On peut désormais s'intéresser au cas particulier des extensions par définition.

\subsection{Extensions par définitions}

On fixe pour tout le reste de la section une signature $\Sigma$. L'objectif d'une
extension par définition est le suivant~: supposons donnée une formule $\varphi(x)$ qui
caractérise un objet $x$, nous souhaitons nommer cet objet, l'ajouter à nos symboles et
considérer $\varphi(x)$ comme un axiome définissant ce symbole. Le théorème principal
est alors que ce procédé, s'il ajoute effectivement des formules à notre ensemble
$\Formula$, ne va pas changer les théorèmes que l'on peut prouver à l'origine. Ainsi,
les définitions permettent d'ajouter des objets mais ne changent pas la théorie ambiante.

Deux cas sont alors à traiter puisque les symboles sont de deux types (symboles de
fonction et symbole de relation). Nous donnons d'abord le cas des symboles de relation
car ceux-ci se traitent assez facilement.

\subsubsection{Définition des relations}

\begin{definition}[Extension par définition d'une relation]
  Soit une théorie $\mathcal T$ et une formule $\varphi(x_1,\ldots,x_n)$, c'est-à-dire
  telle que $\VL(\varphi) \subseteq \{x_1,\ldots,x_n\}$. On définit l'extension
  par définition de $\varphi$ par la signature
  \[\Sigma' \defeq \Sigma \cup \{{R_\varphi}^n\}\]
  où $R_\varphi$ est un symbole de relation d'arité $n$, et la théorie
  \[
    \mathcal T' \defeq \mathcal T \cup
    \{`` \forall x_1,\ldots,x_n, R_\varphi(x_1,\ldots,x_n)
    \leftrightarrow \varphi(x_1,\ldots,x_n)"\}
  \]
  (On se réfère à l'\cref{exo.equiv} pour traiter du symbole $\leftrightarrow$.)
\end{definition}

Le symbole $R_\varphi$ joue exactement le rôle de $\varphi$~: la raison pour laquelle
$\mathcal T'$ est une extension conservative de $\mathcal T$ est donc simplement
que dans toutes les démonstrations, on peut remplacer $R_\varphi$ par $\varphi$. On
souhaite donc~:
\begin{itemize}
\item construire pour chaque formule $\psi$ une nouvelle formule $\psi_{-R_\varphi}$ qui
  remplace les occurrences de $R_\varphi$ par $\varphi$~;
\item prouver que $\mathcal T' \vdash \psi \leftrightarrow \psi_{-R_\varphi}$ pour toute
  formule $\psi$~;
\item prouver que $\mathcal T' \vdash \psi$ si et seulement si
  $\mathcal T \vdash \psi_{-R_\varphi}$.
\end{itemize}

\begin{definition}[Traduction sans $R_\varphi$]
  Soit une formule $\psi \in \Formula(\Sigma')$. On définit sa traductions sans
  $R_\varphi$ par inductino sur la structure des formules~:
  \begin{itemize}
  \item si $\psi = \top, \bot, t = u, S(t_1,\ldots,t_k)$ où $S \neq R_\varphi$, alors
    $\psi_{-R_\varphi} \defeq \psi$~;
  \item si $\psi = R_\varphi(t_1,\ldots,t_n)$, alors
    $\psi_{-R_\varphi} \defeq \varphi(t_1,\ldots,t_n)$~;
  \item si $\psi = \lnot \psi'$, alors $\psi_{-R_\varphi} \defeq \lnot \psi'_{-R_\varphi}$~;
  \item si $\psi = \psi' \oplus \psi''$ où $\oplus \in \{\land,\lor,\to\}$,
    alors $\psi_{-R_\varphi} \defeq \psi'_{-R_\varphi} \oplus \psi''_{-R_\varphi}$~;
  \item si $\psi = Q x, \psi'$ où $Q$ est un quantificateur, alors
    $\psi_{-R_\varphi} \defeq Q x, \psi'_{-R_\varphi}$.
  \end{itemize}
\end{definition}

\begin{proposition}
  Pour toute formule $\psi \in \Formula(\Sigma')$, on a
  \[\mathcal T' \vdash \psi \leftrightarrow \psi_{-R_\varphi}\]
\end{proposition}

\begin{proof}
  On raisonne par induction sur $\psi$~:
  \begin{itemize}
  \item si $\psi = \top, \bot, t = u, S(t_1,\ldots,t_k)$ où $S \neq R_\varphi$, alors
    on veut prouver que $\mathcal T' \vdash \psi \leftrightarrow \psi$ ce qui est
    automatique~;
  \item si $\psi = R_\varphi(t_1,\ldots,t_n)$, alors on veut prouver que
    $\mathcal T' \vdash R_\varphi(t_1,\ldots,t_n) \leftrightarrow \varphi(t_1,\ldots,t_n)$
    ce qui s'obtient exactement en spécialisant l'axiome ajouté dans $\mathcal T'$
    grâce à la règle $\forall_\mathrm e$.
  \item pour tous les cas suivants, la preuve est la conséquence d'un lemme que l'on
    donne en exercice.
  \end{itemize}

  Ainsi $\mathcal T' \vdash \psi \leftrightarrow \psi_{-R\varphi}$.
\end{proof}

\begin{exercise}
  Montrer le lemme suivant~:
  \begin{lemma}
    On se donne une signature et des formules sur ces signatures, montrer les
    implications suivantes~:
    \[\begin{array}{ccc}
      (\vdash \varphi \leftrightarrow \varphi') &\implies&
      \vdash \lnot \varphi \leftrightarrow \lnot \varphi'\\
      (\vdash \varphi \leftrightarrow \varphi') \land
      (\vdash \psi \leftrightarrow \psi) &\implies&
      \vdash (\varphi \land \psi) \leftrightarrow (\varphi' \land \psi') \\
      (\vdash \varphi \leftrightarrow \varphi') \land
      (\vdash \psi \leftrightarrow \psi) &\implies&
      \vdash (\varphi \lor \psi) \leftrightarrow (\varphi' \lor \psi') \\
      (\vdash \varphi \leftrightarrow \varphi') \land
      (\vdash \psi \leftrightarrow \psi) &\implies&
      \vdash (\varphi \to \psi) \leftrightarrow (\varphi' \to \psi') \\
      (\vdash \varphi \leftrightarrow \varphi') &\implies&
      \vdash \exists x, \varphi \leftrightarrow \exists x, \varphi'\\
      (\vdash \varphi \leftrightarrow \varphi') &\implies&
      \vdash \forall x, \varphi \leftrightarrow \forall x, \varphi'\\
    \end{array}\]
  \end{lemma}
\end{exercise}

\begin{proposition}
  La théorie $\mathcal T'$ est une extension conservative de $\mathcal T$.
\end{proposition}

\begin{exercise}
  Montrer, par induction sur la relation $\vdash$, qu'on a l'implication suivante pour
  tout contexte $\Gamma \in \List(\Formula(\Sigma'))$ et formule
  $\psi \in \Formula(\Sigma')$~:
  \[(\mathcal T', \Gamma \vdash \psi) \implies
  \mathcal T, \Gamma_{-R_\varphi} \vdash \psi_{-R_\varphi}\]
  En déduire la proposition.
\end{exercise}

On a ainsi traité le cas des symboles de relation~: il est toujours possible, sans
changer l'ensemble des théorèmes de notre langage d'origine, d'ajouter de nouveaux
symboles de relation définis par une formule paramétrée.

\subsubsection{Définition des fonctions}

Le cas des fonctions est plus complexe. Tout d'abord, qu'est-ce que signifie le fait de
définir une fonction ? Si l'on est habitué à la théorie des ensembles, on sait qu'il
suffit de montrer que, pour une instance de l'entrée donnée, il existe une unique
instance de sortie. Si l'on écrit cela à partir du graphe $\Gamma$ de notre fonction,
cela signifie
\[\forall x_1,\ldots,x_n, \exists! y, (x_1,\ldots,x_n,y) \in \Gamma\]
Ainsi, une formule $\varphi$ définit une fonction dès lors qu'on peut prouver un tel
énoncé en remplaçant l'appartenance à $\Gamma$ par la formule $\varphi$. Il est donc
nécessaire d'ajouter une contrainte aux formules pour lesquelles on souhaite ajouter un
symbole de fonction.

\begin{definition}[Extension par définition d'une fonction]
  Soit une théorie $\mathcal T$ et une formule $\varphi(x_1,\ldots,x_n,y)$ telle que
  \[\mathcal T \vdash \forall x_1,\ldots,x_n, \exists! y, \varphi(x_1,\ldots,x_n,y)\]
  On définit l'extension par définition de $\varphi$ par la signature
  \[\Sigma' \defeq \Sigma \cup \{{f_\varphi}^n\}\]
  où $f_\varphi$ est un symbole de fonction d'arité $n$, et la théorie
  \[
    \mathcal T' \defeq \mathcal T \cup \\
    \{`` \forall x_1,\ldots,x_n, \varphi(x_1,\ldots,x_n,f_\varphi(x_1,\ldots,x_n))"\}
  \]
\end{definition}

Comme précédemment, on souhaite définir une traduction $\psi_{-f_\varphi}$, prouver que
$\mathcal T' \vdash \psi \leftrightarrow \psi_{-f_\varphi}$ et prouver que l'extension est
conservative. Cependant, la définition de la traduction est moins immédiate~: l'idée
de la traduction est de remplacer toute occurrence de $f(t_1,\ldots,t_n)$ par une
variable $y$ dont on vérifie, à part, qu'on a bien $\varphi(t_1,\ldots,t_n,y)$.
Il est donc nécessaire de faire attention aux variables. En particulier, il est
nécessaire de garder en mémoire toutes les variables introduites et les termes auxquels
elles sont associées lorsqu'on transforme un terme. On crée donc d'abord l'environnement
de traduction d'un terme $t$ qui nous permet ensuite de définir la traduction
$\psi_{-f_\varphi}$ d'une formule $\psi$.

\begin{definition}[Traduction sans $f_{\varphi}$]\label{def.trad.sans.fun}
  Pour tout terme $t$, on définit par induction la liste $\mathcal V_t$ et le terme
  $t_{-f_\varphi}$~:
  \begin{itemize}
  \item si $t = x$, alors $\mathcal V_t \defeq \varnothing$ et $t_{-f_\varphi} \defeq t$.
  \item si $t = g(t_1,\ldots,t_n)$ où $g \neq f$, alors
    \[\mathcal V_t \defeq \bigcup_{i = 1}^n \mathcal V_{t_i} \quad\text{et}\quad
    t_{-f_\varphi} \defeq g(t_{1,-f_\varphi},\ldots,t_{n,-f_\varphi})\]
  \item si $t = f(t_1,\ldots,t_n)$ alors
    \[\mathcal V_t \defeq \left(\bigcup_{i = 1}^n \mathcal V_{t_i}\right) \cup
    \{(t_{1,-f_\varphi},\ldots,t_{n,-f_\varphi})\} \quad\text{et}\quad
    t_{-f_\varphi} \defeq z\]
    où $z$ est une variable fraîche (non apparue jusqu'alors).
  \end{itemize}

  On définit maintenant pour toute formule $\psi \in \Formula(\Sigma')$ une formule
  $\psi_{-f_\varphi}$ par induction sur la structure des formules~:
  \begin{itemize}
  \item si $\psi = \top,\bot$ alors $\psi_{-f_\varphi} \defeq \psi$~;
  \item si $\psi = R(t_1,\ldots,t_n)$ où $R$ est un symbole de relation ou le symbole
    $=$, alors
    \[\psi_{-f_\varphi} \defeq \mathop{\forall}_{i = 1}^n
    \mathop{\forall}_{(u_1,\ldots,u_k,x)\in \mathcal V_{t_i}}x,
    \varphi(u_1,\ldots,u_k,x) \to R(t_{1,-f_\varphi},\ldots,t_{n,-f_\varphi})\]
    où les indices des $\forall$ signifient que pour chaque $i$ et chaque
    $(u_1,\ldots,u_k,x) \in \mathcal V_{t_1}$, on ajoute
    $\forall x, \varphi(u_1,\ldots,u_k,x) \to$ avant le reste de la formule~;
  \item si $\psi = \lnot \psi'$, alors $\psi_{-f_\varphi} \defeq \lnot \psi'_{-f_\varphi}$~;
  \item si $\psi = \psi' \oplus \psi''$ où $\oplus \in \{\lor,\land,\to\}$, alors
    $\psi_{-f_\varphi} \defeq \psi'_{-f_\varphi} \oplus \psi''_{-f_\varphi}$~;
  \item si $\psi = Q x, \psi'$ où $Q$ est une quantification, alors
    $\psi_{-f_\varphi} \defeq Q x,\psi'_{-f_\varphi}$.
  \end{itemize}
\end{definition}

Il reste maintenant à prouver les deux propriétés. Les preuves sont les mêmes que pour
le cas d'un symbole de relation, à ceci près que la conservativité va être
plus technique à prouver. On mentionne en particulier les cas suivants~:
\begin{itemize}
\item pour le cas $\exists_\mathrm i$, on suppose qu'on peut prouver pour un certain
  terme $t \in \Term(\Sigma')$ que
  \[\mathcal T',\Gamma \vdash \psi[t/x]\]
  et notre hypothèse d'induction, elle, nous permet de supposer
  \[\mathcal T, \Gamma_{-f_\varphi} \vdash (\psi[t/x])_{-f_\varphi}\]
  Cependant, comme $t$ fait potentiellement apparaitre $f$, cette formule est différente
  de $\psi_{-f_\varphi}[t/x]$ puisqu'elle contient aux formules atomiques de nouvelles
  quantifications de la forme $\forall x, \varphi(t_1,\ldots,t_n,x) \to$.
  Il faut donc montrer qu'il est possible de prouver l'un à partir de l'autre.
  Si ce lemme peut se prouver par induction sur $\psi$, on en donne ici une idée
  globale.

  Le cas qui nous intéresse est le cas atomique, dans lequel on a donc dans notre
  hypothèse $\forall x, \varphi(t_1,\ldots,t_n,x) \to \chi$ où $\chi$ est le reste de
  la formule (on se contente du cas où $t = f_\varphi(t_1,\ldots,t_n)$ pour donner une
  idée de l'argument à employer). Comme on sait prouvable dans $\mathcal T$ que
  $\forall x_1,\ldots,x_n, \exists! y, \varphi(x_1,\ldots,x_n,y)$, on peut donc
  appliquer $\forall_\mathrm e$ jusqu'à avoir $\exists! y, \varphi(t_1,\ldots,t_n,y)$
  et, alors, utiliser $\exists_\mathrm e$ pour extraire ce $y$. Il suffit
  désormais de spécialiser la formule initiale en $y$, dont on sait que la
  prémisse de l'implication $\varphi(t_1,\ldots,t_n,y)$ est vérifiée.

  Dans le cas où $t$ contient plusieurs appels à $f$, il suffit de répéter cet
  argument autant de fois que nécessaire.

\item pour le cas $=_\mathrm i$, il nous faut prouver pour n'importe quel terme
  $t\in\Term(\Sigma')$ la formule $(t = t)_{-f_\varphi}$. Dans le cas où
  $t = f(t_1,\ldots,t_n)$, la formule $(t = t)_{-f_\varphi}$ correspond à
  $\forall x, \varphi(t_1,\ldots,t_n,x) \to \forall y, \varphi(t_1,\ldots,t_n,y) \to
  x = y$
  ce qui se prouve en utilisant l'unicité pour $\exists! y, \varphi(x_1,\ldots,x_n,y)$.

\item pour le cas $=_\mathrm e$, notre hypothèse d'induction donne les deux séquents
  suivants~:
  \[\mathcal T, \Gamma_{-f_\varphi}\vdash (t=u)_{-f_\varphi} \qquad
  \mathcal T, \Gamma_{-f_\varphi}\vdash (\psi[t/x])_{-f_\varphi}\]
  On a déjà vu comment passer de la seconde hypothèse à l'hypothèse
  $\mathcal T, \Gamma_{-f_\varphi}\vdash \psi_{-f_\varphi}[t/x]$. La première hypothèse
  fonctionne d'une façon similaire, en supprimant les occurrences de
  $\forall x, \varphi(t_1,\ldots,t_n,x) \to$ en extrayant des témoins de
  $\exists!y, \varphi(t_1,\ldots,t_n,y)$.
\end{itemize}

\begin{proposition}
  Pour toute formule $\psi \in \Formula(\Sigma')$, on a
  \[\mathcal T' \vdash \psi \leftrightarrow \psi_{-f_\varphi}\]
\end{proposition}

\begin{proposition}
  La signature la théorie $\mathcal T'$ est une extension conservative de $\mathcal T$.
\end{proposition}

\begin{exercise}
  Prouver les deux propositions précédentes.
  \textit{On pourra montrer pour la seconde proposition l'implication suivante~:}
  \[\forall \Gamma\in\List(\Formula(\Sigma')), \forall \psi \in \Formula(\Sigma'),
  \mathcal T', \Gamma\vdash \psi \implies \mathcal T',
  \Gamma_{-f_\varphi}\vdash \psi_{-f_\varphi}\]
\end{exercise}

On vient donc de prouver deux choses~:
\begin{itemize}
\item si l'on donne une définition d'un objet par une formule univoque (dont l'objet
  vérifiant la formule est unique), alors donner un nom à l'objet défini ne change
  pas les formules que l'on prouve~;
\item mieux encore, on peut trouver pour toute formule $\psi$ une formule $\psi'$
  qui a le même sens que $\psi$ mais qui n'emploie pas ce raccourci. Le symbole ajouté
  peut donc être vu comme un raccourci d'écriture que l'on peut remplacer à la volée par
  la formule le définissant, ce qui donnera une formule équivalente.
\end{itemize}

Les propositions précédentes justifient donc que l'on donne des définitions dans nos
théories formelles. En effet, une formulation telle que
\begin{quote}
  On définit l'objet $X(t_1,\ldots,t_n)$ par
  \[X(t_1,\ldots,t_n) \defeq \varphi(t_1,\ldots,t_n)\]
\end{quote}
signifie que l'on applique notre théorème d'extension par définition dans le cas de
$\varphi$. Généralement, lorsque $X$ est l'ajout d'un nouveau symbole de fonction,
il n'est pas nécessaire d'écrire la formule le caractérisant et il suffit d'écrire
un terme $T(t_1,\ldots,t_n)$~: la formule associée est alors
\[\varphi(t_1,\ldots,t_n,y) \defeq `` y = T(t_1,\ldots,t_n)"\]
Dans un tel cas, la preuve de l'existence et de l'unicité de $T$ est évidente, puisque
$T$ (en tant que terme) est une fonction totale. On remarque que cela justifie néanmoins
les précautions d'usage de vérifier en cas de situation litigieuse que l'image est
bien définie et est unique (par exemple lorsqu'on définit une fonction depuis un
ensemble quotient en donnant l'image en fonction d'un représentant).

\begin{remark}
  L'utilisation d'un symbole différent entre $\defeq$ et $=$ permet ici de faire une
  différence entre le fait d'attribuer le nom (dans l'exemple ci-dessus, $X$) à un
  symbole dont on donne la sémantique (dans l'exemple ci-dessus, la formule $\varphi$)
  et le fait d'identifier la sémantique de deux objets de part et d'autre du $=$.

  On peut aussi noter une différence de statut entre les deux symboles~: le symbole
  $=$ appartient réellement à la syntaxe des formules et des preuves, tandis que le
  symbole $\defeq$ fait appelle à un théorème de méta-mathématiques pour construire une
  nouvelle notation.
\end{remark}

\newpage

\section{Exercices et problèmes}

\subsection{Exercices}

\begin{exercise}[Mise en jambes]
  \begin{enumerate}[label=(\roman*)]
      \item Prouver les formules suivantes~:
      \[\begin{array}{cc}
      (\exists x, \forall y, R(x,y)) \to (\forall y, \exists x, R(x,y)) &
      \exists x, (\varphi(x) \to \forall y, \varphi(y)) \\
      (\varphi \to \psi) \lor (\psi \to \varphi) &
      \lnot\lnot(\varphi \land \psi) \leftrightarrow \lnot\lnot\varphi\land
      \lnot\lnot\psi
      \end{array}\]
      \item Soit la signature $\Sigma = \{f^1, R^1\}$ (où l'exposant
      représente l'arité) où $f$ est un symbole de fonction et
      $R$ est un symbole de relation. Montrer la proposition
      \[(\forall x, R(f(x)) \to \forall x, R(f(f(x)))\]
      \item On considère la signature des groupes $\mathcal L_{\mathrm{Grp}}$
      donnée au début du chapitre
      et la théorie $\mathrm{Grp}$ des groupes abéliens définie par les axiomes~:
      \[\begin{array}{ccc}
          \forall x,y,z, x \times (y \times z) = (x \times y) \times z
          & \forall x, x \times e = x & \forall x, e \times x = x\\
          \forall x, x \times x^{-1} = e &  \forall x, x^{-1} \times x = e \\
      \end{array}\]
      Montrer dans cette théorie les deux formules suivantes~:
      \[\forall x, (\forall y, (x \times y = y) \land (y \times x = y)) \to 
      x = e \qquad \forall x,y, (x\times y)^{-1} = y^{-1} \times x^{-1}\]
  \end{enumerate}
\end{exercise}

\begin{exercise}[Construire des modèles]
    Pour les propositions suivantes, dire si les 
\end{exercise}

\begin{exercise}[Le connecteur d'équivalence]\label{exo.equiv}
  Dans notre définition actuelle des formules, nous n'avons pas le connecteur
  $\leftrightarrow$, permettant d'exprimer que deux formules sont équivalentes.
  Il est possible d'encoder
  \[\varphi\leftrightarrow\psi \defeq
  (\varphi \to \psi)\land (\psi \to \varphi)\]
  mais aussi d'ajouter ce constructeur, et les règles
  \[\begin{array}{cc}
  \AxiomC{$\Gamma, \varphi\vdash \psi$}
  \AxiomC{$\Gamma, \psi \vdash \varphi$}
  \RightLabel{$\leftrightarrow_\mathrm i$}
  \BinaryInfC{$\Gamma\vdash \varphi\leftrightarrow\psi$}
  \DisplayProof &
  \AxiomC{$\Gamma \vdash \varphi \leftrightarrow \psi$}
  \AxiomC{$\Gamma\vdash \varphi$}
  \RightLabel{$\leftrightarrow_\mathrm e^\mathrm g$}
  \BinaryInfC{$\Gamma\vdash \psi$}
  \DisplayProof
  \quad
  \AxiomC{$\Gamma \vdash \varphi \leftrightarrow \psi$}
  \AxiomC{$\Gamma\vdash \psi$}
  \RightLabel{$\leftrightarrow_\mathrm e^\mathrm d$}
  \BinaryInfC{$\Gamma\vdash \varphi$}
  \end{array}\]
  Montrer que les deux systèmes sont équivalents~: un séquent avec de possibles
  connecteurs $\leftrightarrow$ est prouvable si et seulement si sa traduction
  avec uniquement $\to$ et $\land$ est prouvable.
\end{exercise}

\begin{exercise}[Des formules utiles pour les cardinaux]
  Soit une signature $\Sigma$ fixée. Cet exercice vise à construire plusieurs
  formules simples et utiles pour évaluer le cardinal d'un modèle.
  \begin{enumerate}[label = (\roman*)]
  \item Soit $x_1,\ldots,x_n$ des variables. Construire une formule
    $\mathrm{Ineq}_{x_1,\ldots,x_n}$ exprimant que toutes les variables
    $x_1,\ldots,x_n$ sont deux à deux différentes.
  \item En déduire pour tout $n$ une proposition $\varphi_n$ telle que pour
    toute $\Sigma$-structure $\mathcal M$, on a
    \[\mathcal M \models \varphi_n \iff \Card(\mathcal M) \geq n\]
    ainsi qu'une proposition $\psi_n$ telle que pour toute $\Sigma$-structure
    $\mathcal M$, on a
    \[\mathcal M \models \psi_n \iff \Card(\mathcal M) = n\]
  \item Pour un cardinal $\kappa$ infini, peut-on trouver une formule
    similaire ? \textit{Indication~: utiliser un théorème du cours.}
  \end{enumerate}
\end{exercise}

\begin{exercise}[Les axiomes des ordres]
    On considère la signature relationnelle
    \[\Sigma \defeq \{\leq^2\}\]
    \begin{enumerate}[label=(\roman*)]
        \item Donner une théorie $\mathcal T_{\mathrm{poset}}$ décrivant les
        ensembles ordonnés.
        \item Donner une théorie $\mathcal T_{\mathrm{toset}}$ décrivant les
        ensembles totalement ordonnés.
    \end{enumerate}
    On considère maintenant la signature relationnelle
    \[\Sigma \defeq \{<^2\}\]
    et les axiomes suivants~:
    \begin{align*}
        A_1 & \defeq \exists x,y, x < y\\
        A_2 & \defeq \forall x,y,z, (x < y) \land (y < z) \to (x < y)\\
        A_3 & \defeq \forall x, \lnot (x < x)\\
        A_4 & \defeq\forall x,y, ((x < y) \to
        \exists z, (x < z) \land (z < y))
    \end{align*}
    Soit $\mathcal T = \{ A_1,A_2,A_3,A_4\}$.
    \begin{enumerate}[label=(\roman*),resume]
        \item Donner deux modèles non isomorphes de $\mathcal T$.
        \item Donner une proposition $\varphi$ telle que
        $\mathcal T\nvdash \varphi$ et $\mathcal T \nvdash \lnot\varphi$,
        on en déduit que $\mathcal T$ n'est pas complète.
        \item Montrer que pour n'importe quel choix de $3$ axiomes parmi les
        $4$ donnés, il existe un modèle fini de cet ensemble de $3$
        axiomes. Montrer que tout modèle de $\mathcal T$ est infini.
    \end{enumerate}
\end{exercise}

\begin{exercise}[Formes prénexes]\label{exo.prenexe}
  Soit une signature $\Sigma$ fixée. On définit l'ensemble des formules sans
  quantificateurs $\Formqf(\Sigma)\subseteq \Formula(\Sigma)$ inductivement par
  \[\varphi, \psi \Coloneq a\mid \top \mid \bot \mid
  \lnot \varphi \mid \varphi \lor \psi \mid \varphi \land \psi \mid
  \varphi \to \psi\]
  où $a \in \Atom(\Sigma)$.

  On définit ensuite inductivement l'ensemble
  $\Formula_{\mathrm{pren}}(\Sigma)\subseteq \Formula(\Sigma)$
  des formules sous forme prénexe par
  \[\varphi\Coloneq \varphi_0\mid \forall x, \varphi \mid \exists x, \varphi\]
  où $\varphi_0 \in \Formqf(\Sigma)$.

  \begin{enumerate}[label=(\roman*)]
  \item Soient $\varphi, \psi$ deux formules. Montrer qu'on a les deux
    équivalences
    \[\forall x, (\varphi \land \psi) \dashv\vdash
    (\forall x, \varphi) \land (\forall x, \psi) \qquad
    \exists x, (\varphi \lor \psi) \dashv\vdash
    (\exists x, \varphi) \lor (\exists x, \psi)\]
  \item Montrer que pour toutes formules $\varphi, \psi$ et variable $x$
    tel que $x$ n'apparait pas dans $\psi$, on a les deux équivalences
    \[(\exists x,\varphi) \land \psi\dashv\vdash
    \exists x, (\varphi \land \psi) \qquad
    (\forall x, \varphi) \lor \psi \dashv\vdash
    \forall x, (\varphi \lor \psi)\]
  \item En déduire que toute formule est équivalente à une formule sous forme
    prénexe~:
    \[\forall \varphi \in \Formula(\Sigma),
    \exists \psi \in \Formula_{\mathrm{pren}}(\Sigma),
    \varphi \dashv\vdash \psi\]
  \end{enumerate}
\end{exercise}

\begin{exercise}[La skolémisation]
  L'objectif de cet exercice est d'utiliser des symboles de fonction pour
  simuler des quantifications.

  On se donne une signature $\Sigma$ et une formule $\varphi$ sur $\Sigma$.

  \begin{enumerate}[label=(\roman*)]
  \item Montrer que la formule $\exists x, \varphi$ est satisfiable dans une
    $\Sigma$-structure si et seulement si, en notant $c$ un nouveau
    symbole de constante et $\Sigma' \defeq \Sigma \cup \{c\}$, la
    formule $\varphi[c/x]$
    est satisfiable dans une $\Sigma'$-structure.
  \item Plus généralement, montrer que la formule
    $\forall x_1,\ldots,\forall x_n, \exists y, \varphi$ est satisfiable dans
    une $\Sigma$-structure si et seulement si, en notant $f$ un nouveau
    symbole de fonction d'arité $n$ et $\Sigma' \defeq \Sigma\cup \{f\}$, la
    formule
    $\forall x_1,\ldots,\forall x_n,(\varphi[f(x_1,\ldots,x_n)/y])$ est
    satisfiable dans une $\Sigma'$-structure.
  \item En déduire que pour toute proposition $\varphi$, on peut trouver une
    formule $\psi$ prénexe sans quantification existentielle sur un
    enrichissement de $\Sigma$ telle qu'une
    structure satisfait $\varphi$ si et seulement si elle admet un
    enrichissement satisfaisant $\psi$.
  \end{enumerate}
\end{exercise}

\begin{exercise}[La théorie des relations d'équivalence]
  On fixe pour cet exercice le langage relationnel
  \[\Sigma \defeq \{\sim^2\}\]
  On définit aussi la théorie
  \[\mathcal T_{\mathrm{equiv}}\defeq \{``\forall x, x \sim x"\}\cup
  \{``\forall x, y, x \sim y \to y \sim x"\}\cup
  \{``\forall x,y,z, x \sim y \to y \sim z \to x \sim z"\}\]

  \begin{enumerate}[label=(\roman*)]
  \item Donner un ensemble d'axiomes $\mathrm{nb}_{\infty}$ tel que
    $\mathcal T_{\mathrm{equiv}}\cup \mathrm{nb}_\infty$ détermine la classe des
    ensembles munis d'une relation d'équivalence $\sim$ avec une infinité de
    classes d'équivalences.
  \item Donner un ensemble d'axiomes $\mathrm{cl}_\infty$ tel que
    $\mathcal T_{\mathrm{equiv}}\cup \mathrm{cl}_\infty$ détermine la classe des
    ensembles munis d'une relation d'équivalence $\sim$ dont toutes les classes
    d'équivalence sont infinies.
  \item On définit
    \[\mathcal T_{\mathrm{equiv},\infty} \defeq
    \mathcal T_{\mathrm{equiv}}\cup \mathrm{nb}_\infty \cup \mathrm{cl}_\infty\]
    Donner un modèle dénombrable de $\mathcal T_{\mathrm{equiv},\infty}$. Montrer
    que tout modèle dénombrable de $\mathcal T_{\mathrm{equiv},\infty}$ est
    isomorphe à ce modèle.
  \end{enumerate}
\end{exercise}

\begin{exercise}[Une application du théorème de compacité]
  Un graphe (orienté) est un couple $(V,E)$ où $V$ est un ensemble et $E$ est
  une relation binaire sur $V$ qui, étant donnés $x,y \in V$ témoigne si
  $x$ et $y$ sont voisins ou non. Il est donc possible de considérer un graphe
  comme une structure pour le langage relationnel $\{E^2\}$.

  On fixe donc \[\Sigma \defeq \{E^2\}\] le langage des graphes.
  \begin{enumerate}[label = (\roman*)]
  \item Donner une théorie dont les modèles sont exactement la classe des
    graphes ayant un cycle de taille $n$.
  \item Donner une théorie dont les modèles sont exactement la classe des
    graphes non orientés~: de tels graphes correspondent à des graphes orientés
    où la relation de voisinage est symétrique.
  \end{enumerate}
  L'objectif des questions suivantes est de prouver qu'il n'existe pas de
    théorie (du premier ordre) représentant la classe des graphes connexes.
    On suppose donc qu'il existe une théorie dont les modèles sont exactement
    les graphes connexes~; on notera cette théorie $\mathcal T$. On introduit
    alors deux nouvelles constantes, $v$ et $w$.
  \begin{enumerate}[label=(\roman*),resume]
  \item Construire pour tout $n$ une formule $\varphi_n$ exprimant que $v$ et $w$
    ne sont reliés par aucun chemin de taille $n$.
  \item En appliquant le théorème de compacité à la théorie
    \[\mathcal T' \defeq \mathcal T \cup \bigcup_{n \in \mathbb N } \{\varphi_n\}\]
    montrer qu'on a une contradiction.
  \end{enumerate}
\end{exercise}
